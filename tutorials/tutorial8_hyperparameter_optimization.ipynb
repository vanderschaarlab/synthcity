{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 8: Hyperparameter Optimization\n",
    "\n",
    "To automatically tune hyperparameters in a `synthcity` plugin to generate more realistic data, we use hyperparameter optimization (HPO) algorithms such as Tree-structured Parzen estimators (TPE), Bayesian optimization, and genetic programming. In this tutorial we will use `optuna`, a very popular HPO library implementing TPE, to tune the hyperparameters of the `nflow` plugin to synthesize the diabetes dataset.\n",
    "\n",
    "This tutorial requires the third party library `plotly` to be installed. This is not included in synthcity, as this tutorial is the only place it is needed. So in order to run this tutorial you will need to run `pip install plotly` as well as install synthcity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install synthcity\n",
    "!pip install plotly\n",
    "!pip uninstall -y torchaudio torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# third party\n",
    "import optuna\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# synthcity absolute\n",
    "import synthcity.logger as log\n",
    "from synthcity.plugins import Plugins\n",
    "from synthcity.plugins.core.dataloader import GenericDataLoader\n",
    "\n",
    "log.add(sink=sys.stderr, level=\"INFO\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "X[\"target\"] = y\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericDataLoader(\n",
    "    X,\n",
    "    target_column=\"target\",\n",
    "    sensitive_columns=[\"sex\"],\n",
    ")\n",
    "train_loader, test_loader = loader.train(), loader.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the plugin class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLUGIN = \"tvae\"\n",
    "plugin_cls = type(Plugins().get(PLUGIN))\n",
    "plugin_cls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the hyperparameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plugin_cls.hyperparameter_space()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a trial to suggest a set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthcity.utils.optuna_sample import suggest_all\n",
    "\n",
    "trial = optuna.create_study().ask()\n",
    "params = suggest_all(trial, plugin_cls.hyperparameter_space())\n",
    "params['n_iter'] = 100  # speed up\n",
    "params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the plugin with the suggested hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthcity.benchmark import Benchmarks\n",
    "\n",
    "plugin = plugin_cls(**params).fit(train_loader)\n",
    "report = Benchmarks.evaluate(\n",
    "    [(\"trial\", PLUGIN, params)],\n",
    "    train_loader,  # Benchmarks.evaluate will split out a validation set\n",
    "    repeats=1,\n",
    "    metrics={\"detection\": [\"detection_mlp\"]},  # DELETE THIS LINE FOR ALL METRICS\n",
    ")\n",
    "report['trial']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Optuna study and optimize the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    hp_space = Plugins().get(PLUGIN).hyperparameter_space()\n",
    "    hp_space[0].high = 100  # speed up for now\n",
    "    params = suggest_all(trial, hp_space)\n",
    "    ID = f\"trial_{trial.number}\"\n",
    "    try:\n",
    "        report = Benchmarks.evaluate(\n",
    "            [(ID, PLUGIN, params)],\n",
    "            train_loader,\n",
    "            repeats=1,\n",
    "            metrics={\"detection\": [\"detection_mlp\"]},  # DELETE THIS LINE FOR ALL METRICS\n",
    "        )\n",
    "    except Exception as e:  # invalid set of params\n",
    "        print(f\"{type(e).__name__}: {e}\")\n",
    "        print(params)\n",
    "        raise optuna.TrialPruned()\n",
    "    score = report[ID].query('direction == \"minimize\"')['mean'].mean()\n",
    "    # average score across all metrics with direction=\"minimize\"\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=2)\n",
    "study.best_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_contour\n",
    "from optuna.visualization import plot_edf\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_slice\n",
    "\n",
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize high-dimensional parameter relationships. \n",
    "plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hyperparameter relationships.\n",
    "fig = plot_contour(study, params=['batch_size', 'lr', 'encoder_dropout', 'decoder_dropout'])\n",
    "fig.update_layout(width=800, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize individual hyperparameters as slice plot.\n",
    "plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter importances.\n",
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn which hyperparameters are affecting the trial duration with hyperparameter importance.\n",
    "optuna.visualization.plot_param_importances(\n",
    "    study, target=lambda t: t.duration.total_seconds(), target_name=\"duration\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize empirical distribution function of the objective.\n",
    "plot_edf(study)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test performance of the optimized plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "report = Benchmarks.evaluate(\n",
    "    [(\"test\", PLUGIN, best_params)],\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    repeats=1,\n",
    "    metrics={\"detection\": [\"detection_mlp\", \"detection_xgb\"]},  # DELETE THIS LINE FOR ALL METRICS\n",
    ")\n",
    "Benchmarks.print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement towards Machine learning and AI for medicine, you can do so in the following ways!\n",
    "\n",
    "### Star [Synthcity](https://github.com/vanderschaarlab/synthcity) on GitHub\n",
    "\n",
    "- The easiest way to help our community is just by starring the Repos! This helps raise awareness of the tools we're building.\n",
    "\n",
    "\n",
    "### Checkout other projects from vanderschaarlab\n",
    "- [HyperImpute](https://github.com/vanderschaarlab/hyperimpute)\n",
    "- [AutoPrognosis](https://github.com/vanderschaarlab/autoprognosis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthcity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
