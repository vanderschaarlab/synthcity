{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733120b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_dataset\n",
    "from synthcity.utils.serialization import save_to_file, load_from_file\n",
    "from synthcity.plugins import Plugins\n",
    "from synthcity.benchmark import Benchmarks\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(\"output\")\n",
    "prefix = \"privacy.identifiability_score\"\n",
    "\n",
    "metrics = [\n",
    "    \"sanity.data_mismatch.score\",\n",
    "    \"sanity.common_rows_proportion.score\",\n",
    "    \"sanity.nearest_syn_neighbor_distance.mean\",\n",
    "    \"sanity.close_values_probability.score\",\n",
    "    \"sanity.distant_values_probability.score\",\n",
    "    \"stats.jensenshannon_dist.marginal\",\n",
    "    \"stats.chi_squared_test.marginal\",\n",
    "    \"stats.feature_corr.joint\",\n",
    "    \"stats.inv_kl_divergence.marginal\",\n",
    "    \"stats.ks_test.marginal\",\n",
    "    \"stats.max_mean_discrepancy.joint\",\n",
    "    \"stats.wasserstein_dist.joint\",\n",
    "    \"stats.prdc.precision\",\n",
    "    \"stats.prdc.recall\",\n",
    "    \"stats.prdc.density\",\n",
    "    \"stats.prdc.coverage\",\n",
    "    \"stats.alpha_precision.delta_precision_alpha\",\n",
    "    \"stats.alpha_precision.delta_coverage_beta\",\n",
    "    \"stats.alpha_precision.authenticity\",\n",
    "    \"performance.linear_model.gt\",\n",
    "    \"performance.linear_model.syn_id\",\n",
    "    \"performance.linear_model.syn_ood\",\n",
    "    \"performance.mlp.gt\",\n",
    "    \"performance.mlp.syn_id\",\n",
    "    \"performance.mlp.syn_ood\",\n",
    "    \"performance.xgb.gt\",\n",
    "    \"performance.xgb.syn_id\",\n",
    "    \"performance.xgb.syn_ood\",\n",
    "    \"detection.detection_xgb.mean\",\n",
    "    \"detection.detection_mlp.mean\",\n",
    "    \"detection.detection_gmm.mean\",\n",
    "    \"privacy.delta-presence.score\",\n",
    "    \"privacy.k-anonymization.gt\",\n",
    "    \"privacy.k-anonymization.syn\",\n",
    "    \"privacy.k-map.score\",\n",
    "    \"privacy.distinct l-diversity.gt\",\n",
    "    \"privacy.distinct l-diversity.syn\",\n",
    "    \"privacy.identifiability_score.score\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_metric(\n",
    "    dataset: str,\n",
    "    metric: str,\n",
    "    models=[\n",
    "        \"baseline_adsgan\",\n",
    "        \"baseline_ctgan\",\n",
    "        \"baseline_tvae\",\n",
    "        \"baseline_privbayes\",\n",
    "        \"baseline_nflow\",\n",
    "        \"survival_survival_gan\",\n",
    "    ],\n",
    "):\n",
    "    results = []\n",
    "    for name in models:\n",
    "        bkp = out_dir / f\"{prefix}.{dataset}_{name}.bkp\"\n",
    "        df, duration_col, event_col, time_horizons = get_dataset(dataset)\n",
    "\n",
    "        try:\n",
    "            score = load_from_file(bkp)\n",
    "        except BaseException as e:\n",
    "            print(\"failed to load file\", e)\n",
    "            continue\n",
    "\n",
    "        for model in score:\n",
    "            method_score = score[model]\n",
    "            local_df = method_score.loc[metric].copy()\n",
    "            local_df[\"model\"] = model\n",
    "            results.append(local_df.to_frame().T)\n",
    "\n",
    "    return pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc182be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b0ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"once\")\n",
    "\n",
    "# sns.set_style(\"white\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7740a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "fontsize = 10\n",
    "text_kwargs = dict(fontsize=fontsize)\n",
    "\n",
    "def generate_plot_for_ax(ax, title, data):\n",
    "    datasets = data[\"dataset\"].unique()\n",
    "    datasets_cnt = len(datasets)\n",
    "\n",
    "    barWidth = 1\n",
    "\n",
    "    max_val = 0\n",
    "    idx = 0\n",
    "\n",
    "    models = [\n",
    "        \"adsgan\",\n",
    "        \"ctgan\",\n",
    "        \"tvae\",\n",
    "        \"privbayes\",\n",
    "        \"nflow\",\n",
    "        \"survival_gan\",\n",
    "    ]\n",
    "    cases = [\n",
    "        (\"#30a2da\", \"adsgan\"),\n",
    "        (\"#ff9cde\", \"ctgan\"),\n",
    "        (\"#e5ae38\", \"tvae\"),\n",
    "        (\"#6d904f\", \"privbayes\"),\n",
    "        (\"#8b8b8b\", \"nflow\"),\n",
    "        (\"#D23e4e\", \"survival_gan\"),\n",
    "    ]\n",
    "    models_cnt = len(models)\n",
    "\n",
    "    maxval = 0\n",
    "\n",
    "    for color, model in cases:\n",
    "        base_mod_mean = data[data[\"model\"] == model][\"mean\"]\n",
    "        base_mod_std = data[data[\"model\"] == model][\"std\"]\n",
    "        if len(base_mod_mean) == 0:\n",
    "            print(\"invalid model\", model)\n",
    "            continue\n",
    "\n",
    "        edgecolor = \"k\"\n",
    "\n",
    "        idxs = [\n",
    "            (idx + ((models_cnt + 1) * r)) * barWidth for r in range(len(base_mod_mean))\n",
    "        ]\n",
    "        ax.bar(\n",
    "            idxs,\n",
    "            base_mod_mean.values,\n",
    "            yerr=base_mod_std.values,\n",
    "            width=barWidth,\n",
    "            label=model,\n",
    "            edgecolor=edgecolor,\n",
    "            color=color,\n",
    "        )\n",
    "\n",
    "        maxval = max(maxval, max((base_mod_mean + base_mod_std).values))\n",
    "        idx += 1\n",
    "\n",
    "    ax.legend(\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 1.2),\n",
    "        ncol=models_cnt,\n",
    "        prop={\"size\": fontsize},\n",
    "    )\n",
    "\n",
    "    pretty_data = [map_dataset(d) for d in datasets]\n",
    "    \n",
    "    ax.set_xticks(\n",
    "        [(models_cnt + 1) * r + int(models_cnt / 2) for r in range(datasets_cnt)],\n",
    "        pretty_data,\n",
    "        rotation=0,\n",
    "        fontsize=fontsize,\n",
    "    )\n",
    "    \n",
    "    #ax.set_yticks([0.2 * v for v in range(int(10 * maxval) + 2)], fontsize=100)\n",
    "    ax.set_ylabel(title, fontsize=fontsize)\n",
    "\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=fontsize)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def generate_plot(title, data):\n",
    "    # ['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast',\n",
    "    #'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', 'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', 'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', 'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10']\n",
    "\n",
    "    plt.style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "    offset = len(data)\n",
    "    fig, axs = plt.subplots(1, figsize=(3 * len(data[\"dataset\"].unique()), 3))\n",
    "\n",
    "    generate_plot_for_ax(axs, title, data)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.35)\n",
    "    plt.savefig(f\"diagrams/metrics_{title}.png\")\n",
    "    plt.savefig(f\"diagrams/metrics_{title}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_performance_table(title, df, datasets):\n",
    "    str_repr = [\n",
    "        f\"{round(m, 4)} +/- {round(s, 4)}\"\n",
    "        for m, s in zip(df[\"mean\"].values, df[\"std\"].values)\n",
    "    ]\n",
    "    df[\"str\"] = str_repr\n",
    "    df = df.drop(columns=[\"mean\", \"std\"])\n",
    "\n",
    "    piv = df.pivot_table(\n",
    "        values=\"str\", index=df[\"model\"], columns=\"dataset\", aggfunc=\"first\"\n",
    "    )\n",
    "    display(piv[datasets])\n",
    "\n",
    "def extract_performance(\n",
    "    datasets: list, title: str, metrics: list, direction: str = \"maximize\"\n",
    "):\n",
    "    headers = [\"model\", \"mean\", \"std\", \"dataset\"]\n",
    "    res_df = pd.DataFrame([], columns=headers)\n",
    "\n",
    "    for dataset in datasets:\n",
    "        results = []\n",
    "\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                local_metrics = get_metric(dataset, metric)\n",
    "\n",
    "                results.append(local_metrics.reset_index(drop=True))\n",
    "            except BaseException as e:\n",
    "                raise\n",
    "                print(\"get metric failed\", e)\n",
    "                continue\n",
    "\n",
    "        if len(results) == 0:\n",
    "            continue\n",
    "\n",
    "        for idx, row in results[0].iterrows():\n",
    "            local_results = pd.concat(\n",
    "                [v.iloc[idx].to_frame().T for v in results], ignore_index=True\n",
    "            )\n",
    "\n",
    "            if direction == \"maximize\":\n",
    "                best_result = local_results[\n",
    "                    local_results[\"mean\"] == local_results[\"mean\"].max()\n",
    "                ]\n",
    "            elif direction == \"minimize\":\n",
    "                best_result = local_results[\n",
    "                    local_results[\"mean\"] == local_results[\"mean\"].min()\n",
    "                ]\n",
    "            else:\n",
    "                raise RuntimeError(f\"invalid direction {direction}\")\n",
    "\n",
    "            local_df = pd.DataFrame(\n",
    "                [\n",
    "                    [\n",
    "                        best_result[\"model\"].values[0],\n",
    "                        best_result[\"mean\"].values[0],\n",
    "                        best_result[\"stddev\"].values[0],\n",
    "                        dataset,\n",
    "                    ]\n",
    "                ],\n",
    "                columns=headers,\n",
    "            )\n",
    "            res_df = pd.concat([res_df, local_df], ignore_index=True)\n",
    "\n",
    "    return res_df\n",
    "\n",
    "def plot_performance(\n",
    "    datasets: list, title: str, metrics: list, direction: str = \"maximize\"\n",
    "):\n",
    "    res_df = extract_performance(datasets, title, metrics, direction)\n",
    "    \n",
    "    print(\"Scenario\", title)\n",
    "    print_performance_table(title, res_df, datasets)\n",
    "    generate_plot(title, res_df)\n",
    "\n",
    "    print(\"=\" * 100)\n",
    "    return res_df\n",
    "\n",
    "def plot_performance_with_baseline(\n",
    "    datasets: list, title: str, metrics: list, baseline_metrics: list,  direction: str = \"maximize\"\n",
    "):\n",
    "    res_df = extract_performance(datasets, title, metrics, direction)\n",
    "    res_base_df = extract_performance(datasets, title, baseline_metrics, direction)\n",
    "    \n",
    "    print(\"Scenario\", title, res_base_df)\n",
    "    print_performance_table(title, res_df, datasets)\n",
    "    generate_plot(title, res_df)\n",
    "\n",
    "    print(\"=\" * 100)\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549c43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b87d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62dc5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3d23f67",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d76f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8812e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0293ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5363d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datasets = [\"aids\" ]\n",
    "\n",
    "for title, direction, metrics in [\n",
    "     (\"Identifiability\", \"minimize\", [\"privacy.identifiability_score.score\"]),\n",
    "]:\n",
    "\n",
    "    plot_performance(datasets, direction=direction, title=title, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd2f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a5333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
