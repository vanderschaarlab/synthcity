{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733120b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pycox import datasets\n",
    "from lifelines.datasets import load_rossi\n",
    "from sksurv.datasets import (\n",
    "    load_aids,\n",
    "    load_breast_cancer,\n",
    "    load_flchain,\n",
    "    load_gbsg2,\n",
    "    load_whas500,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def get_dataset(name: str):\n",
    "    if name == \"metabric\":\n",
    "        df = datasets.metabric.read_df()\n",
    "    elif name == \"support\":\n",
    "        df = datasets.support.read_df()\n",
    "    elif name == \"gbsg\":\n",
    "        df = datasets.gbsg.read_df()\n",
    "    elif name == \"rossi\":\n",
    "        df = load_rossi()\n",
    "        df = df.rename(columns={\"week\": \"duration\", \"arrest\": \"event\"})\n",
    "    elif name == \"aids\":\n",
    "        X, Y = load_aids()\n",
    "        Y_unp = np.array(Y, dtype=[(\"event\", \"int\"), (\"duration\", \"float\")])\n",
    "        df = X.copy()\n",
    "        df[\"event\"] = Y_unp[\"event\"]\n",
    "        df[\"duration\"] = Y_unp[\"duration\"]\n",
    "    elif name == \"flchain\":\n",
    "        X, Y = load_flchain()\n",
    "        Y_unp = np.array(Y, dtype=[(\"event\", \"int\"), (\"duration\", \"float\")])\n",
    "        df = X.copy()\n",
    "        df[\"event\"] = Y_unp[\"event\"]\n",
    "        df[\"duration\"] = Y_unp[\"duration\"]\n",
    "    elif name == \"gbsg2\":\n",
    "        X, Y = load_gbsg2()\n",
    "        Y_unp = np.array(Y, dtype=[(\"event\", \"int\"), (\"duration\", \"float\")])\n",
    "        df = X.copy()\n",
    "        df[\"event\"] = Y_unp[\"event\"]\n",
    "        df[\"duration\"] = Y_unp[\"duration\"]\n",
    "    elif name == \"whas500\":\n",
    "        X, Y = load_whas500()\n",
    "        Y_unp = np.array(Y, dtype=[(\"event\", \"int\"), (\"duration\", \"float\")])\n",
    "        df = X.copy()\n",
    "        df[\"event\"] = Y_unp[\"event\"]\n",
    "        df[\"duration\"] = Y_unp[\"duration\"]\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype.name in [\"object\", \"category\"]:\n",
    "            df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "    duration_col = \"duration\"\n",
    "    event_col = \"event\"\n",
    "\n",
    "    X = df.drop(columns=[duration_col, event_col])\n",
    "    Y = df[event_col]\n",
    "    T = df[duration_col]\n",
    "\n",
    "    time_horizons = np.linspace(T.min(), T.max(), num=5)[1:-1]\n",
    "\n",
    "    return df, X, T, Y, time_horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f7740a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthcity.plugins import Plugins\n",
    "from adjutorium.utils.tester import evaluate_survival_estimator\n",
    "from adjutorium.plugins.prediction.risk_estimation import RiskEstimation\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def constant_columns(dataframe: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    Drops constant value columns of pandas dataframe.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for column in dataframe.columns:\n",
    "        if len(dataframe[column].unique()) == 1:\n",
    "            result.append(column)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _train_and_evaluate(\n",
    "    X_train, T_train, Y_train, X_test, T_test, Y_test, time_horizons, n_folds=3\n",
    "):\n",
    "    predictor = RiskEstimation().get(\"survival_xgboost\")\n",
    "    n_folds = 3\n",
    "\n",
    "    const_cols = constant_columns(X_train)\n",
    "    X_train = X_train.drop(columns=const_cols)\n",
    "    X_test = X_test.drop(columns=const_cols)\n",
    "    try:\n",
    "        predictor.fit(X_train, T_train, Y_train)\n",
    "\n",
    "        return evaluate_survival_estimator(\n",
    "            [predictor] * n_folds,\n",
    "            X_test,\n",
    "            T_test,\n",
    "            Y_test,\n",
    "            time_horizons=time_horizons,\n",
    "            n_folds=n_folds,\n",
    "            pretrained=True,\n",
    "        )[\"str\"]\n",
    "\n",
    "    except BaseException:\n",
    "        return {\n",
    "            \"c_index\": \"0 +/-0\",\n",
    "            \"aucroc\": \"0 +/- 0\",\n",
    "            \"brier_score\": \"1 +/- 0\",\n",
    "        }\n",
    "\n",
    "\n",
    "def _fold_evaluate(\n",
    "    X_test,\n",
    "    T_test,\n",
    "    Y_test,\n",
    "    time_horizons,\n",
    "    n_folds=3,\n",
    "):\n",
    "    predictor = RiskEstimation().get(\"survival_xgboost\")\n",
    "    n_folds = 3\n",
    "\n",
    "    const_cols = constant_columns(X_test)\n",
    "    X_test = X_test.drop(columns=const_cols)\n",
    "\n",
    "    try:\n",
    "        return evaluate_survival_estimator(\n",
    "            predictor,\n",
    "            X_test,\n",
    "            T_test,\n",
    "            Y_test,\n",
    "            time_horizons=time_horizons,\n",
    "            n_folds=n_folds,\n",
    "        )[\"str\"]\n",
    "    except BaseException:\n",
    "        return {\n",
    "            \"c_index\": \"0 +/-0\",\n",
    "            \"aucroc\": \"0 +/- 0\",\n",
    "            \"brier_score\": \"1 +/- 0\",\n",
    "        }\n",
    "\n",
    "\n",
    "def evaluate_surv_generation(generative_method: str, dataset: str):\n",
    "    df, X_real, T_real, Y_real, time_horizons = get_dataset(dataset)\n",
    "\n",
    "    syn_model = Plugins().get(generative_method)\n",
    "    syn_model.fit(df)\n",
    "\n",
    "    df_generated = syn_model.generate()\n",
    "\n",
    "    X_syn = df_generated.drop(columns=[\"duration\", \"event\"])\n",
    "    Y_syn = df_generated[\"event\"]\n",
    "    T_syn = df_generated[\"duration\"]\n",
    "\n",
    "    real_real_score = _fold_evaluate(X_real, T_real, Y_real, time_horizons)\n",
    "    syn_syn_score = _fold_evaluate(X_syn, T_syn, Y_syn, time_horizons)\n",
    "    real_syn_score = _train_and_evaluate(\n",
    "        X_real, T_real, Y_real, X_syn, T_syn, Y_syn, time_horizons\n",
    "    )\n",
    "    syn_real_score = _train_and_evaluate(\n",
    "        X_syn, T_syn, Y_syn, X_real, T_real, Y_real, time_horizons\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"real_real\": real_real_score,\n",
    "        \"syn_syn\": syn_syn_score,\n",
    "        \"real_syn\": real_syn_score,\n",
    "        \"syn_real\": syn_real_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0619b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def expected_time_error(T, E, predicted_event_time):\n",
    "    censored_err = T[E == 0] - predicted_event_time[E == 0]\n",
    "    censored_err[censored_err < 0] = 0\n",
    "\n",
    "    return sqrt(\n",
    "        mean_squared_error(T[E == 1], predicted_event_time[E == 1])\n",
    "        + np.mean(censored_err ** 2)\n",
    "    )\n",
    "\n",
    "\n",
    "def ranking_error(T, E, predicted_time):\n",
    "    rank_errs = 0\n",
    "\n",
    "    for idx in range(len(T)):\n",
    "        actual_order = (T[E == 1] <= T[idx]).astype(int).squeeze()\n",
    "        pred_order = (\n",
    "            (predicted_time[E == 1] <= predicted_time[idx]).astype(int).squeeze()\n",
    "        )\n",
    "        rank_errs += mean_squared_error(pred_order, actual_order)\n",
    "\n",
    "    return rank_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de7fa67",
   "metadata": {},
   "source": [
    "## AIDS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2222b71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>cd4</th>\n",
       "      <th>hemophil</th>\n",
       "      <th>ivdrug</th>\n",
       "      <th>karnof</th>\n",
       "      <th>priorzdv</th>\n",
       "      <th>raceth</th>\n",
       "      <th>sex</th>\n",
       "      <th>strat2</th>\n",
       "      <th>tx</th>\n",
       "      <th>txgrp</th>\n",
       "      <th>event</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>189.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.0</td>\n",
       "      <td>149.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>287.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.0</td>\n",
       "      <td>23.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>286.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>44.0</td>\n",
       "      <td>65.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>41.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>43.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>272.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>44.0</td>\n",
       "      <td>282.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>44.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1151 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age    cd4  hemophil  ivdrug  karnof  priorzdv  raceth  sex  strat2  \\\n",
       "0     34.0  169.0         0       0       0      39.0       0    0       1   \n",
       "1     34.0  149.5         0       0       3      15.0       1    1       1   \n",
       "2     20.0   23.5         1       0       0       9.0       0    0       0   \n",
       "3     48.0   46.0         0       0       3      53.0       0    0       1   \n",
       "4     46.0   10.0         0       2       3      12.0       0    0       0   \n",
       "...    ...    ...       ...     ...     ...       ...     ...  ...     ...   \n",
       "1146  44.0   65.5         0       0       0     103.0       0    0       1   \n",
       "1147  41.0    7.5         0       0       2      20.0       1    0       0   \n",
       "1148  43.0  170.0         0       2       3      27.0       1    0       1   \n",
       "1149  44.0  282.5         0       2       2      12.0       0    0       1   \n",
       "1150  44.0  120.0         0       0       3      26.0       0    0       1   \n",
       "\n",
       "      tx  txgrp  event  duration  \n",
       "0      0      0      0     189.0  \n",
       "1      0      0      0     287.0  \n",
       "2      1      1      0     242.0  \n",
       "3      0      0      0     199.0  \n",
       "4      1      1      0     286.0  \n",
       "...   ..    ...    ...       ...  \n",
       "1146   1      1      0     273.0  \n",
       "1147   1      1      1      47.0  \n",
       "1148   0      0      0     272.0  \n",
       "1149   0      0      0     192.0  \n",
       "1150   1      1      0     132.0  \n",
       "\n",
       "[1151 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df, X_real, T_real, Y_real, time_horizons = get_dataset(\"aids\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78387c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>cd4</th>\n",
       "      <th>hemophil</th>\n",
       "      <th>ivdrug</th>\n",
       "      <th>karnof</th>\n",
       "      <th>priorzdv</th>\n",
       "      <th>raceth</th>\n",
       "      <th>sex</th>\n",
       "      <th>strat2</th>\n",
       "      <th>tx</th>\n",
       "      <th>txgrp</th>\n",
       "      <th>event</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>33.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>35.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>298.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>33.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>47.0</td>\n",
       "      <td>33.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>42.0</td>\n",
       "      <td>35.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>56.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>45.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>39.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>174.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>41.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age   cd4  hemophil  ivdrug  karnof  priorzdv  raceth  sex  strat2  tx  \\\n",
       "13    33.0  16.0         0       0       2      24.0       1    1       0   0   \n",
       "16    35.0  17.5         0       0       3      40.0       2    0       0   0   \n",
       "22    33.0  23.0         0       0       3       6.0       2    0       0   0   \n",
       "29    31.0  11.0         0       0       3      12.0       1    0       0   0   \n",
       "56    47.0  33.5         0       0       2      28.0       2    0       0   0   \n",
       "...    ...   ...       ...     ...     ...       ...     ...  ...     ...  ..   \n",
       "1075  42.0  35.5         0       0       3      32.0       0    0       0   0   \n",
       "1106  56.0  63.0         0       0       2      48.0       1    0       1   0   \n",
       "1120  45.0  64.0         0       0       2      30.0       0    1       1   0   \n",
       "1126  39.0  18.0         0       0       2      22.0       2    0       0   1   \n",
       "1147  41.0   7.5         0       0       2      20.0       1    0       0   1   \n",
       "\n",
       "      txgrp  event  duration  \n",
       "13        0      1     206.0  \n",
       "16        0      1     298.0  \n",
       "22        0      1     190.0  \n",
       "29        0      1      82.0  \n",
       "56        0      1      61.0  \n",
       "...     ...    ...       ...  \n",
       "1075      0      1     186.0  \n",
       "1106      0      1     114.0  \n",
       "1120      0      1      20.0  \n",
       "1126      1      1     174.0  \n",
       "1147      1      1      47.0  \n",
       "\n",
       "[96 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"event\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb6c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "331f8bc4",
   "metadata": {},
   "source": [
    "# COX PH expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "97635d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lifelines.CoxPHFitter: fitted with 1151 total observations, 1055 right-censored observations>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "\n",
    "model = CoxPHFitter()\n",
    "\n",
    "model.fit(df, \"duration\", \"event\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "799be378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       359.003910\n",
       "1       354.991700\n",
       "2       349.654394\n",
       "3       314.187092\n",
       "4       334.745364\n",
       "           ...    \n",
       "1146    349.480252\n",
       "1147    322.003981\n",
       "1148    358.798987\n",
       "1149    362.359435\n",
       "1150    355.562560\n",
       "Length: 1151, dtype: float64"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expectation using trapezoidal rule\n",
    "from scipy.integrate import trapz\n",
    "\n",
    "surv_f = model.predict_survival_function(df)\n",
    "\n",
    "coxph_expectation = pd.Series(\n",
    "    trapz(surv_f.values.T, surv_f.index), index=surv_f.T.index\n",
    ")\n",
    "\n",
    "coxph_expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "7f53076f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228.63227491698547"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_time_error(df[\"duration\"], df[\"event\"], coxph_expectation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "7679cc2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362.12499999999983"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_error(df[\"duration\"], df[\"event\"], coxph_expectation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "af3a1e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9287576020851434"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(T <= coxph_expectation).sum() / len(coxph_expectation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf5f5b",
   "metadata": {},
   "source": [
    "## Random survival forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904c8c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "0803f557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomSurvivalForest()"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "\n",
    "model = RandomSurvivalForest()\n",
    "\n",
    "X = df.drop(columns=[\"event\", \"duration\"])\n",
    "y = df[[\"event\", \"duration\"]]\n",
    "\n",
    "y = [(df[\"event\"].iloc[i], df[\"duration\"].iloc[i]) for i in range(len(y))]\n",
    "y = np.array(y, dtype=[(\"status\", \"bool\"), (\"time\", \"<f8\")])\n",
    "\n",
    "model.fit(X.values, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c05ee758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       288.776432\n",
       "1       296.054711\n",
       "2       271.956147\n",
       "3       279.186137\n",
       "4       273.784811\n",
       "           ...    \n",
       "1146    292.560137\n",
       "1147    198.523644\n",
       "1148    295.494869\n",
       "1149    294.069708\n",
       "1150    296.988556\n",
       "Length: 1151, dtype: float64"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expectations = []\n",
    "for surv_f in model.predict_survival_function(X):\n",
    "    expectations.append(trapz(surv_f.y, surv_f.x))\n",
    "\n",
    "rsf_expected_time = pd.Series(expectations, index=X.index)\n",
    "\n",
    "rsf_expected_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "3b3dad7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136.85045142871962"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_time_error(df[\"duration\"], df[\"event\"], rsf_expected_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "e8adc3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169.7187499999999"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_error(df[\"duration\"], df[\"event\"], rsf_expected_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "28ee7c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6629018245004344"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(T <= rsf_expected_time).sum() / len(rsf_expected_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871dfa47",
   "metadata": {},
   "source": [
    "## Survival XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "60b05cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgbse.converters import convert_to_structured\n",
    "\n",
    "X = df.drop(columns=[\"event\", \"duration\"])\n",
    "E = df[\"event\"]\n",
    "T = df[\"duration\"]\n",
    "\n",
    "y = convert_to_structured(T, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "74ffe4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBSEDebiasedBCE(lr_params={'C': 0.001, 'max_iter': 500},\n",
       "                 xgb_params={'aft_loss_distribution': 'normal',\n",
       "                             'aft_loss_distribution_scale': 1,\n",
       "                             'booster': 'dart', 'colsample_bynode': 0.5,\n",
       "                             'eval_metric': 'aft-nloglik',\n",
       "                             'learning_rate': 0.05, 'max_depth': 8,\n",
       "                             'min_child_weight': 50,\n",
       "                             'objective': 'survival:aft', 'subsample': 0.5,\n",
       "                             'tree_method': 'hist'})"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgbse import XGBSEDebiasedBCE\n",
    "\n",
    "# fitting xgbse model\n",
    "xgbse_model = XGBSEDebiasedBCE()\n",
    "xgbse_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a9211934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       281.952605\n",
       "1       284.301821\n",
       "2       276.389738\n",
       "3       274.693388\n",
       "4       265.944473\n",
       "           ...    \n",
       "1146    281.949675\n",
       "1147    260.382612\n",
       "1148    281.461917\n",
       "1149    280.725644\n",
       "1150    283.447342\n",
       "Length: 1151, dtype: float64"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surv_f = xgbse_model.predict(X)\n",
    "\n",
    "xgb_expected_time = pd.Series(trapz(surv_f.values, surv_f.T.index), index=surv_f.index)\n",
    "\n",
    "xgb_expected_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "4765690c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183.16253863127292"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_time_error(T, E, xgb_expected_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f43935cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347.43750000000034"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_error(df[\"duration\"], df[\"event\"], xgb_expected_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e0361669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6177237185056472"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(T <= xgb_expected_time).sum() / len(xgb_expected_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6994ab11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b4c8baf",
   "metadata": {},
   "source": [
    "## Deephit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "470e8ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<plugin_deephit.py.DeepHitRiskEstimationPlugin at 0x7f978e9eedf0>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from adjutorium.plugins.prediction.risk_estimation import RiskEstimation\n",
    "\n",
    "model = RiskEstimation().get(\"deephit\", n_iter=5000)\n",
    "\n",
    "model.fit(X, T, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "53126a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       350.153704\n",
       "1       337.546935\n",
       "2       295.672012\n",
       "3       290.470147\n",
       "4       315.291187\n",
       "           ...    \n",
       "1146    343.135426\n",
       "1147    283.278161\n",
       "1148    343.990151\n",
       "1149    363.056695\n",
       "1150    331.392516\n",
       "Length: 1151, dtype: float64"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.net.eval()\n",
    "\n",
    "X_np = np.asarray(X).astype(\"float32\")\n",
    "\n",
    "surv_f = model.model.predict_surv_df(X_np)\n",
    "\n",
    "\n",
    "dh_expected_time = pd.Series(trapz(surv_f.T.values, surv_f.index), index=surv_f.T.index)\n",
    "\n",
    "dh_expected_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ac0dc03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204.13984294012428"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_time_error(T, E, dh_expected_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "115a453c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353.43749999999966"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_error(T, E, dh_expected_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "498a7b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8349261511728931"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(T <= dh_expected_time).sum() / len(dh_expected_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030915ca",
   "metadata": {},
   "source": [
    "## DATE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "b581be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "import torch\n",
    "from pydantic import validate_arguments\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# synthcity absolute\n",
    "import synthcity.logger as log\n",
    "from synthcity.utils.reproducibility import enable_reproducible_results\n",
    "from synthcity.plugins.models.mlp import MLP\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class TimeEventGAN(nn.Module):\n",
    "    @validate_arguments(config=dict(arbitrary_types_allowed=True))\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        n_units_latent: int,\n",
    "        generator_n_layers_hidden: int = 2,\n",
    "        generator_n_units_hidden: int = 250,\n",
    "        generator_nonlin: str = \"leaky_relu\",\n",
    "        generator_nonlin_out: Optional[List[Tuple[str, int]]] = None,\n",
    "        generator_n_iter: int = 5000,\n",
    "        generator_batch_norm: bool = False,\n",
    "        generator_dropout: float = 0,\n",
    "        generator_loss: Optional[Callable] = None,\n",
    "        generator_lr: float = 2e-4,\n",
    "        generator_weight_decay: float = 1e-3,\n",
    "        generator_residual: bool = True,\n",
    "        generator_opt_betas: tuple = (0.9, 0.999),\n",
    "        discriminator_n_layers_hidden: int = 3,\n",
    "        discriminator_n_units_hidden: int = 300,\n",
    "        discriminator_nonlin: str = \"leaky_relu\",\n",
    "        discriminator_n_iter: int = 1,\n",
    "        discriminator_batch_norm: bool = False,\n",
    "        discriminator_dropout: float = 0.1,\n",
    "        discriminator_loss: Optional[Callable] = None,\n",
    "        discriminator_lr: float = 2e-4,\n",
    "        discriminator_weight_decay: float = 1e-3,\n",
    "        discriminator_opt_betas: tuple = (0.9, 0.999),\n",
    "        batch_size: int = 100,\n",
    "        n_iter_print: int = 50,\n",
    "        seed: int = 0,\n",
    "        n_iter_min: int = 100,\n",
    "        clipping_value: int = 0,\n",
    "        lambda_gradient_penalty: float = 10,\n",
    "    ) -> None:\n",
    "        super(TimeEventGAN, self).__init__()\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.n_units_latent = n_units_latent\n",
    "        self.lambda_gradient_penalty = lambda_gradient_penalty\n",
    "\n",
    "        self.generator = MLP(\n",
    "            task_type=\"regression\",\n",
    "            n_units_in=n_features + n_units_latent,\n",
    "            n_units_out=1,  # time to event\n",
    "            n_layers_hidden=generator_n_layers_hidden,\n",
    "            n_units_hidden=generator_n_units_hidden,\n",
    "            nonlin=generator_nonlin,\n",
    "            nonlin_out=generator_nonlin_out,\n",
    "            n_iter=generator_n_iter,\n",
    "            batch_norm=generator_batch_norm,\n",
    "            dropout=generator_dropout,\n",
    "            loss=generator_loss,\n",
    "            seed=seed,\n",
    "            lr=generator_lr,\n",
    "            residual=generator_residual,\n",
    "            opt_betas=generator_opt_betas,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        self.discriminator = MLP(\n",
    "            task_type=\"regression\",\n",
    "            n_units_in=n_features + 1,\n",
    "            n_units_out=1,  # fake/true\n",
    "            n_layers_hidden=discriminator_n_layers_hidden,\n",
    "            n_units_hidden=discriminator_n_units_hidden,\n",
    "            nonlin=discriminator_nonlin,\n",
    "            nonlin_out=[(\"none\", 1)],\n",
    "            n_iter=discriminator_n_iter,\n",
    "            batch_norm=discriminator_batch_norm,\n",
    "            dropout=discriminator_dropout,\n",
    "            loss=discriminator_loss,\n",
    "            seed=seed,\n",
    "            lr=discriminator_lr,\n",
    "            opt_betas=discriminator_opt_betas,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # training\n",
    "        self.generator_n_iter = generator_n_iter\n",
    "        self.discriminator_n_iter = discriminator_n_iter\n",
    "        self.n_iter_print = n_iter_print\n",
    "        self.n_iter_min = n_iter_min\n",
    "        self.batch_size = batch_size\n",
    "        self.clipping_value = clipping_value\n",
    "\n",
    "        self.seed = seed\n",
    "        enable_reproducible_results(seed)\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        T: np.ndarray,\n",
    "        E: np.ndarray,\n",
    "    ) -> \"GAN\":\n",
    "        Xt = self._check_tensor(X)\n",
    "        Tt = self._check_tensor(T)\n",
    "        Et = self._check_tensor(E)\n",
    "\n",
    "        self._train(\n",
    "            Xt,\n",
    "            Tt,\n",
    "            Et,\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def generate(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = self._check_tensor(X).float()\n",
    "\n",
    "        return self(X).cpu().numpy()\n",
    "\n",
    "    @validate_arguments(config=dict(arbitrary_types_allowed=True))\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        self.generator.eval()\n",
    "\n",
    "        fixed_noise = torch.randn(len(X), self.n_units_latent, device=DEVICE)\n",
    "        with torch.no_grad():\n",
    "            return self.generator(torch.hstack([X, fixed_noise])).detach().cpu()\n",
    "\n",
    "    def dataloader(\n",
    "        self, X: torch.Tensor, T: torch.Tensor, E: torch.Tensor\n",
    "    ) -> DataLoader:\n",
    "        dataset = TensorDataset(X, T, E)\n",
    "\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, pin_memory=False)\n",
    "\n",
    "    def _generate_training_outputs(\n",
    "        self, X: torch.Tensor, T: torch.Tensor, E: torch.Tensor\n",
    "    ) -> tuple:\n",
    "        # Train with non-censored true batch\n",
    "        Xnc = X[E == 1].to(DEVICE)\n",
    "        Tnc = T[E == 1].to(DEVICE)\n",
    "        true_labels = torch.ones((len(Xnc),), device=DEVICE)\n",
    "        true_features_time = torch.hstack([Xnc, Tnc.reshape(-1, 1)])\n",
    "\n",
    "        true_output = self.discriminator(true_features_time).squeeze().float()\n",
    "\n",
    "        # Train with fake batch\n",
    "        noise = torch.randn(len(X), self.n_units_latent, device=DEVICE)\n",
    "        noise = torch.hstack([X, noise])\n",
    "        fake_T = self.generator(noise)\n",
    "\n",
    "        fake_labels = torch.zeros((len(X),), device=DEVICE)\n",
    "        fake_features_time = torch.hstack([X, fake_T.reshape(-1, 1)])\n",
    "\n",
    "        fake_output = self.discriminator(fake_features_time.detach()).squeeze()\n",
    "\n",
    "        return true_features_time, true_output, fake_features_time, fake_output\n",
    "\n",
    "    def _train_epoch_generator(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "        T: torch.Tensor,\n",
    "        E: torch.Tensor,\n",
    "    ) -> float:\n",
    "        # Update the G network\n",
    "        self.generator.optimizer.zero_grad()\n",
    "\n",
    "        # Evaluate noncensored error\n",
    "        Xnc = X[E == 1].to(DEVICE)\n",
    "        Tnc = T[E == 1].to(DEVICE)\n",
    "\n",
    "        batch_size = len(Xnc)\n",
    "\n",
    "        noise = torch.randn(batch_size, self.n_units_latent, device=DEVICE)\n",
    "        noncen_input = torch.hstack([Xnc, noise])\n",
    "        fake_T = self.generator(noncen_input)\n",
    "\n",
    "        errG_noncen = nn.MSELoss()(\n",
    "            fake_T, T\n",
    "        )  # fake_T should be == T for noncensored data\n",
    "\n",
    "        # Evaluate censored error\n",
    "        Xc = X[E == 0].to(DEVICE)\n",
    "        Tc = T[E == 0].to(DEVICE)\n",
    "\n",
    "        batch_size = len(Xc)\n",
    "\n",
    "        noise = torch.randn(batch_size, self.n_units_latent, device=DEVICE)\n",
    "        cen_input = torch.hstack([Xc, noise])\n",
    "        fake_T = self.generator(cen_input)\n",
    "\n",
    "        errG_cen = torch.mean(\n",
    "            nn.ReLU()(Tc - fake_T)\n",
    "        )  # fake_T should be >= T for censored data\n",
    "\n",
    "        # Discriminator loss\n",
    "        _, real_output, _, fake_output = self._generate_training_outputs(X, T, E)\n",
    "\n",
    "        errG_discr = torch.mean(real_output) - torch.mean(fake_output)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = errG_noncen + errG_cen + errG_discr\n",
    "\n",
    "        assert errG.isnan().sum() == 0\n",
    "\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "\n",
    "        # Update G\n",
    "        if self.clipping_value > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.generator.parameters(), self.clipping_value\n",
    "            )\n",
    "        self.generator.optimizer.step()\n",
    "\n",
    "        # Return loss\n",
    "        return errG.item()\n",
    "\n",
    "    def _train_epoch_discriminator(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "        T: torch.Tensor,\n",
    "        E: torch.Tensor,\n",
    "    ) -> float:\n",
    "        # Update the D network\n",
    "        errors = []\n",
    "\n",
    "        batch_size = min(self.batch_size, len(X))\n",
    "\n",
    "        for epoch in range(self.discriminator_n_iter):\n",
    "            self.discriminator.zero_grad()\n",
    "\n",
    "            (\n",
    "                true_features_time,\n",
    "                true_output,\n",
    "                fake_features_time,\n",
    "                fake_output,\n",
    "            ) = self._generate_training_outputs(X, T, E)\n",
    "\n",
    "            act = nn.Sigmoid()\n",
    "            errD = -torch.mean(torch.log(act(true_output))) - torch.mean(\n",
    "                torch.log(1 - act(fake_output))\n",
    "            )\n",
    "\n",
    "            assert errD.isnan().sum() == 0\n",
    "            errD.backward()\n",
    "\n",
    "            # Update D\n",
    "            if self.clipping_value > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.discriminator.parameters(), self.clipping_value\n",
    "                )\n",
    "            self.discriminator.optimizer.step()\n",
    "\n",
    "            errors.append(errD.item())\n",
    "\n",
    "        return np.mean(errors)\n",
    "\n",
    "    def _train_epoch(\n",
    "        self,\n",
    "        loader: DataLoader,\n",
    "    ) -> Tuple[float, float]:\n",
    "\n",
    "        G_losses = []\n",
    "        D_losses = []\n",
    "\n",
    "        for i, data in enumerate(loader):\n",
    "            G_losses.append(\n",
    "                self._train_epoch_generator(\n",
    "                    *data,\n",
    "                )\n",
    "            )\n",
    "            D_losses.append(\n",
    "                self._train_epoch_discriminator(\n",
    "                    *data,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return np.mean(G_losses), np.mean(D_losses)\n",
    "\n",
    "    def _train(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "        T: torch.Tensor,\n",
    "        E: torch.Tensor,\n",
    "    ) -> \"TimeEventGAN\":\n",
    "        X = self._check_tensor(X).float()\n",
    "        T = self._check_tensor(T).float()\n",
    "        E = self._check_tensor(E).long()\n",
    "\n",
    "        # Load Dataset\n",
    "        loader = self.dataloader(X, T, E)\n",
    "\n",
    "        # Train loop\n",
    "        for i in range(self.generator_n_iter):\n",
    "            g_loss, d_loss = self._train_epoch(loader)\n",
    "            # Check how the generator is doing by saving G's output on fixed_noise\n",
    "            if (i + 1) % self.n_iter_print == 0:\n",
    "                print(\n",
    "                    f\"[{i}/{self.generator_n_iter}]\\tLoss_D: {d_loss}\\tLoss_G: {g_loss}\"\n",
    "                )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _check_tensor(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        if isinstance(X, torch.Tensor):\n",
    "            return X.to(DEVICE)\n",
    "        else:\n",
    "            return torch.from_numpy(np.asarray(X)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d0460f44",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49/5000]\tLoss_D: 0.20019801488767067\tLoss_G: 11.527365763982138\n",
      "[99/5000]\tLoss_D: 0.12331527331843972\tLoss_G: 14.093079487482706\n",
      "[149/5000]\tLoss_D: 0.07148932960505287\tLoss_G: 15.957402308781942\n",
      "[199/5000]\tLoss_D: 0.09080297748247783\tLoss_G: 17.300039450327557\n",
      "[249/5000]\tLoss_D: 0.10174048303936918\tLoss_G: 19.185155232747395\n",
      "[299/5000]\tLoss_D: 0.06687433959450573\tLoss_G: 20.655827442804974\n",
      "[349/5000]\tLoss_D: 0.07761384453624487\tLoss_G: 21.532618681589764\n",
      "[399/5000]\tLoss_D: 0.06795037506769101\tLoss_G: 21.988333702087402\n",
      "[449/5000]\tLoss_D: 0.06315301696304232\tLoss_G: 23.156710465749104\n",
      "[499/5000]\tLoss_D: 0.05330320028588176\tLoss_G: 24.417380174001057\n",
      "[549/5000]\tLoss_D: 0.03193874141046157\tLoss_G: 25.697362740834553\n",
      "[599/5000]\tLoss_D: 0.044267649529501796\tLoss_G: 26.166507403055828\n",
      "[649/5000]\tLoss_D: 0.04674430992842341\tLoss_G: 26.074907302856445\n",
      "[699/5000]\tLoss_D: 0.026824829129812617\tLoss_G: 27.09940528869629\n",
      "[749/5000]\tLoss_D: 0.03517821369071802\tLoss_G: 28.588987509409588\n",
      "[799/5000]\tLoss_D: 0.029976016900036484\tLoss_G: 29.08811664581299\n",
      "[849/5000]\tLoss_D: 0.03510492900386453\tLoss_G: 28.575114568074543\n",
      "[899/5000]\tLoss_D: 0.032335895589009546\tLoss_G: 29.43676503499349\n",
      "[949/5000]\tLoss_D: 0.029827298732319225\tLoss_G: 29.040194352467854\n",
      "[999/5000]\tLoss_D: 0.02671557237044908\tLoss_G: 28.157873789469402\n",
      "[1049/5000]\tLoss_D: 0.022437989284905296\tLoss_G: 29.226810773213703\n",
      "[1099/5000]\tLoss_D: 0.023210503859445453\tLoss_G: 28.36096938451131\n",
      "[1149/5000]\tLoss_D: 0.020233206877795357\tLoss_G: 29.29019896189372\n",
      "[1199/5000]\tLoss_D: 0.033354362666917346\tLoss_G: 28.22259982426961\n",
      "[1249/5000]\tLoss_D: 0.02221295444178395\tLoss_G: 28.108963171641033\n",
      "[1299/5000]\tLoss_D: 0.026588229559517156\tLoss_G: 27.154139518737793\n",
      "[1349/5000]\tLoss_D: 0.019012717035366222\tLoss_G: 27.682987848917644\n",
      "[1399/5000]\tLoss_D: 0.028563041880261153\tLoss_G: 26.790467739105225\n",
      "[1449/5000]\tLoss_D: 0.02871888344331334\tLoss_G: 26.170273780822754\n",
      "[1499/5000]\tLoss_D: 0.021754705550847575\tLoss_G: 25.931514581044514\n",
      "[1549/5000]\tLoss_D: 0.021826236625202\tLoss_G: 26.04349406560262\n",
      "[1599/5000]\tLoss_D: 0.018650985934073105\tLoss_G: 25.342904726664226\n",
      "[1649/5000]\tLoss_D: 0.023599250610762585\tLoss_G: 25.40705156326294\n",
      "[1699/5000]\tLoss_D: 0.01797729093232192\tLoss_G: 25.12289635340373\n",
      "[1749/5000]\tLoss_D: 0.020590088475728408\tLoss_G: 25.537636756896973\n",
      "[1799/5000]\tLoss_D: 0.02606616759051879\tLoss_G: 24.94154119491577\n",
      "[1849/5000]\tLoss_D: 0.01944070679989333\tLoss_G: 25.152422428131104\n",
      "[1899/5000]\tLoss_D: 0.02197971703329434\tLoss_G: 25.97135003407796\n",
      "[1949/5000]\tLoss_D: 0.01922179763399375\tLoss_G: 25.17698033650716\n",
      "[1999/5000]\tLoss_D: 0.01719473294603328\tLoss_G: 25.628273328145344\n",
      "[2049/5000]\tLoss_D: 0.023156947378690045\tLoss_G: 25.790196259816486\n",
      "[2099/5000]\tLoss_D: 0.02225262191495858\tLoss_G: 25.891636689503986\n",
      "[2149/5000]\tLoss_D: 0.015495937181791911\tLoss_G: 25.273839632670086\n",
      "[2199/5000]\tLoss_D: 0.02196418454210895\tLoss_G: 26.26024643580119\n",
      "[2249/5000]\tLoss_D: 0.018445877843381215\tLoss_G: 25.629260857899983\n",
      "[2299/5000]\tLoss_D: 0.01773253534338437\tLoss_G: 26.097596009572346\n",
      "[2349/5000]\tLoss_D: 0.011228763939773975\tLoss_G: 25.606524626413982\n",
      "[2399/5000]\tLoss_D: 0.021383822235899668\tLoss_G: 25.093183358510334\n",
      "[2449/5000]\tLoss_D: 0.01597017088109472\tLoss_G: 25.520263195037842\n",
      "[2499/5000]\tLoss_D: 0.02327025142343094\tLoss_G: 24.900436878204346\n",
      "[2549/5000]\tLoss_D: 0.020825194670275476\tLoss_G: 25.492162704467773\n",
      "[2599/5000]\tLoss_D: 0.016715172134960692\tLoss_G: 25.05161650975545\n",
      "[2649/5000]\tLoss_D: 0.01803219018620439\tLoss_G: 25.90366792678833\n",
      "[2699/5000]\tLoss_D: 0.01634334257687442\tLoss_G: 25.162464141845703\n",
      "[2749/5000]\tLoss_D: 0.021689298485095303\tLoss_G: 24.834677855173748\n",
      "[2799/5000]\tLoss_D: 0.01547505974303931\tLoss_G: 24.393814245859783\n",
      "[2849/5000]\tLoss_D: 0.02060257166158408\tLoss_G: 24.94872538248698\n",
      "[2899/5000]\tLoss_D: 0.02334285987308249\tLoss_G: 24.482961177825928\n",
      "[2949/5000]\tLoss_D: 0.02103987248847261\tLoss_G: 24.822551409403484\n",
      "[2999/5000]\tLoss_D: 0.0192000994963261\tLoss_G: 24.969481468200684\n",
      "[3049/5000]\tLoss_D: 0.02414540839769567\tLoss_G: 26.058744589487713\n",
      "[3099/5000]\tLoss_D: 0.017944339274739225\tLoss_G: 24.766292572021484\n",
      "[3149/5000]\tLoss_D: 0.02577708710062628\tLoss_G: 24.676945845286053\n",
      "[3199/5000]\tLoss_D: 0.022519940801430494\tLoss_G: 25.21972115834554\n",
      "[3249/5000]\tLoss_D: 0.020459067677923787\tLoss_G: 25.172540346781414\n",
      "[3299/5000]\tLoss_D: 0.021214784763287753\tLoss_G: 24.36223618189494\n",
      "[3349/5000]\tLoss_D: 0.018215238019668806\tLoss_G: 24.885579268137615\n",
      "[3399/5000]\tLoss_D: 0.020028018140389275\tLoss_G: 24.67712990442912\n",
      "[3449/5000]\tLoss_D: 0.02533953751359756\tLoss_G: 24.7705241839091\n",
      "[3499/5000]\tLoss_D: 0.018165918862602364\tLoss_G: 25.284192085266113\n",
      "[3549/5000]\tLoss_D: 0.020809766564828653\tLoss_G: 24.970853010813396\n",
      "[3599/5000]\tLoss_D: 0.01623509102500975\tLoss_G: 24.79365046819051\n",
      "[3649/5000]\tLoss_D: 0.016624593816231936\tLoss_G: 25.013226985931396\n",
      "[3699/5000]\tLoss_D: 0.023011617343096685\tLoss_G: 24.64788230260213\n",
      "[3749/5000]\tLoss_D: 0.02186645697414254\tLoss_G: 24.266024271647137\n",
      "[3799/5000]\tLoss_D: 0.02060563270545875\tLoss_G: 25.110872268676758\n",
      "[3849/5000]\tLoss_D: 0.024799414483519893\tLoss_G: 24.986361185709637\n",
      "[3899/5000]\tLoss_D: 0.019835935323499143\tLoss_G: 25.170438289642334\n",
      "[3949/5000]\tLoss_D: 0.021827812830451876\tLoss_G: 25.141656239827473\n",
      "[3999/5000]\tLoss_D: 0.020995727720825624\tLoss_G: 24.677473068237305\n",
      "[4049/5000]\tLoss_D: 0.025072309479583055\tLoss_G: 25.253599007924397\n",
      "[4099/5000]\tLoss_D: 0.019489060835136723\tLoss_G: 25.04031229019165\n",
      "[4149/5000]\tLoss_D: 0.021658869799769793\tLoss_G: 24.570401032765705\n",
      "[4199/5000]\tLoss_D: 0.02320282016686785\tLoss_G: 25.15274492899577\n",
      "[4249/5000]\tLoss_D: 0.02061526811060806\tLoss_G: 24.683011690775555\n",
      "[4299/5000]\tLoss_D: 0.025442278274567798\tLoss_G: 24.742204348246258\n",
      "[4349/5000]\tLoss_D: 0.02601916132456002\tLoss_G: 24.628188769022625\n",
      "[4399/5000]\tLoss_D: 0.022034180622237425\tLoss_G: 24.97761821746826\n",
      "[4449/5000]\tLoss_D: 0.01866785951036339\tLoss_G: 24.864526589711506\n",
      "[4499/5000]\tLoss_D: 0.024169740829772007\tLoss_G: 24.614293098449707\n",
      "[4549/5000]\tLoss_D: 0.021257913923667122\tLoss_G: 24.83834997812907\n",
      "[4599/5000]\tLoss_D: 0.018782790498031925\tLoss_G: 25.065391222635906\n",
      "[4649/5000]\tLoss_D: 0.01981651761646693\tLoss_G: 24.894428730010986\n",
      "[4699/5000]\tLoss_D: 0.021747797591766965\tLoss_G: 24.570852915445965\n",
      "[4749/5000]\tLoss_D: 0.028192613826831803\tLoss_G: 24.75411017735799\n",
      "[4799/5000]\tLoss_D: 0.022655552776996046\tLoss_G: 24.633390108744305\n",
      "[4849/5000]\tLoss_D: 0.02028520645884176\tLoss_G: 24.468191623687744\n",
      "[4899/5000]\tLoss_D: 0.019015527417650446\tLoss_G: 25.243218421936035\n",
      "[4949/5000]\tLoss_D: 0.02322336534659068\tLoss_G: 24.701927661895752\n",
      "[4999/5000]\tLoss_D: 0.02163269975183842\tLoss_G: 24.54454819361369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimeEventGAN(\n",
       "  (generator): MLP(\n",
       "    (model): Sequential(\n",
       "      (0): ResidualLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=21, out_features=250, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=271, out_features=271, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (2): Linear(in_features=542, out_features=1, bias=True)\n",
       "    )\n",
       "    (loss): MSELoss()\n",
       "  )\n",
       "  (discriminator): MLP(\n",
       "    (model): Sequential(\n",
       "      (0): LinearLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=12, out_features=300, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (1): LinearLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.1, inplace=False)\n",
       "          (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (2): LinearLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.1, inplace=False)\n",
       "          (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (3): Linear(in_features=300, out_features=1, bias=True)\n",
       "      (4): MultiActivationHead()\n",
       "    )\n",
       "    (loss): MSELoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "model = TimeEventGAN(\n",
    "    n_features=X.shape[1],\n",
    "    n_units_latent=10,\n",
    ")\n",
    "\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "enc_X = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_T = MinMaxScaler()\n",
    "enc_T = scaler_T.fit_transform(T.values.reshape(-1, 1)).squeeze()\n",
    "\n",
    "model.fit(enc_X, enc_T, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "198edce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([314.0263 , 393.81577, 360.4408 , ..., 346.5795 , 443.94827,\n",
       "       355.42078], dtype=float32)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "enc_pred_T = model.generate(enc_X)\n",
    "\n",
    "adv_time_to_event = scaler_T.inverse_transform(enc_pred_T).squeeze()\n",
    "\n",
    "adv_time_to_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "4098bdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([267.54825, 245.98071, 262.05115, 243.53607, 250.33655, 228.9197 ,\n",
       "       257.41516, 269.35757, 286.9277 , 247.93243, 233.30562, 244.12299,\n",
       "       273.93744, 252.66742, 226.18564, 247.0547 , 252.74051, 252.9439 ,\n",
       "       244.32718, 228.18945, 268.00543, 284.67722, 279.92752, 254.25429,\n",
       "       231.64702, 242.30023, 272.16202, 258.86584, 298.59848, 252.22182,\n",
       "       276.31607, 308.05933, 318.5639 , 248.59091, 244.41428, 275.648  ,\n",
       "       253.58578, 247.47708, 286.98837, 239.87407, 270.72705, 240.30164,\n",
       "       241.15096, 261.2846 , 258.7759 , 237.62503, 246.4053 , 263.5498 ,\n",
       "       231.86751, 219.23193, 279.80328, 252.33907, 230.61731, 242.9455 ,\n",
       "       226.53604, 261.59644, 226.7726 , 238.3301 , 244.26926, 253.35579,\n",
       "       239.2181 , 252.02248, 288.96683, 267.7728 , 230.7128 , 235.33846,\n",
       "       280.22635, 240.31956, 241.65619, 222.62445, 250.12166, 256.84702,\n",
       "       232.65787, 243.95787, 243.41786, 265.9128 , 278.2363 , 260.14536,\n",
       "       260.04416, 251.6236 , 224.98022, 288.61652, 289.91235, 275.0925 ,\n",
       "       314.5952 , 267.19873, 300.93552, 253.3036 , 262.682  , 273.64554,\n",
       "       270.92017, 280.01498, 235.95445, 244.47893, 245.33925, 235.01884],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_time_to_event[E == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "bfadb476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170.14575064261015"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_time_error(T, E, adv_time_to_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a3a7f152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216.59374999999991"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_error(T, E, adv_time_to_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "96ab14e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8696785403996524"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(T <= adv_time_to_event).sum() / len(adv_time_to_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d480ce8",
   "metadata": {},
   "source": [
    "## NN prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d311b914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "import torch\n",
    "from pydantic import validate_arguments\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# synthcity absolute\n",
    "import synthcity.logger as log\n",
    "from synthcity.utils.reproducibility import enable_reproducible_results\n",
    "from synthcity.plugins.models.mlp import MLP\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class KaplanMeier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KaplanMeier, self).__init__()\n",
    "\n",
    "    def forward(self, T: torch.Tensor, E: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Estimages the Kaplan-Meier survival function\n",
    "        parameters:\n",
    "        - t: time steps\n",
    "        - e: whether death occured at time step (1) or not (0)\n",
    "        \"\"\"\n",
    "        T = self._check_tensor(T)\n",
    "        E = self._check_tensor(E)\n",
    "\n",
    "        Tnc = T[E == 1]\n",
    "\n",
    "        Tnc_unique, nc_counts = torch.unique(T[E == 1], return_counts=True)\n",
    "        T_unique, T_idxs, counts = torch.unique(\n",
    "            T, return_inverse=True, return_counts=True\n",
    "        )\n",
    "\n",
    "        result = torch.ones(1, requires_grad=True).to(DEVICE)\n",
    "\n",
    "        for i, t in enumerate(Tnc_unique):\n",
    "            di = nc_counts[i]\n",
    "            ni = torch.sum(counts[T_unique > t])\n",
    "\n",
    "            result *= 1 - di / ni\n",
    "        return result\n",
    "\n",
    "    def _check_tensor(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        if isinstance(X, torch.Tensor):\n",
    "            return X.to(DEVICE)\n",
    "        else:\n",
    "            return torch.from_numpy(np.asarray(X)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "74c538bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T[E == 1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d1f06c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9007], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KaplanMeier()\n",
    "\n",
    "model(T, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "a6a5f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "\n",
    "    Arguments:\n",
    "        indices: a list of indices\n",
    "        num_samples: number of samples to draw\n",
    "        callback_get_label: a callback-like function which takes two arguments - dataset and index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, T, E):\n",
    "        # if indices is not provided, all elements in the dataset will be considered\n",
    "        self.indices = list(range(X.shape[0]))\n",
    "\n",
    "        # if num_samples is not provided, draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices)\n",
    "\n",
    "        # distribution of classes in the dataset\n",
    "        df = pd.DataFrame()\n",
    "        df[\"label\"] = E.cpu().numpy()\n",
    "        df.index = self.indices\n",
    "        df = df.sort_index()\n",
    "\n",
    "        label_to_count = df[\"label\"].value_counts()\n",
    "\n",
    "        weights = 1.0 / label_to_count[df[\"label\"]]\n",
    "\n",
    "        self.weights = torch.DoubleTensor(weights.to_list())\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (\n",
    "            self.indices[i]\n",
    "            for i in torch.multinomial(self.weights, self.num_samples, replacement=True)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "d2807fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "import torch\n",
    "from pydantic import validate_arguments\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# synthcity absolute\n",
    "import synthcity.logger as log\n",
    "from synthcity.utils.reproducibility import enable_reproducible_results\n",
    "from synthcity.plugins.models.mlp import MLP\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class TimeEventNN(nn.Module):\n",
    "    @validate_arguments(config=dict(arbitrary_types_allowed=True))\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        n_layers_hidden: int = 2,\n",
    "        n_units_hidden: int = 250,\n",
    "        nonlin: str = \"leaky_relu\",\n",
    "        nonlin_out: Optional[List[Tuple[str, int]]] = None,\n",
    "        n_iter: int = 5000,\n",
    "        batch_norm: bool = False,\n",
    "        dropout: float = 0,\n",
    "        lr: float = 2e-4,\n",
    "        weight_decay: float = 1e-3,\n",
    "        residual: bool = True,\n",
    "        opt_betas: tuple = (0.9, 0.999),\n",
    "        batch_size: int = 1000,\n",
    "        n_iter_print: int = 500,\n",
    "        seed: int = 0,\n",
    "        n_iter_min: int = 100,\n",
    "        clipping_value: int = 0,\n",
    "        lambda_calibration=10,\n",
    "        lambda_regression_nc=10,\n",
    "        lambda_regression_c=1,\n",
    "        verbose: bool = False,\n",
    "    ) -> None:\n",
    "        super(TimeEventNN, self).__init__()\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.lambda_calibration = lambda_calibration\n",
    "        self.lambda_regression_nc = lambda_regression_nc\n",
    "        self.lambda_regression_c = lambda_regression_c\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.generator = MLP(\n",
    "            task_type=\"regression\",\n",
    "            n_units_in=n_features,\n",
    "            n_units_out=1,  # time to event\n",
    "            n_layers_hidden=n_layers_hidden,\n",
    "            n_units_hidden=n_units_hidden,\n",
    "            nonlin=nonlin,\n",
    "            nonlin_out=nonlin_out,\n",
    "            n_iter=n_iter,\n",
    "            batch_norm=batch_norm,\n",
    "            dropout=dropout,\n",
    "            seed=seed,\n",
    "            lr=lr,\n",
    "            residual=residual,\n",
    "            opt_betas=opt_betas,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # training\n",
    "        self.n_iter = n_iter\n",
    "        self.n_iter_print = n_iter_print\n",
    "        self.n_iter_min = n_iter_min\n",
    "        self.batch_size = batch_size\n",
    "        self.clipping_value = clipping_value\n",
    "\n",
    "        self.seed = seed\n",
    "        enable_reproducible_results(seed)\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        T: np.ndarray,\n",
    "        E: np.ndarray,\n",
    "    ) -> \"TimeEventNN\":\n",
    "        Xt = self._check_tensor(X)\n",
    "        Tt = self._check_tensor(T)\n",
    "        Et = self._check_tensor(E)\n",
    "\n",
    "        self._train(\n",
    "            Xt,\n",
    "            Tt,\n",
    "            Et,\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def generate(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = self._check_tensor(X).float()\n",
    "\n",
    "        return self(X).cpu().numpy()\n",
    "\n",
    "    @validate_arguments(config=dict(arbitrary_types_allowed=True))\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        self.generator.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            return self.generator(X).detach().cpu()\n",
    "\n",
    "    def dataloader(\n",
    "        self, X: torch.Tensor, T: torch.Tensor, E: torch.Tensor\n",
    "    ) -> DataLoader:\n",
    "        dataset = TensorDataset(X, T, E)\n",
    "        sampler = ImbalancedDatasetSampler(X, T, E)\n",
    "\n",
    "        return DataLoader(\n",
    "            dataset, batch_size=self.batch_size, sampler=sampler, pin_memory=False\n",
    "        )\n",
    "\n",
    "    def _train_epoch_generator(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "        T: torch.Tensor,\n",
    "        E: torch.Tensor,\n",
    "    ) -> float:\n",
    "        # Update the G network\n",
    "        self.generator.optimizer.zero_grad()\n",
    "\n",
    "        # Calculate G's loss based on noncensored data\n",
    "        errG_nc = self._loss_regression_nc(\n",
    "            X[E == 1], T[E == 1]\n",
    "        ) + self._loss_calibration(X[E == 1], T[E == 1])\n",
    "\n",
    "        # Calculate G's loss based on censored data\n",
    "        errG_c = self._loss_regression_c(X[E == 0], T[E == 0])\n",
    "\n",
    "        # Calculate total loss\n",
    "        errG = errG_nc + errG_c\n",
    "\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "\n",
    "        # Update G\n",
    "        if self.clipping_value > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.generator.parameters(), self.clipping_value\n",
    "            )\n",
    "        self.generator.optimizer.step()\n",
    "\n",
    "        # Return loss\n",
    "        return errG.item()\n",
    "\n",
    "    def _train_epoch(\n",
    "        self,\n",
    "        loader: DataLoader,\n",
    "    ) -> float:\n",
    "\n",
    "        G_losses = []\n",
    "\n",
    "        for i, data in enumerate(loader):\n",
    "            G_losses.append(\n",
    "                self._train_epoch_generator(\n",
    "                    *data,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return np.mean(G_losses)\n",
    "\n",
    "    def _train(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "        T: torch.Tensor,\n",
    "        E: torch.Tensor,\n",
    "    ) -> \"TimeEventGAN\":\n",
    "        X = self._check_tensor(X).float()\n",
    "        T = self._check_tensor(T).float()\n",
    "        E = self._check_tensor(E).long()\n",
    "\n",
    "        # Load Dataset\n",
    "        loader = self.dataloader(X, T, E)\n",
    "\n",
    "        # Train loop\n",
    "        for i in range(self.n_iter):\n",
    "            g_loss = self._train_epoch(loader)\n",
    "            # Check how the generator is doing by saving G's output on fixed_noise\n",
    "            if self.verbose and (i + 1) % self.n_iter_print == 0:\n",
    "                print(f\"[{i}/{self.n_iter}]\\tLoss_G: {g_loss}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _check_tensor(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        if isinstance(X, torch.Tensor):\n",
    "            return X.to(DEVICE)\n",
    "        else:\n",
    "            return torch.from_numpy(np.asarray(X)).to(DEVICE)\n",
    "\n",
    "    def _loss_calibration(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "        T: torch.Tensor,\n",
    "    ) -> float:\n",
    "        # Evaluate noncensored error\n",
    "        X = X.to(DEVICE)\n",
    "        T = T.to(DEVICE)\n",
    "        pred_T = self.generator(X).squeeze()\n",
    "\n",
    "        def _inner_dist(arr):\n",
    "            lhs = arr.view(-1, 1).repeat(1, len(arr))\n",
    "\n",
    "            return lhs - lhs.T\n",
    "\n",
    "        inner_T_dist = _inner_dist(T)\n",
    "        inner_pred_T_dist = _inner_dist(pred_T)\n",
    "\n",
    "        return self.lambda_calibration * nn.MSELoss()(inner_T_dist, inner_pred_T_dist)\n",
    "\n",
    "    def _loss_regression_c(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "        T: torch.Tensor,\n",
    "    ) -> float:\n",
    "        # Evaluate censored error\n",
    "        X = X.to(DEVICE)\n",
    "        T = T.to(DEVICE)\n",
    "        fake_T = self.generator(X)\n",
    "\n",
    "        errG_cen = torch.mean(\n",
    "            nn.ReLU()(T - fake_T)\n",
    "        )  # fake_T should be >= T for censored data\n",
    "\n",
    "        # Calculate G's loss based on this output\n",
    "        return self.lambda_regression_c * errG_cen\n",
    "\n",
    "    def _loss_regression_nc(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "        T: torch.Tensor,\n",
    "    ) -> float:\n",
    "        # Evaluate noncensored error\n",
    "        X = X.to(DEVICE)\n",
    "        T = T.to(DEVICE)\n",
    "        fake_T = self.generator(X)\n",
    "\n",
    "        errG_noncen = nn.MSELoss()(\n",
    "            fake_T, T\n",
    "        )  # fake_T should be == T for noncensored data\n",
    "\n",
    "        return self.lambda_regression_nc * errG_noncen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "3c1085c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[499/7000]\tLoss_G: 3.3376758098602295\n",
      "[999/7000]\tLoss_G: 2.5953210830688476\n",
      "[1499/7000]\tLoss_G: 3.282554006576538\n",
      "[1999/7000]\tLoss_G: 3.105288362503052\n",
      "[2499/7000]\tLoss_G: 3.2453786849975588\n",
      "[2999/7000]\tLoss_G: 2.88686466217041\n",
      "[3499/7000]\tLoss_G: 3.3921277046203615\n",
      "[3999/7000]\tLoss_G: 3.1902450561523437\n",
      "[4499/7000]\tLoss_G: 3.3084179878234865\n",
      "[4999/7000]\tLoss_G: 2.8367167949676513\n",
      "[5499/7000]\tLoss_G: 2.9377728939056396\n",
      "[5999/7000]\tLoss_G: 3.0153308391571043\n",
      "[6499/7000]\tLoss_G: 2.8836273193359374\n",
      "[6999/7000]\tLoss_G: 2.9327690601348877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "args = {\n",
    "    \"n_layers_hidden\": 5,\n",
    "    \"n_units_hidden\": 300,\n",
    "    \"nonlin\": \"leaky_relu\",\n",
    "    \"batch_norm\": False,\n",
    "    \"dropout\": 0.061771005405779913,\n",
    "    \"lr\": 0.001,\n",
    "    \"residual\": False,\n",
    "    \"batch_size\": 250,\n",
    "    \"lambda_calibration\": 10,\n",
    "    \"lambda_regression_nc\": 50,\n",
    "    \"lambda_regression_c\": 100,\n",
    "}\n",
    "\n",
    "model = TimeEventNN(\n",
    "    n_features=X.shape[1],\n",
    "    verbose=True,\n",
    "    n_iter=7000,\n",
    "    **args,\n",
    ")\n",
    "\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "enc_X = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_T = MinMaxScaler()\n",
    "enc_T = scaler_T.fit_transform(T.values.reshape(-1, 1)).squeeze()\n",
    "\n",
    "_ = model.fit(enc_X, enc_T, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "a62c60d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([135.50903 , 156.26138 , 130.72322 , 102.08743 ,  89.81493 ,\n",
       "       109.457184,  77.09829 , 107.370026, 126.5123  ,  94.76485 ,\n",
       "        81.10462 ,  97.82861 ,  78.832436, 124.84132 ,  77.28345 ,\n",
       "        84.33965 , 106.36029 ,  99.56727 , 162.57362 ,  81.21036 ,\n",
       "       103.52593 ,  81.37758 , 106.711044,  92.11203 , 115.14865 ,\n",
       "        85.97696 , 101.89047 , 109.74017 , 143.50134 , 103.52821 ,\n",
       "       100.02467 ,  77.67313 , 101.65781 , 145.27959 ,  93.36886 ,\n",
       "        95.60155 , 101.93185 , 102.141396,  82.05487 ,  77.1862  ,\n",
       "       159.59782 , 112.75272 ,  89.26897 , 111.00989 ,  76.352295,\n",
       "        84.64619 , 103.18254 ,  77.86974 , 111.73801 ,  92.1155  ,\n",
       "        87.00022 , 141.66734 , 140.58092 ,  79.11524 ,  78.059135,\n",
       "        96.59738 , 102.839584,  94.07241 ,  83.65452 ,  93.17826 ,\n",
       "        74.23955 , 143.58727 ,  75.559654, 122.65561 ,  82.35041 ,\n",
       "        80.23777 , 114.29236 , 131.49608 ,  93.977905,  75.604294,\n",
       "        82.44831 , 104.547455,  99.078705,  79.57662 ,  77.16882 ,\n",
       "       129.49007 , 144.25995 ,  92.66831 ,  95.15421 , 123.15925 ,\n",
       "       129.70297 , 151.44313 ,  79.13335 , 143.16566 , 103.493774,\n",
       "       112.04134 , 124.64146 , 116.67069 ,  76.205925, 135.1542  ,\n",
       "       109.17518 , 130.17882 , 104.19965 ,  77.51134 , 147.82997 ,\n",
       "        89.616776], dtype=float32)"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "enc_pred_T = model.generate(enc_X)\n",
    "\n",
    "nn_time_to_event = scaler_T.inverse_transform(enc_pred_T).squeeze()\n",
    "\n",
    "nn_time_to_event[E == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "467bb173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.56784196623553"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_time_error(T, E, nn_time_to_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "201dd1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133.76041666666669"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_error(T, E, nn_time_to_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "7734c303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9574283231972198"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(T <= nn_time_to_event).sum() / len(nn_time_to_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "c1573b4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-27 12:50:13,141]\u001b[0m A new study created in memory with name: no-name-6f00874f-ae6b-41aa-b635-dd2e29b8c49c\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:51:52,024]\u001b[0m Trial 0 finished with value: 213.61458333333343 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 250, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.18158716416518192, 'lr': 0.001, 'residual': True, 'batch_size': 250, 'lambda_calibration': 100, 'lambda_regression_nc': 1, 'lambda_regression_c': 100}. Best is trial 0 with value: 213.61458333333343.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:52:10,914]\u001b[0m Trial 1 finished with value: 318.7291666666671 and parameters: {'n_layers_hidden': 2, 'n_units_hidden': 250, 'nonlin': 'relu', 'batch_norm': True, 'dropout': 0.03171068850288972, 'lr': 0.001, 'residual': True, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 10, 'lambda_regression_c': 100}. Best is trial 0 with value: 213.61458333333343.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:52:27,938]\u001b[0m Trial 2 finished with value: 364.3645833333329 and parameters: {'n_layers_hidden': 3, 'n_units_hidden': 200, 'nonlin': 'relu', 'batch_norm': True, 'dropout': 0.03201180133393933, 'lr': 0.01, 'residual': True, 'batch_size': 500, 'lambda_calibration': 50, 'lambda_regression_nc': 50, 'lambda_regression_c': 10}. Best is trial 0 with value: 213.61458333333343.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:52:45,345]\u001b[0m Trial 3 finished with value: 514.8125 and parameters: {'n_layers_hidden': 4, 'n_units_hidden': 50, 'nonlin': 'relu', 'batch_norm': True, 'dropout': 0.12895197802383693, 'lr': 0.01, 'residual': False, 'batch_size': 500, 'lambda_calibration': 1, 'lambda_regression_nc': 100, 'lambda_regression_c': 1}. Best is trial 0 with value: 213.61458333333343.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:53:00,642]\u001b[0m Trial 4 finished with value: 161.28125 and parameters: {'n_layers_hidden': 2, 'n_units_hidden': 100, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.07437607866952729, 'lr': 0.01, 'residual': False, 'batch_size': 250, 'lambda_calibration': 50, 'lambda_regression_nc': 50, 'lambda_regression_c': 10}. Best is trial 4 with value: 161.28125.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:53:27,473]\u001b[0m Trial 5 finished with value: 407.70833333333394 and parameters: {'n_layers_hidden': 1, 'n_units_hidden': 200, 'nonlin': 'leaky_relu', 'batch_norm': True, 'dropout': 0.061800423367444714, 'lr': 0.0001, 'residual': True, 'batch_size': 100, 'lambda_calibration': 50, 'lambda_regression_nc': 1, 'lambda_regression_c': 10}. Best is trial 4 with value: 161.28125.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:54:03,086]\u001b[0m Trial 6 finished with value: 182.43749999999977 and parameters: {'n_layers_hidden': 3, 'n_units_hidden': 100, 'nonlin': 'relu', 'batch_norm': False, 'dropout': 0.17900196999879964, 'lr': 0.01, 'residual': True, 'batch_size': 100, 'lambda_calibration': 100, 'lambda_regression_nc': 100, 'lambda_regression_c': 10}. Best is trial 4 with value: 161.28125.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:54:41,687]\u001b[0m Trial 7 finished with value: 316.36458333333354 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 150, 'nonlin': 'relu', 'batch_norm': True, 'dropout': 0.031236596967963062, 'lr': 0.0001, 'residual': True, 'batch_size': 1000, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 50}. Best is trial 4 with value: 161.28125.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:54:58,079]\u001b[0m Trial 8 finished with value: 235.3125000000002 and parameters: {'n_layers_hidden': 4, 'n_units_hidden': 100, 'nonlin': 'relu', 'batch_norm': False, 'dropout': 0.022525220480132925, 'lr': 0.01, 'residual': True, 'batch_size': 500, 'lambda_calibration': 100, 'lambda_regression_nc': 1, 'lambda_regression_c': 50}. Best is trial 4 with value: 161.28125.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:55:28,227]\u001b[0m Trial 9 finished with value: 194.71875000000023 and parameters: {'n_layers_hidden': 4, 'n_units_hidden': 200, 'nonlin': 'relu', 'batch_norm': False, 'dropout': 0.13317365673286716, 'lr': 0.001, 'residual': True, 'batch_size': 250, 'lambda_calibration': 50, 'lambda_regression_nc': 1, 'lambda_regression_c': 50}. Best is trial 4 with value: 161.28125.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:55:37,892]\u001b[0m Trial 10 finished with value: 280.6666666666665 and parameters: {'n_layers_hidden': 1, 'n_units_hidden': 50, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.08627681205879781, 'lr': 0.01, 'residual': False, 'batch_size': 1000, 'lambda_calibration': 1, 'lambda_regression_nc': 50, 'lambda_regression_c': 1}. Best is trial 4 with value: 161.28125.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:56:05,406]\u001b[0m Trial 11 finished with value: 200.18749999999991 and parameters: {'n_layers_hidden': 2, 'n_units_hidden': 100, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.1897294624158764, 'lr': 0.01, 'residual': False, 'batch_size': 100, 'lambda_calibration': 100, 'lambda_regression_nc': 100, 'lambda_regression_c': 10}. Best is trial 4 with value: 161.28125.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:56:32,825]\u001b[0m Trial 12 finished with value: 217.73958333333331 and parameters: {'n_layers_hidden': 2, 'n_units_hidden': 100, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.14589649930366372, 'lr': 0.01, 'residual': False, 'batch_size': 100, 'lambda_calibration': 100, 'lambda_regression_nc': 100, 'lambda_regression_c': 10}. Best is trial 4 with value: 161.28125.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:57:05,101]\u001b[0m Trial 13 finished with value: 162.8958333333333 and parameters: {'n_layers_hidden': 3, 'n_units_hidden': 150, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.09031836007208888, 'lr': 0.01, 'residual': False, 'batch_size': 100, 'lambda_calibration': 50, 'lambda_regression_nc': 10, 'lambda_regression_c': 10}. Best is trial 4 with value: 161.28125.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:57:20,632]\u001b[0m Trial 14 finished with value: 272.125 and parameters: {'n_layers_hidden': 2, 'n_units_hidden': 150, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.08132268371995673, 'lr': 0.0001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 50, 'lambda_regression_nc': 10, 'lambda_regression_c': 10}. Best is trial 4 with value: 161.28125.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:57:38,326]\u001b[0m Trial 15 finished with value: 159.8125000000001 and parameters: {'n_layers_hidden': 3, 'n_units_hidden': 300, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.10856588083023917, 'lr': 0.01, 'residual': False, 'batch_size': 250, 'lambda_calibration': 50, 'lambda_regression_nc': 10, 'lambda_regression_c': 10}. Best is trial 15 with value: 159.8125000000001.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:57:51,351]\u001b[0m Trial 16 finished with value: 167.71874999999997 and parameters: {'n_layers_hidden': 1, 'n_units_hidden': 300, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.11344425599516048, 'lr': 0.01, 'residual': False, 'batch_size': 250, 'lambda_calibration': 50, 'lambda_regression_nc': 10, 'lambda_regression_c': 10}. Best is trial 15 with value: 159.8125000000001.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:58:06,716]\u001b[0m Trial 17 finished with value: 143.4895833333334 and parameters: {'n_layers_hidden': 2, 'n_units_hidden': 300, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.061733531757536125, 'lr': 0.01, 'residual': False, 'batch_size': 250, 'lambda_calibration': 50, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 17 with value: 143.4895833333334.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:58:24,280]\u001b[0m Trial 18 finished with value: 168.65625 and parameters: {'n_layers_hidden': 3, 'n_units_hidden': 300, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.00156049831393245, 'lr': 0.0001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 50, 'lambda_regression_nc': 10, 'lambda_regression_c': 100}. Best is trial 17 with value: 143.4895833333334.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:58:43,413]\u001b[0m Trial 19 finished with value: 144.9687500000001 and parameters: {'n_layers_hidden': 4, 'n_units_hidden': 250, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.05298113810806284, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 17 with value: 143.4895833333334.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:58:56,384]\u001b[0m Trial 20 finished with value: 160.5208333333333 and parameters: {'n_layers_hidden': 4, 'n_units_hidden': 250, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.05447390271899058, 'lr': 0.001, 'residual': False, 'batch_size': 1000, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 17 with value: 143.4895833333334.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-27 12:59:16,587]\u001b[0m Trial 21 finished with value: 142.09375000000003 and parameters: {'n_layers_hidden': 4, 'n_units_hidden': 300, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.10469494723478397, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 21 with value: 142.09375000000003.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 12:59:39,241]\u001b[0m Trial 22 finished with value: 142.5000000000001 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 300, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.05605696899266862, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 21 with value: 142.09375000000003.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:00:01,881]\u001b[0m Trial 23 finished with value: 139.93750000000006 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 300, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.061771005405779913, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:00:24,395]\u001b[0m Trial 24 finished with value: 143.07291666666669 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 300, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.10269017856558835, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:00:45,574]\u001b[0m Trial 25 finished with value: 147.3541666666666 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 250, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.1533530732560521, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:01:14,102]\u001b[0m Trial 26 finished with value: 494.6979166666669 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 300, 'nonlin': 'leaky_relu', 'batch_norm': True, 'dropout': 0.04408763257961469, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:01:36,308]\u001b[0m Trial 27 finished with value: 143.46875000000006 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 250, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.07176233979698249, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:01:53,337]\u001b[0m Trial 28 finished with value: 220.55208333333334 and parameters: {'n_layers_hidden': 4, 'n_units_hidden': 300, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.0021766172909702466, 'lr': 0.001, 'residual': False, 'batch_size': 1000, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 1}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:02:18,273]\u001b[0m Trial 29 finished with value: 143.63541666666677 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 250, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.11958887231224924, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:02:36,754]\u001b[0m Trial 30 finished with value: 167.8958333333333 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 200, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.0925995346131739, 'lr': 0.001, 'residual': False, 'batch_size': 500, 'lambda_calibration': 1, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:03:01,125]\u001b[0m Trial 31 finished with value: 143.25000000000003 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 300, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.09588930798568801, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:03:25,604]\u001b[0m Trial 32 finished with value: 142.18750000000003 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 300, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.10387393020284394, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:03:48,510]\u001b[0m Trial 33 finished with value: 141.7083333333334 and parameters: {'n_layers_hidden': 4, 'n_units_hidden': 300, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.07004950662896134, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:04:16,251]\u001b[0m Trial 34 finished with value: 362.02083333333326 and parameters: {'n_layers_hidden': 4, 'n_units_hidden': 250, 'nonlin': 'leaky_relu', 'batch_norm': True, 'dropout': 0.0742893695763399, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:04:38,034]\u001b[0m Trial 35 finished with value: 144.46874999999997 and parameters: {'n_layers_hidden': 4, 'n_units_hidden': 250, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.12537845402233727, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:04:57,119]\u001b[0m Trial 36 finished with value: 391.69791666666634 and parameters: {'n_layers_hidden': 4, 'n_units_hidden': 300, 'nonlin': 'relu', 'batch_norm': True, 'dropout': 0.1381115658937581, 'lr': 0.001, 'residual': False, 'batch_size': 500, 'lambda_calibration': 10, 'lambda_regression_nc': 1, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:05:36,050]\u001b[0m Trial 37 finished with value: 225.4270833333335 and parameters: {'n_layers_hidden': 4, 'n_units_hidden': 250, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.04185938210526052, 'lr': 0.001, 'residual': True, 'batch_size': 250, 'lambda_calibration': 1, 'lambda_regression_nc': 50, 'lambda_regression_c': 1}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:06:04,203]\u001b[0m Trial 38 finished with value: 408.9270833333335 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 300, 'nonlin': 'relu', 'batch_norm': True, 'dropout': 0.07024878881069535, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 100, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:06:19,438]\u001b[0m Trial 39 finished with value: 206.4166666666666 and parameters: {'n_layers_hidden': 3, 'n_units_hidden': 200, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.10299501892766284, 'lr': 0.0001, 'residual': True, 'batch_size': 500, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 50}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:06:40,045]\u001b[0m Trial 40 finished with value: 168.08333333333317 and parameters: {'n_layers_hidden': 4, 'n_units_hidden': 300, 'nonlin': 'relu', 'batch_norm': False, 'dropout': 0.16511434932571842, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 1, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-27 13:07:04,880]\u001b[0m Trial 41 finished with value: 144.77083333333337 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 300, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.020316355297418716, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:07:28,801]\u001b[0m Trial 42 finished with value: 140.17708333333326 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 300, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.0621653207199051, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:07:57,270]\u001b[0m Trial 43 finished with value: 145.33333333333337 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 250, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.07698255672247944, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:09:31,729]\u001b[0m Trial 44 finished with value: 158.62499999999994 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 300, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.06467253407174192, 'lr': 0.001, 'residual': True, 'batch_size': 1000, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:09:58,689]\u001b[0m Trial 45 finished with value: 160.2291666666667 and parameters: {'n_layers_hidden': 4, 'n_units_hidden': 300, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.08342855754268123, 'lr': 0.001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 1, 'lambda_regression_nc': 50, 'lambda_regression_c': 50}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:11:12,369]\u001b[0m Trial 46 finished with value: 367.3854166666669 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 250, 'nonlin': 'leaky_relu', 'batch_norm': True, 'dropout': 0.03394978763638892, 'lr': 0.001, 'residual': False, 'batch_size': 100, 'lambda_calibration': 100, 'lambda_regression_nc': 100, 'lambda_regression_c': 1}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:11:36,535]\u001b[0m Trial 47 finished with value: 264.94791666666623 and parameters: {'n_layers_hidden': 4, 'n_units_hidden': 300, 'nonlin': 'relu', 'batch_norm': False, 'dropout': 0.11168550200676919, 'lr': 0.0001, 'residual': False, 'batch_size': 250, 'lambda_calibration': 10, 'lambda_regression_nc': 1, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:11:58,775]\u001b[0m Trial 48 finished with value: 150.64583333333331 and parameters: {'n_layers_hidden': 3, 'n_units_hidden': 50, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.04496988969196128, 'lr': 0.001, 'residual': True, 'batch_size': 250, 'lambda_calibration': 100, 'lambda_regression_nc': 50, 'lambda_regression_c': 100}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n",
      "\u001b[32m[I 2022-04-27 13:12:45,136]\u001b[0m Trial 49 finished with value: 141.56250000000003 and parameters: {'n_layers_hidden': 5, 'n_units_hidden': 250, 'nonlin': 'leaky_relu', 'batch_norm': False, 'dropout': 0.08479627372839296, 'lr': 0.001, 'residual': False, 'batch_size': 100, 'lambda_calibration': 10, 'lambda_regression_nc': 50, 'lambda_regression_c': 50}. Best is trial 23 with value: 139.93750000000006.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.DEBUG)\n",
    "optuna.logging.enable_propagation()\n",
    "optuna.logging.enable_default_handler()\n",
    "\n",
    "\n",
    "def _trial_params(trial, param_space):\n",
    "    out = {}\n",
    "\n",
    "    for param in param_space:\n",
    "        if hasattr(param, \"choices\"):\n",
    "            out[param.name] = trial.suggest_categorical(\n",
    "                param.name, choices=param.choices\n",
    "            )\n",
    "        elif hasattr(param, \"step\"):\n",
    "            out[param.name] = trial.suggest_int(\n",
    "                param.name, param.low, param.high, param.step\n",
    "            )\n",
    "        else:\n",
    "            out[param.name] = trial.suggest_float(param.name, param.low, param.high)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    args = {\n",
    "        \"n_layers_hidden\": trial.suggest_int(\"n_layers_hidden\", 1, 5),\n",
    "        \"n_units_hidden\": trial.suggest_int(\"n_units_hidden\", 50, 300, step=50),\n",
    "        \"nonlin\": trial.suggest_categorical(\"nonlin\", [\"relu\", \"leaky_relu\"]),\n",
    "        \"n_iter\": 500,\n",
    "        \"batch_norm\": trial.suggest_categorical(\"batch_norm\", [True, False]),\n",
    "        \"dropout\": trial.suggest_float(\"dropout\", 0, 0.2),\n",
    "        \"lr\": trial.suggest_categorical(\"lr\", [1e-2, 1e-3, 1e-4]),\n",
    "        \"residual\": trial.suggest_categorical(\"residual\", [True, False]),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [100, 250, 500, 1000]),\n",
    "        \"lambda_calibration\": trial.suggest_categorical(\n",
    "            \"lambda_calibration\", [1, 10, 50, 100]\n",
    "        ),\n",
    "        \"lambda_regression_nc\": trial.suggest_categorical(\n",
    "            \"lambda_regression_nc\", [1, 10, 50, 100]\n",
    "        ),\n",
    "        \"lambda_regression_c\": trial.suggest_categorical(\n",
    "            \"lambda_regression_c\", [1, 10, 50, 100]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    model = TimeEventNN(\n",
    "        n_features=X.shape[1],\n",
    "        **args,\n",
    "    )\n",
    "    model.fit(enc_X, enc_T, E)\n",
    "    model.eval()\n",
    "\n",
    "    enc_pred_T = model.generate(enc_X)\n",
    "\n",
    "    nn_time_to_event = scaler_T.inverse_transform(enc_pred_T).squeeze()\n",
    "\n",
    "    return ranking_error(T, E, nn_time_to_event)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d003b46",
   "metadata": {},
   "source": [
    "## Predict censoring time, as seen in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "255bfcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_expected_time = pd.Series(adv_time_to_event, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1bead6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>cd4</th>\n",
       "      <th>hemophil</th>\n",
       "      <th>ivdrug</th>\n",
       "      <th>karnof</th>\n",
       "      <th>priorzdv</th>\n",
       "      <th>raceth</th>\n",
       "      <th>sex</th>\n",
       "      <th>strat2</th>\n",
       "      <th>tx</th>\n",
       "      <th>txgrp</th>\n",
       "      <th>pred_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.0</td>\n",
       "      <td>149.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.0</td>\n",
       "      <td>23.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>25.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>44.0</td>\n",
       "      <td>65.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>43.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>44.0</td>\n",
       "      <td>282.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>44.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1055 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age    cd4  hemophil  ivdrug  karnof  priorzdv  raceth  sex  strat2  \\\n",
       "0     34.0  169.0         0       0       0      39.0       0    0       1   \n",
       "1     34.0  149.5         0       0       3      15.0       1    1       1   \n",
       "2     20.0   23.5         1       0       0       9.0       0    0       0   \n",
       "3     48.0   46.0         0       0       3      53.0       0    0       1   \n",
       "4     46.0   10.0         0       2       3      12.0       0    0       0   \n",
       "...    ...    ...       ...     ...     ...       ...     ...  ...     ...   \n",
       "1145  25.0   16.5         0       0       0       7.0       0    0       0   \n",
       "1146  44.0   65.5         0       0       0     103.0       0    0       1   \n",
       "1148  43.0  170.0         0       2       3      27.0       1    0       1   \n",
       "1149  44.0  282.5         0       2       2      12.0       0    0       1   \n",
       "1150  44.0  120.0         0       0       3      26.0       0    0       1   \n",
       "\n",
       "      tx  txgrp    pred_T  \n",
       "0      0      0  1.000000  \n",
       "1      0      0  1.000000  \n",
       "2      1      1  1.000004  \n",
       "3      0      0  1.000000  \n",
       "4      1      1  1.000001  \n",
       "...   ..    ...       ...  \n",
       "1145   0      0  1.000001  \n",
       "1146   1      1  1.000000  \n",
       "1148   0      0  1.000000  \n",
       "1149   0      0  1.000000  \n",
       "1150   1      1  1.000000  \n",
       "\n",
       "[1055 rows x 12 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict censoring time\n",
    "pred_T = pd.Series(synthetic_expected_time, index=X.index)\n",
    "\n",
    "fill_X = X[E == 0].copy()\n",
    "fill_X[\"pred_T\"] = pred_T[E == 0]\n",
    "\n",
    "fill_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34134ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<plugin_xgboost_regressor.py.XGBoostRegressorPlugin at 0x7f7529168160>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from adjutorium.plugins.prediction.regression import Regression\n",
    "\n",
    "xgb_regression = Regression().get(\"xgboost_regressor\")\n",
    "\n",
    "xgb_regression.fit(fill_X, T[E == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d445e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>210.753510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>275.778442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>245.945831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200.979675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>301.189606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>294.570587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>275.296448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>269.980957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>183.680069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>201.239227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1055 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0     210.753510\n",
       "1     275.778442\n",
       "2     245.945831\n",
       "3     200.979675\n",
       "4     301.189606\n",
       "...          ...\n",
       "1050  294.570587\n",
       "1051  275.296448\n",
       "1052  269.980957\n",
       "1053  183.680069\n",
       "1054  201.239227\n",
       "\n",
       "[1055 rows x 1 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_regression.predict(fill_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0895355a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>cd4</th>\n",
       "      <th>hemophil</th>\n",
       "      <th>ivdrug</th>\n",
       "      <th>karnof</th>\n",
       "      <th>priorzdv</th>\n",
       "      <th>raceth</th>\n",
       "      <th>sex</th>\n",
       "      <th>strat2</th>\n",
       "      <th>tx</th>\n",
       "      <th>txgrp</th>\n",
       "      <th>pred_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.0</td>\n",
       "      <td>149.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.0</td>\n",
       "      <td>23.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>25.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>44.0</td>\n",
       "      <td>65.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>43.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>44.0</td>\n",
       "      <td>282.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>44.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1055 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age    cd4  hemophil  ivdrug  karnof  priorzdv  raceth  sex  strat2  \\\n",
       "0     34.0  169.0         0       0       0      39.0       0    0       1   \n",
       "1     34.0  149.5         0       0       3      15.0       1    1       1   \n",
       "2     20.0   23.5         1       0       0       9.0       0    0       0   \n",
       "3     48.0   46.0         0       0       3      53.0       0    0       1   \n",
       "4     46.0   10.0         0       2       3      12.0       0    0       0   \n",
       "...    ...    ...       ...     ...     ...       ...     ...  ...     ...   \n",
       "1145  25.0   16.5         0       0       0       7.0       0    0       0   \n",
       "1146  44.0   65.5         0       0       0     103.0       0    0       1   \n",
       "1148  43.0  170.0         0       2       3      27.0       1    0       1   \n",
       "1149  44.0  282.5         0       2       2      12.0       0    0       1   \n",
       "1150  44.0  120.0         0       0       3      26.0       0    0       1   \n",
       "\n",
       "      tx  txgrp    pred_T  \n",
       "0      0      0  1.000000  \n",
       "1      0      0  1.000000  \n",
       "2      1      1  1.000004  \n",
       "3      0      0  1.000000  \n",
       "4      1      1  1.000001  \n",
       "...   ..    ...       ...  \n",
       "1145   0      0  1.000001  \n",
       "1146   1      1  1.000000  \n",
       "1148   0      0  1.000000  \n",
       "1149   0      0  1.000000  \n",
       "1150   1      1  1.000000  \n",
       "\n",
       "[1055 rows x 12 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cens_samples = X[E == 0]\n",
    "\n",
    "censoring_input = cens_samples.copy()\n",
    "censoring_input[\"pred_T\"] = synthetic_expected_time[cens_samples.index]\n",
    "\n",
    "censoring_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc8a7804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "1146    0\n",
       "1147    1\n",
       "1148    0\n",
       "1149    0\n",
       "1150    0\n",
       "Length: 1151, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_T = T\n",
    "synth_T[cens_samples.index] = xgb_regression.predict(censoring_input).values.squeeze()\n",
    "\n",
    "synth_E = pd.Series([1] * len(X), index=X.index)\n",
    "synth_E[cens_samples.index] = 0\n",
    "\n",
    "synth_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c489befe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       210.753510\n",
       "1       275.778442\n",
       "2       245.945831\n",
       "3       200.979675\n",
       "4       301.189606\n",
       "           ...    \n",
       "1146    275.296448\n",
       "1147     47.000000\n",
       "1148    269.980957\n",
       "1149    183.680069\n",
       "1150    201.239227\n",
       "Name: duration, Length: 1151, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_T[synth_T <= 0] = 1\n",
    "\n",
    "synth_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6331b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c_index': '0.7393 +/- 0.0234',\n",
       " 'brier_score': '0.0616 +/- 0.0024',\n",
       " 'aucroc': '0.7268 +/- 0.0062'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline evaluation\n",
    "from synthcity.plugins import Plugins\n",
    "from adjutorium.utils.tester import evaluate_survival_estimator\n",
    "from adjutorium.plugins.prediction.risk_estimation import RiskEstimation\n",
    "\n",
    "predictor = RiskEstimation().get_type(\"survival_xgboost\")\n",
    "n_folds = 3\n",
    "\n",
    "const_cols = constant_columns(X)\n",
    "X = X.drop(columns=const_cols)\n",
    "\n",
    "evaluate_survival_estimator(\n",
    "    predictor(),\n",
    "    X,\n",
    "    T,\n",
    "    E,\n",
    "    time_horizons=time_horizons,\n",
    "    n_folds=n_folds,\n",
    ")[\"str\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7cd88c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 76\n",
      "0.8058526240040862\n",
      "20 76\n",
      "0.7735097732622199\n",
      "19 77\n",
      "0.6938282076696742\n",
      "19 77\n",
      "0.6864121889174984\n",
      "19 77\n",
      "0.8058270948396166\n",
      "19 77\n",
      "0.7898577979116185\n",
      "19 77\n",
      "0.7424563520782076\n",
      "19 77\n",
      "0.7464877060572261\n",
      "19 77\n",
      "0.713249877748108\n",
      "19 77\n",
      "0.6398226815366069\n"
     ]
    }
   ],
   "source": [
    "# Baseline evaluation\n",
    "from synthcity.plugins import Plugins\n",
    "from adjutorium.utils.tester import evaluate_survival_estimator\n",
    "from adjutorium.plugins.prediction.risk_estimation import RiskEstimation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from adjutorium.utils.metrics import (\n",
    "    evaluate_skurv_brier_score,\n",
    "    evaluate_skurv_c_index,\n",
    ")\n",
    "from adjutorium.utils.third_party.nonparametric import (\n",
    "    CensoringDistributionEstimator,\n",
    "    SurvivalFunctionEstimator,\n",
    ")\n",
    "\n",
    "predictor = RiskEstimation().get_type(\"survival_xgboost\")\n",
    "n_folds = 3\n",
    "\n",
    "const_cols = constant_columns(X)\n",
    "X = X.drop(columns=const_cols)\n",
    "\n",
    "\n",
    "skf = StratifiedKFold()\n",
    "\n",
    "time_horizons = np.linspace(synth_T.min(), synth_T.max(), num=5)[2:-1]\n",
    "\n",
    "eval_X = X\n",
    "eval_T = synth_T\n",
    "eval_E = synth_E\n",
    "\n",
    "for train_index, test_index in skf.split(eval_X, eval_E):\n",
    "    X_train = eval_X.loc[eval_X.index[train_index]]\n",
    "    Y_train = eval_E.loc[eval_E.index[train_index]]\n",
    "    T_train = eval_T.loc[eval_T.index[train_index]]\n",
    "    X_test = eval_X.loc[eval_X.index[test_index]]\n",
    "    Y_test = eval_E.loc[eval_E.index[test_index]]\n",
    "    T_test = eval_T.loc[eval_T.index[test_index]]\n",
    "\n",
    "    model = predictor()\n",
    "    model.fit(X_train, T_train, Y_train)\n",
    "\n",
    "    pred = model.predict(X_test, time_horizons).values\n",
    "\n",
    "    cest = CensoringDistributionEstimator()\n",
    "\n",
    "    train_structured = [(Y_train.iloc[i], T_train.iloc[i]) for i in range(len(Y_train))]\n",
    "    train_structured = np.array(\n",
    "        train_structured, dtype=[(\"status\", \"bool\"), (\"time\", \"<f8\")]\n",
    "    )\n",
    "\n",
    "    cest.fit(train_structured)\n",
    "\n",
    "    for k in range(len(time_horizons)):\n",
    "        eval_horizon = min(time_horizons[k], np.max(T_test) - 1)\n",
    "\n",
    "        Y_filter = Y_test[T_test < eval_horizon]\n",
    "        T_filter = T_test[T_test < eval_horizon]\n",
    "\n",
    "        test_structured = [\n",
    "            (Y_filter.iloc[i], T_filter.iloc[i]) for i in range(len(Y_filter))\n",
    "        ]\n",
    "        test_structured = np.array(\n",
    "            test_structured, dtype=[(\"status\", \"bool\"), (\"time\", \"<f8\")]\n",
    "        )\n",
    "\n",
    "        cest.predict_ipcw(test_structured)\n",
    "\n",
    "        print(Y_test.sum(), Y_train.sum())\n",
    "        score = evaluate_skurv_c_index(\n",
    "            T_train,\n",
    "            Y_train,\n",
    "            pred[:, k],\n",
    "            T_test,\n",
    "            Y_test,\n",
    "            eval_horizon,\n",
    "        )\n",
    "        print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de6210a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13      1.000001\n",
       "16      1.000001\n",
       "22      1.000000\n",
       "29      1.000000\n",
       "56      1.000000\n",
       "          ...   \n",
       "1075    1.000000\n",
       "1106    1.000000\n",
       "1120    1.000000\n",
       "1126    1.000000\n",
       "1147    1.000000\n",
       "Length: 96, dtype: float32"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_T[E == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "561bb694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13      206.0\n",
       "16      298.0\n",
       "22      190.0\n",
       "29       82.0\n",
       "56       61.0\n",
       "        ...  \n",
       "1075    186.0\n",
       "1106    114.0\n",
       "1120     20.0\n",
       "1126    174.0\n",
       "1147     47.0\n",
       "Name: duration, Length: 96, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T[E == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6bd4141d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13      262.474997\n",
       "16      266.742325\n",
       "22      273.714523\n",
       "29      266.343808\n",
       "56      266.526362\n",
       "           ...    \n",
       "1075    270.930636\n",
       "1106    277.411832\n",
       "1120    275.439517\n",
       "1126    269.017622\n",
       "1147    260.382612\n",
       "Length: 96, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_expected_time[E == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db066b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
