CURRENT CODES:
src/synthcity/plugins/core/models/syn_seq/syn_seq_encoder.py
from typing import Optional, Dict, Any, List, Union
import pandas as pd
import numpy as np
from sklearn.base import TransformerMixin, BaseEstimator

# [ADDED/CHANGED] We'll import datetime conversion if needed
from synthcity.plugins.core.models.feature_encoder import DatetimeEncoder

class Syn_SeqEncoder(TransformerMixin, BaseEstimator):
    """
    Syn_SeqEncoder handles preprocessing and postprocessing tasks using fit/transform pattern,
    plus manages a 'variable_selection' matrix (like a prediction matrix).
    """

    def __init__(
        self,
        columns_special_values: Optional[Dict[str, Any]] = None,
        syn_order: Optional[List[str]] = None,
        max_categories: int = 20,
        user_variable_selection: Optional[pd.DataFrame] = None,
        # [ADDED/CHANGED] user-declared column types (e.g. {"C1":"category","N1":"numeric","D1":"date"})
        col_type: Optional[Dict[str, str]] = None,
    ) -> None:
        """
        Args:
            columns_special_values : {colName : [specialVals...]}
            syn_order : optional column order to use
            max_categories : threshold for deciding numeric vs categorical
            user_variable_selection : optional DataFrame(n x n) with row=target, col=predictor
            col_type : user-declared column types, e.g. {"C2":"category","N1":"numeric","D1":"date"}
        """
        self.columns_special_values = columns_special_values or {}
        self.syn_order = syn_order or []
        self.max_categories = max_categories
        self.user_variable_selection = user_variable_selection

        # track info about columns
        self.categorical_info_: Dict[str, Dict[str, Any]] = {}
        self.numeric_info_: Dict[str, Dict[str, Any]] = {}
        # [ADDED/CHANGED] We'll track date columns separately
        self.date_info_: Dict[str, Dict[str, Any]] = {}

        self.column_order_: List[str] = []
        self.method_assignments: Dict[str, str] = {}
        self.variable_selection_: Optional[pd.DataFrame] = None

        # [ADDED/CHANGED] store user col_type
        self.col_type = col_type or {}

    def fit(self, X: pd.DataFrame, y=None) -> "Syn_SeqEncoder":
        # copy X to avoid side effects
        X = X.copy()

        self._detect_column_order(X)
        self._detect_col_types(X)
        self._detect_special_values(X)
        self._build_variable_selection_matrix(X)
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()

        # 1) reorder columns
        X = self._reorder_columns(X)
        # 2) split numeric => numeric + numeric_cat
        X = self._split_numeric_cols(X)
        # 3) update dtypes (including date if declared)
        X = self._update_column_types(X)
        # 4) assign placeholder methods
        X = self._assign_methods(X)
        # 5) update variable_selection for newly created columns
        self._update_variable_selection_after_split(X)

        return X

    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()

        # 1) restore special values for numeric/date
        for col, vals in self.columns_special_values.items():
            if col in X.columns:
                # if a single special value, use it directly
                if isinstance(vals, list) and len(vals) == 1:
                    vals = vals[0]
                X[col] = X[col].replace(pd.NA, vals)

        # 2) restore dtype for categorical
        for col, info in self.categorical_info_.items():
            if col in X.columns:
                X[col] = X[col].astype(info["dtype"])

        # 3) restore date columns
        for col, info in self.date_info_.items():
            if col in X.columns:
                X[col] = pd.to_datetime(X[col], errors="coerce")

        # 4) restore numeric
        for col, info in self.numeric_info_.items():
            if col in X.columns:
                X[col] = X[col].astype(info["dtype"])

        return X

    # -----------------------------------------------------
    # variable_selection building
    # -----------------------------------------------------
    def _build_variable_selection_matrix(self, X: pd.DataFrame) -> None:
        """
        If user provided a variable_selection DataFrame, validate it. Otherwise,
        build a default (row=target, columns=all prior columns).
        """
        n = len(self.column_order_)
        df_cols = self.column_order_

        if self.user_variable_selection is not None:
            vs = self.user_variable_selection.copy()
            if vs.shape != (n, n):
                raise ValueError(
                    f"user_variable_selection must be shape ({n},{n}), got {vs.shape}"
                )
            # check row/col alignment
            if list(vs.index) != df_cols or list(vs.columns) != df_cols:
                raise ValueError(
                    "Mismatch in user_variable_selection index/columns vs syn_order."
                )
            self.variable_selection_ = vs
        else:
            # default: row=target col=predictor => row i uses columns [0..i-1]
            vs = pd.DataFrame(0, index=df_cols, columns=df_cols)
            for i in range(n):
                for j in range(i):
                    vs.iat[i, j] = 1
            self.variable_selection_ = vs

    def _update_variable_selection_after_split(self, X: pd.DataFrame) -> None:
        """
        If numeric columns were split (col => col + col_cat), we add the new
        columns into the variable_selection matrix. They replicate the row/col
        from the original but don't self-predict.
        """
        if self.variable_selection_ is None:
            return
        old_vs = self.variable_selection_
        old_rows = old_vs.index.tolist()
        old_cols = old_vs.columns.tolist()

        final_cols = list(X.columns)
        new_cols = [c for c in final_cols if c not in old_rows]
        if not new_cols:
            return

        vs_new = pd.DataFrame(0, index=old_rows + new_cols, columns=old_cols + new_cols)

        # copy old content
        for r in old_rows:
            for c in old_cols:
                vs_new.at[r, c] = old_vs.at[r, c]

        # handle new splitted columns
        for c_new in new_cols:
            if c_new.endswith("_cat"):
                c_base = c_new[:-4]  # e.g. "age_cat" => "age"
                if c_base in vs_new.index and c_base in vs_new.columns:
                    # copy entire row from base
                    for c2 in old_cols:
                        vs_new.at[c_new, c2] = vs_new.at[c_base, c2]
                    # copy entire column from base
                    for r2 in old_rows:
                        vs_new.at[r2, c_new] = vs_new.at[r2, c_base]

                vs_new.at[c_new, c_new] = 0  # new col doesn't predict itself
            else:
                # brand new col => remain all zeros
                pass

        self.variable_selection_ = vs_new

    # -----------------------------------------------------
    # HELPER sub-routines
    # -----------------------------------------------------
    def _detect_column_order(self, X: pd.DataFrame):
        """
        If user gave syn_order, we filter columns. Otherwise, use X.columns.
        """
        if self.syn_order:
            self.column_order_ = [c for c in self.syn_order if c in X.columns]
        else:
            self.column_order_ = list(X.columns)

    def _detect_col_types(self, X: pd.DataFrame):
        """
        For each column, decide if numeric/categorical/date, unless
        col_type overrides. Then store in numeric_info_, categorical_info_, date_info_.
        """
        self.numeric_info_.clear()
        self.categorical_info_.clear()
        self.date_info_.clear()

        for col in X.columns:
            declared_type = self.col_type.get(col, "").lower()  # [ADDED/CHANGED]
            if declared_type == "category":
                self.categorical_info_[col] = {"dtype": "category"}
            elif declared_type == "numeric":
                self.numeric_info_[col] = {"dtype": X[col].dtype}
            elif declared_type == "date":
                self.date_info_[col] = {"dtype": "datetime64[ns]"}
            else:
                # fallback auto-detect
                nuniq = X[col].nunique()
                # if #unique <= max_categories => treat as category
                if nuniq > self.max_categories:
                    # might be numeric or date
                    if pd.api.types.is_datetime64_any_dtype(X[col]):
                        self.date_info_[col] = {"dtype": "datetime64[ns]"}
                    else:
                        self.numeric_info_[col] = {"dtype": X[col].dtype}
                else:
                    self.categorical_info_[col] = {"dtype": "category"}

    def _detect_special_values(self, X: pd.DataFrame):
        """
        For each col, if a single value has >90% freq, consider it a "special" value
        and store in columns_special_values. The user can override or supply additional.
        """
        for col in X.columns:
            freq = X[col].value_counts(normalize=True)
            high_vals = freq[freq > 0.9].index.tolist()
            if high_vals:
                existing_vals = self.columns_special_values.get(col, [])
                combined = set(existing_vals).union(set(high_vals))
                self.columns_special_values[col] = list(combined)

    def _reorder_columns(self, X: pd.DataFrame) -> pd.DataFrame:
        if self.column_order_:
            new_cols = [c for c in self.column_order_ if c in X.columns]
            return X[new_cols]
        return X

    def _split_numeric_cols(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        For numeric columns, create an extra col for "special or missing" => col_cat
        Then in col, we set them to NA to ensure they are not used as numeric. 
        """
        for col in list(self.numeric_info_.keys()):
            if col not in X.columns:
                continue
            cat_col = f"{col}_cat"
            special_vals = self.columns_special_values.get(col, [])

            # Mark special or missing in cat_col
            X[cat_col] = X[col].apply(
                lambda x: x if (x in special_vals or pd.isna(x)) else -777
            )
            X[cat_col] = X[cat_col].fillna(-9999)  # placeholder for NA
            # In the numeric column, remove those special/NA
            X[col] = X[col].apply(
                lambda x: x if (x not in special_vals and not pd.isna(x)) else pd.NA
            )
            X[cat_col] = X[cat_col].astype("category")

        return X

    def _update_column_types(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Attempt to cast date columns to datetime,
        numeric to numeric dtype,
        categorical to category dtype.
        """
        # handle date columns first
        for col in self.date_info_:
            if col in X.columns:
                X[col] = pd.to_datetime(X[col], errors="coerce")

        # handle numeric / categorical
        for col in X.columns:
            if col in self.numeric_info_:
                X[col] = X[col].astype(self.numeric_info_[col]["dtype"])
            elif col in self.categorical_info_:
                X[col] = X[col].astype("category")

        return X

    def _assign_methods(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Assign placeholder methods to each column, purely for demonstration
        (the real logic in R's synthpop is more dynamic).
        """
        self.method_assignments.clear()
        first = True
        for c in X.columns:
            if first:
                self.method_assignments[c] = "random_sampling"
                first = False
            else:
                self.method_assignments[c] = "CART"
        return X

    # -----------------------------------------------------
    # user update variable selection
    # -----------------------------------------------------
    @staticmethod
    def update_variable_selection(
        var_sel_df: pd.DataFrame,
        user_dict: Dict[str, List[str]]
    ) -> pd.DataFrame:
        """
        In the variable_selection matrix, set row=target_col => 1 in the predictor columns.
          user_dict = { "D": ["B","C"], "A": ["B"] }
          => row "D", col "B","C" => 1, row "A", col "B" => 1
        """
        for target_col, predictor_list in user_dict.items():
            if target_col not in var_sel_df.index:
                print(f"[WARNING] '{target_col}' not in var_sel_df.index => skipping.")
                continue
            # set row=target_col to 0 first
            var_sel_df.loc[target_col, :] = 0
            # set 1 only for the user-specified predictors
            for pred in predictor_list:
                if pred in var_sel_df.columns:
                    var_sel_df.loc[target_col, pred] = 1
                else:
                    print(f"[WARNING] predictor '{pred}' not in columns => skipping.")
        return var_sel_df


src/synthcity/plugins/core/models/syn_seq/syn_seq.py
# File: syn_seq.py
#
# A self-contained aggregator for the sequential column-by-column approach,
# with no separate _fit or _generate. We unify logic into fit(...) and generate(...).
# We now incorporate the `SynSeqConstraints` (or base Constraints) usage for
# both direct substitution AND row/sequence filtering.

from typing import Any, Dict, List, Optional, Union
import pandas as pd
import numpy as np
from random import sample

# local references
from src.synthcity.plugins.core.dataloader import Syn_SeqDataLoader
from src.synthcity.plugins.core.constraints import Constraints
from src.synthcity.plugins.core.models.syn_seq_constraints import SynSeqConstraints


def _to_synseq_constraints(
    constraint_input: Union[None, Dict[str, List[Any]], Constraints]
) -> Optional[SynSeqConstraints]:
    """
    A helper that converts user-supplied constraints (dict or Constraints)
    into a SynSeqConstraints object.
    
    Example dictionary format:
      {
         "N1": ["=", 999],
         "C2": ["in", ["A","B"]]
      }
    => [("N1","=",999),("C2","in",["A","B"])]

    If the user already has a SynSeqConstraints or base Constraints, we wrap or copy.
    """
    if constraint_input is None:
        return None
    
    if isinstance(constraint_input, Constraints):
        # Already a (Syn)Constraints?
        if isinstance(constraint_input, SynSeqConstraints):
            return constraint_input
        else:
            # Copy its rules into a SynSeqConstraints
            return SynSeqConstraints(rules=constraint_input.rules)
    
    if isinstance(constraint_input, dict):
        # Simple parse: each key => (op, val)
        # e.g. "col" : ["=", 999]
        # if len(...) < 2 => skip
        rules_list = []
        for col, rule_list in constraint_input.items():
            if not isinstance(rule_list, list) or len(rule_list) < 2:
                print(f"[WARNING] Malformed constraint for '{col}' => {rule_list}")
                continue
            op = rule_list[0]
            val = rule_list[1]
            rules_list.append((col, op, val))
        return SynSeqConstraints(rules=rules_list)
    
    raise ValueError(f"Unsupported constraint type: {type(constraint_input)}")


class Syn_Seq:
    """
    The aggregator model for a sequential (column-by-column) synthetic data approach.

    - Provides public .fit(...) and .generate(...) (no separate _fit/_generate).
    - Maintains per-column model info, variable selection, constraints, etc.
    - Integrates `SynSeqConstraints` for direct substitution ('=') and row/sequence filtering.

    Usage:
        aggregator = Syn_Seq(...)
        aggregator.fit(loader, method=[...], variable_selection={...})
        syn_data = aggregator.generate(count=..., constraint={...})
    """

    def __init__(
        self,
        random_state: int = 0,
        default_first_method: str = "SWR",
        default_other_method: str = "CART",
        strict: bool = True,
        sampling_patience: int = 500,
        seq_id_col: str = "seq_id",   # if you want sequence-level constraints
        seq_time_col: str = "seq_time_id",
        **kwargs: Any
    ):
        """
        Args:
            random_state: for reproducibility.
            default_first_method: fallback for the first column if not user-specified.
            default_other_method: fallback for subsequent columns if not user-specified.
            strict: if True, constraints are strictly enforced with repeated tries.
            sampling_patience: how many times we attempt new draws if constraints fail.
            seq_id_col: for sequence-level constraints in SynSeqConstraints.
            seq_time_col: time index column, if needed.
            **kwargs: aggregator-level arguments (unused here).
        """
        self.random_state = random_state
        self.default_first_method = default_first_method
        self.default_other_method = default_other_method
        self.strict = strict
        self.sampling_patience = sampling_patience

        # Per-column model info
        self._column_models: Dict[str, Any] = {}

        # For user-supplied or fallback
        self.method_list: List[str] = []
        self.variable_selection: Dict[str, List[str]] = {}

        # If used by constraints
        self.seq_id_col = seq_id_col
        self.seq_time_col = seq_time_col

        self._model_trained = False

    # ----------------------------------------------------------------
    # fit(...)
    # ----------------------------------------------------------------
    def fit(
        self,
        loader: Syn_SeqDataLoader,
        method: Optional[List[str]] = None,
        variable_selection: Optional[Dict[str, List[str]]] = None,
        *args: Any,
        **kwargs: Any
    ) -> "Syn_Seq":
        """
        Fit a column-by-column model.
         1) Expand user method array if needed
         2) Build variable-selection matrix
         3) Train minimal model for each column
        """
        df = loader.dataframe()
        if df.empty:
            raise ValueError("No data in Syn_SeqDataLoader for training.")

        self.method_list = method or []
        self.variable_selection = variable_selection or {}

        col_list = list(df.columns)
        n_cols = len(col_list)

        # 1) Expand methods if needed
        final_methods = []
        for i, col in enumerate(col_list):
            if i < len(self.method_list):
                final_methods.append(self.method_list[i])
            else:
                # fallback
                fallback = self.default_first_method if i == 0 else self.default_other_method
                final_methods.append(fallback)

        # 2) Build variable_selection matrix
        vs_matrix = pd.DataFrame(0, index=col_list, columns=col_list)
        for i in range(n_cols):
            vs_matrix.iloc[i, :i] = 1

        # incorporate user-specified variable_selection
        for target_col, pred_cols in self.variable_selection.items():
            if target_col in vs_matrix.index:
                vs_matrix.loc[target_col, :] = 0
                for pc in pred_cols:
                    if pc in vs_matrix.columns:
                        vs_matrix.loc[target_col, pc] = 1

        print("[INFO] aggregator: final method assignment:")
        for col, m in zip(col_list, final_methods):
            print(f"  {col} => {m}")
        print("[INFO] aggregator: final variable_selection matrix:")
        print(vs_matrix)

        # 3) Train a minimal "model" for each column
        self._column_models.clear()
        for i, col in enumerate(col_list):
            chosen_method = final_methods[i]
            preds = vs_matrix.columns[(vs_matrix.loc[col] == 1)].tolist()
            model_info = self._train_column_model(df, target_col=col, predictor_cols=preds, method=chosen_method)
            self._column_models[col] = {
                "method": chosen_method,
                "predictors": preds,
                "model": model_info,
            }

        self._model_trained = True
        return self

    # ----------------------------------------------------------------
    # generate(...)
    # ----------------------------------------------------------------
    def generate(
        self,
        count: int = 10,
        constraint: Union[None, Dict[str, List[Any]], Constraints] = None,
        *args: Any,
        **kwargs: Any
    ) -> Syn_SeqDataLoader:
        """
        Generate synthetic data row-by-row.

        Steps:
          1) Convert `constraint` to SynSeqConstraints if needed
          2) If strict => repeated tries
             else => single pass + constraints
          3) Return a new Syn_SeqDataLoader
        """
        if not self._model_trained:
            raise RuntimeError("fit() must be called before generate().")

        # 1) unify constraints
        syn_constraints = _to_synseq_constraints(constraint)
        if syn_constraints is not None:
            syn_constraints.seq_id_feature = self.seq_id_col
            syn_constraints.seq_time_id_feature = self.seq_time_col

        # 2) Strict => repeated tries
        if syn_constraints and self.strict:
            syn_df = self._attempt_strict_generation(count, syn_constraints)
        else:
            # single pass
            syn_df = self._generate_once(count)
            # direct substitution + match if we have constraints
            if syn_constraints:
                syn_df = self._apply_synseq_corrections(syn_df, syn_constraints)
                syn_df = syn_constraints.match(syn_df)

        # 3) wrap up
        return Syn_SeqDataLoader(data=syn_df, syn_order=list(syn_df.columns))

    # ----------------------------------------------------------------
    # Helpers
    # ----------------------------------------------------------------
    def _train_column_model(
        self,
        df: pd.DataFrame,
        target_col: str,
        predictor_cols: List[str],
        method: str,
    ) -> dict:
        """
        Minimal training logic; store raw data for now. 
        Real use would implement CART, pmm, etc.
        """
        model_info = {
            "predictors": predictor_cols,
            "target_data": df[target_col].values,
            "method": method,
        }
        return model_info

    def _generate_for_column(
        self,
        count: int,
        col: str,
        method: str,
        predictor_cols: List[str],
        model_obj: dict,
        partial_df: pd.DataFrame,
    ) -> pd.Series:
        """
        Column-level sampling approach:
          - "swr" => sample without replacement
          - "cart", "pmm" => placeholder => random from real
          - fallback => random from real
        """
        real_data = model_obj["target_data"]
        rng = np.random.default_rng(self.random_state + hash(col) % 999999)

        if method.lower() == "swr":
            n_real = len(real_data)
            if count <= n_real:
                picks = sample(list(real_data), count)
            else:
                picks = list(real_data)
                overshoot = count - n_real
                picks += sample(list(real_data), overshoot)
            return pd.Series(picks)

        elif method.lower() in ["cart", "pmm"]:
            # placeholder => random
            picks = rng.choice(real_data, size=count, replace=True)
            return pd.Series(picks)

        else:
            # fallback => random
            picks = rng.choice(real_data, size=count, replace=True)
            return pd.Series(picks)

    def _generate_once(self, count: int) -> pd.DataFrame:
        """
        Single pass ignoring constraints.
        """
        col_list = list(self._column_models.keys())
        syn_df = pd.DataFrame(index=range(count))
        for col in col_list:
            info = self._column_models[col]
            method = info["method"]
            preds = info["predictors"]
            model_obj = info["model"]

            new_vals = self._generate_for_column(
                count, col, method, preds, model_obj, partial_df=syn_df
            )
            syn_df[col] = new_vals

        return syn_df

    def _attempt_strict_generation(
        self,
        count: int,
        syn_constraints: SynSeqConstraints
    ) -> pd.DataFrame:
        """
        Repeated tries if strict => generate, correct '=' constraints, match => 
        accumulate until we have `count` rows or out of patience.
        """
        result_df = pd.DataFrame()
        tries = 0
        while len(result_df) < count and tries < self.sampling_patience:
            tries += 1
            chunk = self._generate_once(count)
            chunk = self._apply_synseq_corrections(chunk, syn_constraints)
            chunk = syn_constraints.match(chunk)
            chunk = chunk.drop_duplicates()

            result_df = pd.concat([result_df, chunk], ignore_index=True)

        return result_df.head(count)

    def _apply_synseq_corrections(
        self,
        df: pd.DataFrame,
        syn_constraints: SynSeqConstraints
    ) -> pd.DataFrame:
        """
        For each (feature, op, val) in constraints, if op in ['=','=='],
        do direct substitution. For other ops => do nothing here.
        """
        new_df = df.copy()
        for (feature, op, val) in syn_constraints.rules:
            if op in ["=", "=="]:
                new_df = syn_constraints._correct(new_df, feature, op, val)
        return new_df


src/synthcity/plugins/core/models/syn_seq/syn_seq_constraints.py
# File: syn_seq_constraints.py

from typing import Any, List, Dict, Tuple, Union
import pandas as pd
import numpy as np

# We import the base Constraints to inherit from
from synthcity.plugins.core.constraints import Constraints

# 
# A single sub-rule is (feature, op, value).
# For "chained" logic, we might interpret:
#    "For rows that pass all sub-rules (1..k-1), apply sub-rule k as a filter or correction."
#
# Example constraint input:
#   {
#       "N1": [
#           ("C1", "in", ["AAA","BBB"]),
#           ("N1", ">", 125)
#       ]
#   }
# means:
#  1) If a row passes (C1 in [AAA,BBB]), 
#  2) Then also enforce (N1 > 125) on that row.
#
# If the first sub-rule is not satisfied, the second doesn't apply to that row.
# If the first sub-rule is satisfied but not the second => row fails entirely.
#

class SynSeqConstraints(Constraints):
    """
    An extension that supports "chained" constraints for sequential logic:
      - If sub-rule 1 is satisfied => we must also pass sub-rule 2, and so on.
      - 'chained_rules' can be a dict of { targetCol : [ (feature, op, val), (feature, op, val), ... ] }.
        Example:
          {
            "N1": [
                ("C1", "in", ["AAA","BBB"]),
                ("N1", ">", 125)
            ]
          }
        read as: "If (C1 in [AAA,BBB]) => enforce (N1 > 125)".
      
      Each sub-rule uses the same _eval logic for <, <=, >, >=, ==, in, etc.
      If a sub-rule fails => that entire row fails (filtered out) if using .match(), 
      or is corrected if possible (.correct).
    """

    def __init__(
        self,
        # You can still pass standard constraints as "rules",
        # or pass new "chained_rules" in dict format
        rules: List[Tuple[str, str, Any]] = None,
        chained_rules: Dict[str, List[Tuple[str, str, Any]]] = None,
        seq_id_feature: str = "seq_id",
        seq_time_id_feature: str = "seq_time_id",
        **kwargs: Any,
    ):
        # Let the base constructor handle standard 'rules'
        super().__init__(rules=rules if rules else [])
        self.seq_id_feature = seq_id_feature
        self.seq_time_id_feature = seq_time_id_feature

        # We'll store the "chained" sub-rules in a separate structure
        # keyed by "target column" or "some label"
        self.chained_rules = chained_rules if chained_rules else {}

    def match(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Overridden. We can do row-level or entire-sequence filtering. 
        If you still want entire-sequence filtering, you can define match_sequential() and call it below.
        For demonstration, let's do row-level filtering with chained sub-rules.
        """
        df_copy = X.copy()
        # first, apply base constraints (self.rules) at row-level:
        base_mask = super().filter(df_copy)
        df_filtered = df_copy[base_mask].copy()
        if df_filtered.empty:
            return df_filtered

        # next, apply "chained" constraints
        df_filtered = self._match_chained(df_filtered)
        return df_filtered

    def _match_chained(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        For each entry in self.chained_rules => interpret a *sequence* of sub-rules.
        If the row passes sub-rule[0], sub-rule[1], ... sub-rule[n-2], 
        then sub-rule[n-1] is also enforced. 
        If it fails at any step => the row is filtered out or "corrected" (depending on your logic).
        
        For simplicity below, we do "if sub-rule[i] is satisfied => proceed, else row is out."
        """
        df = X.copy()
        for target_key, rule_chain in self.chained_rules.items():
            # We interpret the chain in order
            # e.g. [("C1","in", [...]), ("N1",">",125)]
            # step i=0 => a filter => if row fails => out
            # step i=1 => a further filter => if row fails => out
            # etc.

            # We'll build a mask for these sub-rules
            keep_mask = pd.Series([True]*len(df), index=df.index)

            for (feature, op, operand) in rule_chain:
                cur_mask = self._eval(df, feature, op, operand)
                # only keep the rows that pass this sub-rule
                keep_mask = keep_mask & cur_mask

            # after we apply all sub-rules in the chain, 
            # rows that didn't pass => out
            df = df[keep_mask].copy()
            if df.empty:
                break

        return df

    def _eval(self, X: pd.DataFrame, feature: str, op: str, operand: Any) -> pd.Index:
        """
        If we want to also handle direct substitution '=' or '==' or to skip it, we can override _eval.
        If the user wants a different meaning for '=' in chain logic, we can do so.
        Otherwise, we rely on base Constraints._eval for <, <=, >, >=, ==, in, dtype, etc.
        """
        # If you want direct substitution for '=' or '==', do so in _correct or in some separate logic.
        if op in ["=", "=="]:
            # interpret as equality check
            return (X[feature] == operand) | X[feature].isna()
        else:
            # fallback to base method
            return super()._eval(X, feature, op, operand)

    def _correct(self, X: pd.DataFrame, feature: str, op: str, operand: Any) -> pd.DataFrame:
        """
        If user wants direct substitution for '=' => set that col's entire column to operand. 
        Or you can do row-level changes only for failing rows, etc.
        """
        if op in ["=", "=="]:
            X.loc[:, feature] = operand
            return X
        return super()._correct(X, feature, op, operand)

    # If you want entire-sequence logic, you'd define match_sequential below:

    # Example
    #     constraint = {
    #   "N1": [
    #     ["C1", "in", ["AAA","BBB"]],
    #     ["N1", ">", 125]
    #   ]
    # }

    # def match_sequential(self, X: pd.DataFrame) -> pd.DataFrame:
    #     """
    #     Example method that applies constraints at the group (sequence) level:
    #       - if a sub-rule fails for ANY row => remove entire sequence
    #       - or do direct substitution in entire sequence
    #     """
    #     df_copy = X.copy()
    #     base_mask = super().filter(df_copy)
    #     # group by seq_id
    #     grouped = df_copy.groupby(self.seq_id_feature)
    #
    #     keep_seq_ids = []
    #     for seq_id, group in grouped:
    #         # if all rows pass => keep entire sequence
    #         if base_mask[group.index].all():
    #             # next, check chained
    #             # for each sub-rule chain, we can test if group passes
    #             # if not => exclude entire seq
    #             # or we can do partial correction
    #             # ...
    #             keep_seq_ids.append(seq_id)
    #
    #     return df_copy[df_copy[self.seq_id_feature].isin(keep_seq_ids)]



src/synthcity/plugins/core/dataloader.py
# stdlib
import random
from abc import ABCMeta, abstractmethod
from typing import Any, Dict, List, Optional, Tuple, Union

# third party
import numpy as np
import numpy.ma as ma
import pandas as pd
import PIL
import torch
from pydantic import validate_arguments
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torchvision import transforms

# synthcity absolute
from synthcity.plugins.core.constraints import Constraints
from synthcity.plugins.core.dataset import FlexibleDataset, TensorDataset
from synthcity.plugins.core.models.feature_encoder import DatetimeEncoder
from synthcity.utils.compression import compress_dataset, decompress_dataset
from synthcity.utils.serialization import dataframe_hash


class DataLoader(metaclass=ABCMeta):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.DataLoader
        :parts: 1

    Base class for all data loaders.

    Each derived class must implement the following methods:
        unpack() - a method that unpacks the columns and returns features and labels (X, y).
        decorate() - a method that creates a new instance of DataLoader by decorating the input data with the same DataLoader properties (e.g. sensitive features, target column, etc.)
        dataframe() - a method that returns the pandas dataframe that contains all features and samples
        numpy() - a method that returns the numpy array that contains all features and samples
        info() - a method that returns a dictionary of DataLoader information
        __len__() - a method that returns the number of samples in the DataLoader
        satisfies() - a method that tests if the current DataLoader satisfies the constraint provided
        match() - a method that returns a new DataLoader where the provided constraints are met
        from_info() - a static method that creates a DataLoader from the data and the information dictionary
        sample() - returns a new DataLoader that contains a random subset of N samples
        drop() - returns a new DataLoader with a list of columns dropped
        __getitem__() - getting features by names
        __setitem__() - setting features by names
        train() - returns a DataLoader containing the training set
        test() - returns a DataLoader containing the testing set
        fillna() - returns a DataLoader with NaN filled by the provided number(s)


    If any method implementation is missing, the class constructor will fail.

    Constructor Args:
        data_type: str
            The type of DataLoader, currently supports "generic", "time_series" and "survival".
        data: Any
            The object that contains the data
        static_features: List[str]
            List of feature names that are static features (as opposed to temporal features).
        temporal_features:
            List of feature names that are temporal features, i.e. observed over time.
        sensitive_features: List[str]
            Name of sensitive features.
        important_features: List[str]
            Default: None. Only relevant for SurvivalGAN method.
        outcome_features:
            The feature name that provides labels for downstream tasks.
    """

    def __init__(
        self,
        data_type: str,
        data: Any,
        static_features: List[str] = [],
        temporal_features: List[str] = [],
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        outcome_features: List[str] = [],
        train_size: float = 0.8,
        random_state: int = 0,
        **kwargs: Any,
    ) -> None:
        self.static_features = static_features
        self.temporal_features = temporal_features
        self.sensitive_features = sensitive_features
        self.important_features = important_features
        self.outcome_features = outcome_features
        self.random_state = random_state

        self.data = data
        self.data_type = data_type
        self.train_size = train_size

    def raw(self) -> Any:
        return self.data

    @abstractmethod
    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        ...

    @abstractmethod
    def decorate(self, data: Any) -> "DataLoader":
        ...

    def type(self) -> str:
        return self.data_type

    @property
    @abstractmethod
    def shape(self) -> tuple:
        ...

    @property
    @abstractmethod
    def columns(self) -> list:
        ...

    @abstractmethod
    def dataframe(self) -> pd.DataFrame:
        ...

    @abstractmethod
    def numpy(self) -> np.ndarray:
        ...

    @property
    def values(self) -> np.ndarray:
        return self.numpy()

    @abstractmethod
    def info(self) -> dict:
        ...

    @abstractmethod
    def __len__(self) -> int:
        ...

    @abstractmethod
    def satisfies(self, constraints: Constraints) -> bool:
        ...

    @abstractmethod
    def match(self, constraints: Constraints) -> "DataLoader":
        ...

    @staticmethod
    @abstractmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        ...

    @abstractmethod
    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        ...

    @abstractmethod
    def drop(self, columns: list = []) -> "DataLoader":
        ...

    @abstractmethod
    def __getitem__(self, feature: Union[str, list]) -> Any:
        ...

    @abstractmethod
    def __setitem__(self, feature: str, val: Any) -> None:
        ...

    @abstractmethod
    def train(self) -> "DataLoader":
        ...

    @abstractmethod
    def test(self) -> "DataLoader":
        ...

    def hash(self) -> str:
        return dataframe_hash(self.dataframe())

    def __repr__(self, *args: Any, **kwargs: Any) -> str:
        return self.dataframe().__repr__(*args, **kwargs)

    def _repr_html_(self, *args: Any, **kwargs: Any) -> Any:
        return self.dataframe()._repr_html_(*args, **kwargs)

    @abstractmethod
    def fillna(self, value: Any) -> "DataLoader":
        ...

    @abstractmethod
    def compression_protected_features(self) -> list:
        ...

    def domain(self) -> Optional[str]:
        return None

    def compress(
        self,
    ) -> Tuple["DataLoader", Dict]:
        to_compress = self.data.copy().drop(
            columns=self.compression_protected_features()
        )
        compressed, context = compress_dataset(to_compress)
        for protected_col in self.compression_protected_features():
            compressed[protected_col] = self.data[protected_col]

        return self.decorate(compressed), context

    def decompress(self, context: Dict) -> "DataLoader":
        decompressed = decompress_dataset(self.data, context)

        return self.decorate(decompressed)

    def encode(
        self,
        encoders: Optional[Dict[str, Any]] = None,
    ) -> Tuple["DataLoader", Dict]:
        encoded = self.dataframe().copy()
        if encoders is not None:
            for col in encoders:
                if col not in encoded.columns:
                    continue
                encoded[col] = encoders[col].transform(encoded[col])
        else:
            encoders = {}

            for col in encoded.columns:
                if (
                    encoded[col].infer_objects().dtype.kind == "i"
                    and encoded[col].min() == 0
                    and encoded[col].max() == len(encoded[col].unique()) - 1
                ):
                    continue

                if (
                    encoded[col].infer_objects().dtype.kind in ["O", "b"]
                    or len(encoded[col].unique()) < 15
                ):
                    encoder = LabelEncoder().fit(encoded[col])
                    encoded[col] = encoder.transform(encoded[col])
                    encoders[col] = encoder
                elif encoded[col].infer_objects().dtype.kind in ["M"]:
                    encoder = DatetimeEncoder().fit(encoded[col])
                    encoded[col] = encoder.transform(encoded[col]).values
                    encoders[col] = encoder
        return self.from_info(encoded, self.info()), encoders

    def decode(
        self,
        encoders: Dict[str, Any],
    ) -> "DataLoader":
        decoded = self.dataframe().copy()

        for col in encoders:
            if isinstance(encoders[col], LabelEncoder):
                decoded[col] = decoded[col].astype(int)
            else:
                decoded[col] = decoded[col].astype(float)

            decoded[col] = encoders[col].inverse_transform(decoded[col])

        return self.from_info(decoded, self.info())

    @abstractmethod
    def is_tabular(self) -> bool:
        ...

    @abstractmethod
    def get_fairness_column(self) -> Union[str, Any]:
        ...


class GenericDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.GenericDataLoader
        :parts: 1

    Data loader for generic tabular data.

    Constructor Args:
        data: Union[pd.DataFrame, list, np.ndarray]
            The dataset. Either a Pandas DataFrame or a Numpy Array.
        sensitive_features: List[str]
            Name of sensitive features.
        important_features: List[str]
            Default: None. Only relevant for SurvivalGAN method.
        target_column: Optional[str]
            The feature name that provides labels for downstream tasks.
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        domain_column: Optional[str]
            Optional domain label, used for domain adaptation algorithms.
        random_state: int
            Defaults to zero.

    Example:
        >>> from sklearn.datasets import load_diabetes
        >>> from synthcity.plugins.core.dataloader import GenericDataLoader
        >>> X, y = load_diabetes(return_X_y=True, as_frame=True)
        >>> X["target"] = y
        >>> # Important note: preprocessing data with OneHotEncoder or StandardScaler is not needed or recommended.
        >>> # Synthcity handles feature encoding and standardization internally.
        >>> loader = GenericDataLoader(X, target_column="target", sensitive_columns=["sex"],)
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        data: Union[pd.DataFrame, list, np.ndarray],
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        target_column: Optional[str] = None,
        fairness_column: Optional[str] = None,
        domain_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        if not isinstance(data, pd.DataFrame):
            data = pd.DataFrame(data)

        data.columns = data.columns.astype(str)
        if target_column is not None:
            self.target_column = target_column
        elif len(data.columns) > 0:
            self.target_column = data.columns[-1]
        else:
            self.target_column = "---"

        self.fairness_column = fairness_column
        self.domain_column = domain_column

        super().__init__(
            data_type="generic",
            data=data,
            static_features=list(data.columns),
            sensitive_features=sensitive_features,
            important_features=important_features,
            outcome_features=[self.target_column],
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data.shape

    def domain(self) -> Optional[str]:
        return self.domain_column

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    @property
    def columns(self) -> list:
        return list(self.data.columns)

    def compression_protected_features(self) -> list:
        out = [self.target_column]
        domain = self.domain()

        if domain is not None:
            out.append(domain)

        return out

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        X = self.data.drop(columns=[self.target_column])
        y = self.data[self.target_column]

        if as_numpy:
            return np.asarray(X), np.asarray(y)
        return X, y

    def dataframe(self) -> pd.DataFrame:
        return self.data

    def numpy(self) -> np.ndarray:
        return self.dataframe().values

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "static_features": self.static_features,
            "sensitive_features": self.sensitive_features,
            "important_features": self.important_features,
            "outcome_features": self.outcome_features,
            "target_column": self.target_column,
            "fairness_column": self.fairness_column,
            "domain_column": self.domain_column,
            "train_size": self.train_size,
        }

    def __len__(self) -> int:
        return len(self.data)

    def decorate(self, data: Any) -> "DataLoader":
        return GenericDataLoader(
            data,
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            target_column=self.target_column,
            random_state=self.random_state,
            train_size=self.train_size,
            fairness_column=self.fairness_column,
            domain_column=self.domain_column,
        )

    def satisfies(self, constraints: Constraints) -> bool:
        return constraints.is_valid(self.data)

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.decorate(constraints.match(self.data))

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        return self.decorate(self.data.sample(count, random_state=random_state))

    def drop(self, columns: list = []) -> "DataLoader":
        return self.decorate(self.data.drop(columns=columns))

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "GenericDataLoader":
        if not isinstance(data, pd.DataFrame):
            raise ValueError(f"Invalid data type {type(data)}")

        return GenericDataLoader(
            data,
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            target_column=info["target_column"],
            fairness_column=info["fairness_column"],
            domain_column=info["domain_column"],
            train_size=info["train_size"],
        )

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self.data[feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self.data[feature] = val

    def _train_test_split(self) -> Tuple:
        stratify = None
        if self.target_column in self.data:
            target = self.data[self.target_column]
            if target.value_counts().min() > 1:
                stratify = target

        return train_test_split(
            self.data,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )

    def train(self) -> "DataLoader":
        train_data, _ = self._train_test_split()
        return self.decorate(train_data.reset_index(drop=True))

    def test(self) -> "DataLoader":
        _, test_data = self._train_test_split()
        return self.decorate(test_data.reset_index(drop=True))

    def fillna(self, value: Any) -> "DataLoader":
        self.data = self.data.fillna(value)
        return self

    def is_tabular(self) -> bool:
        return True


class SurvivalAnalysisDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.SurvivalAnalysisDataLoader
        :parts: 1

    Data Loader for Survival Analysis Data

    Constructor Args:
        data: Union[pd.DataFrame, list, np.ndarray]
            The dataset. Either a Pandas DataFrame or a Numpy Array.
        time_to_event_column: str
            Survival Analysis specific time-to-event feature
        target_column: str
            The outcome: event or censoring.
        sensitive_features: List[str]
            Name of sensitive features.
        important_features: List[str]
            Default: None. Only relevant for SurvivalGAN method.
        target_column: str
            The feature name that provides labels for downstream tasks.
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        domain_column: Optional[str]
            Optional domain label, used for domain adaptation algorithms.
        random_state: int
            Defaults to zero.
        train_size: float
            The ratio to use for train splits.

    Example:
        >>> TODO
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        data: pd.DataFrame,
        time_to_event_column: str,
        target_column: str,
        time_horizons: list = [],
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        fairness_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        if target_column not in data.columns:
            raise ValueError(f"Event column {target_column} not found in the dataframe")

        if time_to_event_column not in data.columns:
            raise ValueError(
                f"Time to event column {time_to_event_column} not found in the dataframe"
            )

        T = data[time_to_event_column]
        data_filtered = data[T > 0]
        row_diff = data.shape[0] - data_filtered.shape[0]
        if row_diff > 0:
            raise ValueError(
                f"The time_to_event_column contains {row_diff} values less than or equal to zero. Please remove them."
            )

        if len(time_horizons) == 0:
            time_horizons = np.linspace(T.min(), T.max(), num=5)[1:-1].tolist()

        data.columns = data.columns.astype(str)

        self.target_column = target_column
        self.time_to_event_column = time_to_event_column
        self.time_horizons = time_horizons
        self.fairness_column = fairness_column

        super().__init__(
            data_type="survival_analysis",
            data=data,
            static_features=list(data.columns.astype(str)),
            sensitive_features=sensitive_features,
            important_features=important_features,
            outcome_features=[self.target_column],
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data.shape

    @property
    def columns(self) -> list:
        return list(self.data.columns)

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    def compression_protected_features(self) -> list:
        out = [self.target_column, self.time_to_event_column]
        domain = self.domain()

        if domain is not None:
            out.append(domain)

        return out

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        X = self.data.drop(columns=[self.target_column, self.time_to_event_column])
        T = self.data[self.time_to_event_column]
        E = self.data[self.target_column]

        if as_numpy:
            return np.asarray(X), np.asarray(T), np.asarray(E)

        return X, T, E

    def dataframe(self) -> pd.DataFrame:
        return self.data

    def numpy(self) -> np.ndarray:
        return self.dataframe().values

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "static_features": list(self.static_features),
            "sensitive_features": self.sensitive_features,
            "important_features": self.important_features,
            "outcome_features": self.outcome_features,
            "target_column": self.target_column,
            "fairness_column": self.fairness_column,
            "time_to_event_column": self.time_to_event_column,
            "time_horizons": self.time_horizons,
            "train_size": self.train_size,
        }

    def __len__(self) -> int:
        return len(self.data)

    def decorate(self, data: Any) -> "DataLoader":
        return SurvivalAnalysisDataLoader(
            data,
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            target_column=self.target_column,
            fairness_column=self.fairness_column,
            time_to_event_column=self.time_to_event_column,
            time_horizons=self.time_horizons,
            random_state=self.random_state,
            train_size=self.train_size,
        )

    def satisfies(self, constraints: Constraints) -> bool:
        return constraints.is_valid(self.data)

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.decorate(
            constraints.match(self.data),
        )

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        return self.decorate(
            self.data.sample(count, random_state=random_state),
        )

    def drop(self, columns: list = []) -> "DataLoader":
        return self.decorate(
            self.data.drop(columns=columns),
        )

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        if not isinstance(data, pd.DataFrame):
            raise ValueError(f"Invalid data type {type(data)}")

        return SurvivalAnalysisDataLoader(
            data,
            target_column=info["target_column"],
            time_to_event_column=info["time_to_event_column"],
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            time_horizons=info["time_horizons"],
            fairness_column=info["fairness_column"],
        )

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self.data[feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self.data[feature] = val

    def train(self) -> "DataLoader":
        stratify = self.data[self.target_column]
        train_data, _ = train_test_split(
            self.data, train_size=self.train_size, random_state=0, stratify=stratify
        )
        return self.decorate(
            train_data.reset_index(drop=True),
        )

    def test(self) -> "DataLoader":
        stratify = self.data[self.target_column]
        _, test_data = train_test_split(
            self.data,
            train_size=self.train_size,
            random_state=0,
            stratify=stratify,
        )
        return self.decorate(
            test_data.reset_index(drop=True),
        )

    def fillna(self, value: Any) -> "DataLoader":
        self.data = self.data.fillna(value)
        return self

    def is_tabular(self) -> bool:
        return True


class TimeSeriesDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.TimeSeriesDataLoader
        :parts: 1

    Data Loader for Time Series Data

    Constructor Args:
        temporal data: List[pd.DataFrame]
            The temporal data. A list of pandas DataFrames
        observation times: List
            List of arrays mapping directly to index of each dataframe in temporal_data
        outcome: Optional[pd.DataFrame] = None
            pandas DataFrame thatn can be anything (eg, labels, regression outcome)
        static_data: Optional[pd.DataFrame] = None
            pandas DataFrame mapping directly to index of each dataframe in temporal_data
        sensitive_features: List[str]
            Name of sensitive features
        important_features List[str]
            Default: None. Only relevant for SurvivalGAN method
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        random_state: int
            Defaults to zero.

    Example:
        >>> TODO
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame] = None,
        static_data: Optional[pd.DataFrame] = None,
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        fairness_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        seq_offset: int = 0,
        **kwargs: Any,
    ) -> None:
        static_features = []
        self.outcome_features = []

        if len(temporal_data) == 0:
            raise ValueError("Empty temporal data")

        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)

        max_window_len = max([len(t) for t in temporal_data])
        if static_data is not None:
            if len(static_data) != len(temporal_data):
                raise ValueError("Static and temporal data mismatch")
            static_features = list(static_data.columns)
        else:
            static_data = pd.DataFrame(np.zeros((len(temporal_data), 0)))

        if outcome is not None:
            if len(outcome) != len(temporal_data):
                raise ValueError("Temporal and outcome data mismatch")
            self.outcome_features = list(outcome.columns)
        else:
            outcome = pd.DataFrame(np.zeros((len(temporal_data), 0)))

        self.window_len = max_window_len
        self.fill = np.nan
        self.seq_offset = seq_offset

        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
            seq_df,
            seq_info,
        ) = TimeSeriesDataLoader.pack_raw_data(
            static_data,
            temporal_data,
            observation_times,
            outcome,
            fill=self.fill,
            seq_offset=seq_offset,
        )
        self.seq_info = seq_info
        self.fairness_column = fairness_column

        super().__init__(
            data={
                "static_data": static_data,
                "temporal_data": temporal_data,
                "observation_times": observation_times,
                "outcome": outcome,
                "seq_data": seq_df,
            },
            data_type="time_series",
            static_features=static_features,
            temporal_features=temporal_features,
            outcome_features=self.outcome_features,
            sensitive_features=sensitive_features,
            important_features=important_features,
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data["seq_data"].shape

    @property
    def columns(self) -> list:
        return self.data["seq_data"].columns

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    def compression_protected_features(self) -> list:
        return self.outcome_features

    @property
    def raw_columns(self) -> list:
        return self.static_features + self.temporal_features + self.outcome_features

    def dataframe(self) -> pd.DataFrame:
        return self.data["seq_data"].copy()

    def numpy(self) -> np.ndarray:
        return self.dataframe().values

    def info(self) -> dict:
        generic_info = {
            "data_type": self.data_type,
            "len": len(self),
            "static_features": self.static_features,
            "temporal_features": self.temporal_features,
            "outcome_features": self.outcome_features,
            "outcome_len": len(self.data["outcome"].values.reshape(-1))
            / len(self.data["outcome"]),
            "window_len": self.window_len,
            "sensitive_features": self.sensitive_features,
            "important_features": self.important_features,
            "fairness_column": self.fairness_column,
            "random_state": self.random_state,
            "train_size": self.train_size,
            "fill": self.fill,
        }

        for key in self.seq_info:
            generic_info[key] = self.seq_info[key]

        return generic_info

    def __len__(self) -> int:
        return len(self.data["seq_data"])

    def decorate(self, data: Any) -> "DataLoader":
        static_data, temporal_data, observation_times, outcome = data

        return TimeSeriesDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            outcome=outcome,
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            fairness_column=self.fairness_column,
            random_state=self.random_state,
            train_size=self.train_size,
            seq_offset=self.seq_offset,
        )

    def unpack_and_decorate(self, data: pd.DataFrame) -> "DataLoader":
        unpacked_data = TimeSeriesDataLoader.unpack_raw_data(
            data,
            self.info(),
        )

        return self.decorate(unpacked_data)

    def satisfies(self, constraints: Constraints) -> bool:
        seq_df = self.dataframe()

        return constraints.is_valid(seq_df)

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.unpack_and_decorate(
            constraints.match(self.dataframe()),
        )

    def drop(self, columns: list = []) -> "DataLoader":
        new_data = self.data["seq_data"].drop(columns=columns)
        return self.unpack_and_decorate(new_data)

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesDataLoader.unpack_raw_data(
            data,
            info,
        )
        return TimeSeriesDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            outcome=outcome,
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            fairness_column=info["fairness_column"],
            fill=info["fill"],
            seq_offset=info["seq_offset"],
        )

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        if pad:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesDataLoader.pad_and_mask(
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )
        else:
            static_data, temporal_data, observation_times, outcome = (
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )

        if as_numpy:
            longest_observation_seq = max([len(seq) for seq in temporal_data])
            padded_temporal_data = np.zeros(
                (len(temporal_data), longest_observation_seq, 5)
            )
            mask = np.ones((len(temporal_data), longest_observation_seq, 5), dtype=bool)
            for i, arr in enumerate(temporal_data):
                padded_temporal_data[i, : arr.shape[0], :] = arr  # Copy the actual data
                mask[
                    i, : arr.shape[0], :
                ] = False  # Set mask to False where actual data is present

            masked_temporal_data = ma.masked_array(padded_temporal_data, mask)
            return (
                np.asarray(static_data),
                masked_temporal_data,  # TODO: check this works with time series benchmarks
                # masked array to handle variable length sequences
                ma.vstack(
                    [
                        ma.array(
                            np.resize(ot, longest_observation_seq),
                            mask=[True for i in range(len(ot))]
                            + [False for j in range(longest_observation_seq - len(ot))],
                        )
                        for ot in observation_times
                    ]
                ),
                np.asarray(outcome),
            )
        return (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        )

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self.data["seq_data"][feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self.data["seq_data"][feature] = val

    def ids(self) -> list:
        id_col = self.seq_info["seq_id_feature"]
        ids = self.data["seq_data"][id_col]

        return list(ids.unique())

    def filter_ids(self, ids_list: list) -> pd.DataFrame:
        seq_data = self.data["seq_data"]
        id_col = self.info()["seq_id_feature"]

        return seq_data[seq_data[id_col].isin(ids_list)]

    def train(self) -> "DataLoader":
        # TODO: stratify
        ids = self.ids()
        train_ids, _ = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
        )
        return self.unpack_and_decorate(self.filter_ids(train_ids))

    def test(self) -> "DataLoader":
        # TODO: stratify
        ids = self.ids()
        _, test_ids = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
        )
        return self.unpack_and_decorate(self.filter_ids(test_ids))

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        ids = self.ids()
        count = min(count, len(ids))
        sampled_ids = random.sample(ids, count)

        return self.unpack_and_decorate(self.filter_ids(sampled_ids))

    def fillna(self, value: Any) -> "DataLoader":
        for key in ["static_data", "outcome", "seq_data"]:
            if self.data[key] is not None:
                self.data[key] = self.data[key].fillna(value)

        for idx, item in enumerate(self.data["temporal_data"]):
            self.data["temporal_data"][idx] = self.data["temporal_data"][idx].fillna(
                value
            )

        return self

    @staticmethod
    def unique_temporal_features(temporal_data: List[pd.DataFrame]) -> List:
        temporal_features = []
        for item in temporal_data:
            temporal_features.extend(item.columns)
        return sorted(np.unique(temporal_features).tolist())

    # Padding helpers
    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pad_raw_features(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
    ) -> Any:
        fill = np.nan

        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)

        for idx, item in enumerate(temporal_data):
            # handling missing features
            for col in temporal_features:
                if col not in item.columns:
                    item[col] = fill
            item = item[temporal_features]

            if list(item.columns) != list(temporal_features):
                raise RuntimeError("Invalid features for packing")

            temporal_data[idx] = item.fillna(fill)

        return static_data, temporal_data, observation_times, outcome

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pad_raw_data(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
    ) -> Any:
        fill = np.nan

        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesDataLoader.pad_raw_features(
            static_data, temporal_data, observation_times, outcome
        )
        max_window_len = max([len(t) for t in temporal_data])
        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)

        for idx, item in enumerate(temporal_data):
            if len(item) != max_window_len:
                pads = fill * np.ones(
                    (max_window_len - len(item), len(temporal_features))
                )
                start = 0
                if len(item.index) > 0:
                    start = max(item.index) + 1
                pads_df = pd.DataFrame(
                    pads,
                    index=[start + i for i in range(len(pads))],
                    columns=item.columns,
                )
                item = pd.concat([item, pads_df])

            # handle missing time points
            if list(item.columns) != list(temporal_features):
                raise RuntimeError(
                    f"Invalid features {item.columns}. Expected {temporal_features}"
                )
            if len(item) != max_window_len:
                raise RuntimeError("Invalid window len")

            temporal_data[idx] = item

        observation_times_padded = []
        for idx, item in enumerate(observation_times):
            item = list(item)
            if len(item) != max_window_len:
                pads = fill * np.ones(max_window_len - len(item))
                item.extend(pads.tolist())
            observation_times_padded.append(item)

        return static_data, temporal_data, observation_times_padded, outcome

    # Masking helpers
    @staticmethod
    def extract_masked_features(full_temporal_features: list) -> tuple:
        temporal_features = []
        mask_features = []
        mask_prefix = "masked_"
        for feat in full_temporal_features:
            feat = str(feat)
            if not feat.startswith(mask_prefix):
                temporal_features.append(feat)
                continue

            other_feat = feat[len(mask_prefix) :]
            if other_feat in full_temporal_features:
                mask_features.append(feat)
            else:
                temporal_features.append(feat)

        return temporal_features, mask_features

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def mask_temporal_data(
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        fill: Any = 0,
    ) -> Any:
        nan_cnt = 0
        for item in temporal_data:
            nan_cnt += np.asarray(np.isnan(item)).sum()

        if nan_cnt == 0:
            return temporal_data, observation_times

        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)
        masked_features = [f"masked_{feat}" for feat in temporal_features]

        for idx, item in enumerate(temporal_data):
            item[masked_features] = (~np.isnan(item)).astype(int)
            item = item.fillna(fill)
            temporal_data[idx] = item

        for idx, item in enumerate(observation_times):
            item = np.nan_to_num(item, nan=fill).tolist()

            observation_times[idx] = item

        return temporal_data, observation_times

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def unmask_temporal_data(
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        fill: Any = np.nan,
    ) -> Any:
        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)
        temporal_features, mask_features = TimeSeriesDataLoader.extract_masked_features(
            temporal_features
        )

        missing_horizons = []
        for idx, item in enumerate(temporal_data):
            # handle existing mask
            if len(mask_features) > 0:
                mask = temporal_data[idx][mask_features].astype(bool)
                item[~mask] = np.nan

            item_missing_rows = item.isna().sum(axis=1).values
            missing_horizons.append(item_missing_rows == len(temporal_features))

            # TODO: review impact on horizons
            temporal_data[idx] = item.dropna()

        observation_times_unmasked = []
        for idx, item in enumerate(observation_times):
            item = list(item)

            for midx, mval in enumerate(missing_horizons[idx]):
                if mval:
                    item[midx] = np.nan

            local_horizons = list(filter(lambda v: v == v, item))
            observation_times_unmasked.append(local_horizons)

        return temporal_data, observation_times_unmasked

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pad_and_mask(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
        only_features: Any = False,
        fill: Any = 0,
    ) -> Any:
        if only_features:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesDataLoader.pad_raw_features(
                static_data,
                temporal_data,
                observation_times,
                outcome,
            )
        else:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesDataLoader.pad_raw_data(
                static_data,
                temporal_data,
                observation_times,
                outcome,
            )

        temporal_data, observation_times = TimeSeriesDataLoader.mask_temporal_data(
            temporal_data, observation_times, fill=fill
        )

        return static_data, temporal_data, observation_times, outcome

    @staticmethod
    def sequential_view(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
        id_col: str = "seq_id",
        time_id_col: str = "seq_time_id",
        seq_offset: int = 0,
    ) -> Tuple[pd.DataFrame, dict]:  # sequential dataframe, loader info
        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesDataLoader.pad_and_mask(
            static_data, temporal_data, observation_times, outcome, only_features=True
        )
        raw_static_features = list(static_data.columns)
        static_features = [f"seq_static_{col}" for col in raw_static_features]

        raw_outcome_features = list(outcome.columns)
        outcome_features = [f"seq_out_{col}" for col in raw_outcome_features]

        raw_temporal_features = TimeSeriesDataLoader.unique_temporal_features(
            temporal_data
        )
        temporal_features = [f"seq_temporal_{col}" for col in raw_temporal_features]
        cols = (
            [id_col, time_id_col]
            + static_features
            + temporal_features
            + outcome_features
        )

        seq = []
        for sidx, static_item in static_data.iterrows():
            real_tidx = 0
            for tidx, temporal_item in temporal_data[sidx].iterrows():
                local_seq_data = (
                    [
                        sidx + seq_offset,
                        observation_times[sidx][real_tidx],
                    ]
                    + static_item[raw_static_features].values.tolist()
                    + temporal_item[raw_temporal_features].values.tolist()
                    + outcome.loc[sidx, raw_outcome_features].values.tolist()
                )
                seq.append(local_seq_data)
                real_tidx += 1

        seq_df = pd.DataFrame(seq, columns=cols)
        info = {
            "seq_static_features": static_features,
            "seq_temporal_features": temporal_features,
            "seq_outcome_features": outcome_features,
            "seq_offset": seq_offset,
            "seq_id_feature": id_col,
            "seq_time_id_feature": time_id_col,
            "seq_features": list(seq_df.columns),
        }
        return seq_df, info

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pack_raw_data(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
        fill: Any = np.nan,
        seq_offset: int = 0,
    ) -> pd.DataFrame:
        # Temporal data: (subjects, temporal_sequence, temporal_feature)
        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)
        temporal_features, mask_features = TimeSeriesDataLoader.extract_masked_features(
            temporal_features
        )
        temporal_data, observation_times = TimeSeriesDataLoader.unmask_temporal_data(
            temporal_data, observation_times
        )
        seq_df, info = TimeSeriesDataLoader.sequential_view(
            static_data=static_data,
            temporal_data=temporal_data,
            observation_times=observation_times,
            outcome=outcome,
            seq_offset=seq_offset,
        )

        return static_data, temporal_data, observation_times, outcome, seq_df, info

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def unpack_raw_data(
        data: pd.DataFrame,
        info: dict,
    ) -> Tuple[
        Optional[pd.DataFrame], List[pd.DataFrame], List, Optional[pd.DataFrame]
    ]:
        id_col = info["seq_id_feature"]
        time_col = info["seq_time_id_feature"]

        static_cols = info["seq_static_features"]
        new_static_cols = [feat.split("seq_static_")[1] for feat in static_cols]

        temporal_cols = info["seq_temporal_features"]
        new_temporal_cols = [feat.split("seq_temporal_")[1] for feat in temporal_cols]

        outcome_cols = info["seq_outcome_features"]
        new_outcome_cols = [feat.split("seq_out_")[1] for feat in outcome_cols]

        ids = sorted(list(set(data[id_col])))

        static_data = []
        temporal_data = []
        observation_times = []
        outcome_data = []

        for item_id in ids:
            item_data = data[data[id_col] == item_id]

            static_data.append(item_data[static_cols].head(1).values.squeeze().tolist())
            outcome_data.append(
                item_data[outcome_cols].head(1).values.squeeze().tolist()
            )
            local_temporal_data = item_data[temporal_cols].copy()
            local_observation_times = item_data[time_col].values.tolist()
            local_temporal_data.columns = new_temporal_cols
            # TODO: review impact on horizons
            local_temporal_data = local_temporal_data.dropna()

            temporal_data.append(local_temporal_data)
            observation_times.append(local_observation_times)

        static_df = pd.DataFrame(static_data, columns=new_static_cols)
        outcome_df = pd.DataFrame(outcome_data, columns=new_outcome_cols)

        return static_df, temporal_data, observation_times, outcome_df

    def is_tabular(self) -> bool:
        return True


class TimeSeriesSurvivalDataLoader(TimeSeriesDataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.TimeSeriesSurvivalDataLoader
        :parts: 1

    Data loader for Time series survival data

    Constructor Args:
        temporal_data: List[pd.DataFrame}
            The temporal data. A list of pandas DataFrames.
        observation_times: List
            List of arrays mapping directly to index of each dataframe in temporal_data
        T: Union[pd.Series, np.ndarray, pd.Series]
            Time-to-event data
        E: Union[pd.Series, np.ndarray, pd.Series]
            E is censored/event data
        static_data Optional[pd.DataFrame] = None
            pandas DataFrame of static features for each subject
        sensitive_features: List[str]
            Name of sensitive features
        important_features: List[str}
            Default: None. Only relevant for SurvivalGAN method.
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        random_state. int
            Defaults to zero.

    Example:
        >>> TODO

    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        temporal_data: List[pd.DataFrame],
        observation_times: Union[List, np.ndarray, pd.Series],
        T: Union[pd.Series, np.ndarray, pd.Series],
        E: Union[pd.Series, np.ndarray, pd.Series],
        static_data: Optional[pd.DataFrame] = None,
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        time_horizons: list = [],
        fairness_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        seq_offset: int = 0,
        **kwargs: Any,
    ) -> None:
        self.time_to_event_col = "time_to_event"
        self.event_col = "event"
        self.fairness_column = fairness_column

        if len(time_horizons) == 0:
            time_horizons = np.linspace(T.min(), T.max(), num=5)[1:-1].tolist()
        self.time_horizons = time_horizons
        outcome = pd.concat([pd.Series(T), pd.Series(E)], axis=1)
        outcome.columns = [self.time_to_event_col, self.event_col]

        self.fill = np.nan

        super().__init__(
            temporal_data=temporal_data,
            observation_times=observation_times,
            outcome=outcome,
            static_data=static_data,
            sensitive_features=sensitive_features,
            important_features=important_features,
            random_state=random_state,
            train_size=train_size,
            seq_offset=seq_offset,
            **kwargs,
        )
        self.data_type = "time_series_survival"

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    def info(self) -> dict:
        parent_info = super().info()
        parent_info["time_to_event_column"] = self.time_to_event_col
        parent_info["event_column"] = self.event_col
        parent_info["time_horizons"] = self.time_horizons
        parent_info["fill"] = self.fill

        return parent_info

    def decorate(self, data: Any) -> "DataLoader":
        static_data, temporal_data, observation_times, outcome = data
        if self.time_to_event_col not in outcome:
            raise ValueError(
                f"Survival outcome is missing tte column {self.time_to_event_col}"
            )
        if self.event_col not in outcome:
            raise ValueError(
                f"Survival outcome is missing event column {self.event_col}"
            )

        return TimeSeriesSurvivalDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            T=outcome[self.time_to_event_col],
            E=outcome[self.event_col],
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            fairness_column=self.fairness_column,
            random_state=self.random_state,
            time_horizons=self.time_horizons,
            train_size=self.train_size,
            seq_offset=self.seq_offset,
        )

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesSurvivalDataLoader.unpack_raw_data(
            data,
            info,
        )
        return TimeSeriesSurvivalDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            T=outcome[info["time_to_event_column"]],
            E=outcome[info["event_column"]],
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            fairness_column=info["fairness_column"],
            time_horizons=info["time_horizons"],
            seq_offset=info["seq_offset"],
        )

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        if pad:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesSurvivalDataLoader.pad_and_mask(
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )
        else:
            static_data, temporal_data, observation_times, outcome = (
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )

        if as_numpy:
            return (
                np.asarray(static_data),
                np.asarray(temporal_data, dtype=object),
                np.asarray(observation_times, dtype=object),
                np.asarray(outcome[self.time_to_event_col]),
                np.asarray(outcome[self.event_col]),
            )
        return (
            static_data,
            temporal_data,
            observation_times,
            outcome[self.time_to_event_col],
            outcome[self.event_col],
        )

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.unpack_and_decorate(
            constraints.match(self.dataframe()),
        )

    def train(self) -> "DataLoader":
        stratify = self.data["outcome"][self.event_col]

        ids = self.ids()
        train_ids, _ = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )
        return self.unpack_and_decorate(self.filter_ids(train_ids))

    def test(self) -> "DataLoader":
        stratify = self.data["outcome"][self.event_col]
        ids = self.ids()
        _, test_ids = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )
        return self.unpack_and_decorate(self.filter_ids(test_ids))


class ImageDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.ImageDataLoader
        :parts: 1

    Data loader for generic image data.

    Constructor Args:
        data: torch.utils.data.Dataset or torch.Tensor
            The image dataset or a tuple of (tensor images, tensor labels)
        random_state: int
            Defaults to zero.
        height: int. Default = 32
            Height to use internally
        width: Optional[int]
            Optional width to use internally. If None, it is used the same value as height.
        train_size: float = 0.8
            Train dataset ratio.
    Example:
        >>> dataset = datasets.MNIST(".", download=True)
        >>>
        >>> loader = ImageDataLoader(
        >>>     data=dataset,
        >>>     train_size=0.8,
        >>>     height=32,
        >>>     width=w32,
        >>> )

    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        data: Union[torch.utils.data.Dataset, Tuple[torch.Tensor, torch.Tensor]],
        height: int = 32,
        width: Optional[int] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        if width is None:
            width = height

        if isinstance(data, tuple):
            X, y = data
            data = TensorDataset(images=X, targets=y)

        self.data_transform = None

        dummy, _ = data[0]
        img_transform = []
        if not isinstance(dummy, PIL.Image.Image):
            img_transform = [transforms.ToPILImage()]

        img_transform.extend(
            [
                transforms.Resize((height, width)),
                transforms.ToTensor(),
                transforms.Normalize(mean=(0.5,), std=(0.5,)),
            ]
        )

        self.data_transform = transforms.Compose(img_transform)
        data = FlexibleDataset(data, transform=self.data_transform)

        self.height = height
        self.width = width
        self.channels = data.shape()[1]

        super().__init__(
            data_type="images",
            data=data,
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data.shape()

    def get_fairness_column(self) -> None:
        """Not implemented for ImageDataLoader"""
        ...

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        return self.data

    def numpy(self) -> np.ndarray:
        x, _ = self.data.numpy()

        return x

    def dataframe(self) -> pd.DataFrame:
        x = self.numpy().reshape(len(self), -1)

        x = pd.DataFrame(x)
        x.columns = x.columns.astype(str)

        return x

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "train_size": self.train_size,
            "height": self.height,
            "width": self.width,
            "channels": self.channels,
            "random_state": self.random_state,
        }

    def __len__(self) -> int:
        return len(self.data)

    def decorate(self, data: Any) -> "DataLoader":
        return ImageDataLoader(
            data,
            random_state=self.random_state,
            train_size=self.train_size,
            height=self.height,
            width=self.width,
        )

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        idxs = np.random.choice(len(self), count, replace=False)
        subset = FlexibleDataset(self.data.data, indices=idxs)
        return self.decorate(subset)

    @staticmethod
    def from_info(data: torch.utils.data.Dataset, info: dict) -> "ImageDataLoader":
        if not isinstance(data, torch.utils.data.Dataset):
            raise ValueError(f"Invalid data type {type(data)}")

        return ImageDataLoader(
            data,
            train_size=info["train_size"],
            height=info["height"],
            width=info["width"],
            random_state=info["random_state"],
        )

    def __getitem__(self, index: Union[list, int, str]) -> Any:
        if isinstance(index, str):
            return self.dataframe()[index]

        return self.numpy()[index]

    def _train_test_split(self) -> Tuple:
        indices = np.arange(len(self.data))
        _, stratify = self.data.numpy()

        return train_test_split(
            indices,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )

    def train(self) -> "DataLoader":
        train_idx, _ = self._train_test_split()
        subset = FlexibleDataset(self.data.data, indices=train_idx)
        return self.decorate(subset)

    def test(self) -> "DataLoader":
        _, test_idx = self._train_test_split()
        subset = FlexibleDataset(self.data.data, indices=test_idx)
        return self.decorate(subset)

    def compress(
        self,
    ) -> Tuple["DataLoader", Dict]:
        return self, {}

    def decompress(self, context: Dict) -> "DataLoader":
        return self

    def encode(
        self,
        encoders: Optional[Dict[str, Any]] = None,
    ) -> Tuple["DataLoader", Dict]:
        return self, {}

    def decode(
        self,
        encoders: Dict[str, Any],
    ) -> "DataLoader":
        return self

    def is_tabular(self) -> bool:
        return False

    @property
    def columns(self) -> list:
        return list(self.dataframe().columns)

    def satisfies(self, constraints: Constraints) -> bool:
        return True

    def match(self, constraints: Constraints) -> "DataLoader":
        return self

    def compression_protected_features(self) -> list:
        raise NotImplementedError("Images do not support the compression call")

    def drop(self, columns: list = []) -> "DataLoader":
        raise NotImplementedError()

    def __setitem__(self, feature: str, val: Any) -> None:
        raise NotImplementedError()

    def fillna(self, value: Any) -> "DataLoader":
        raise NotImplementedError()



@validate_arguments(config=dict(arbitrary_types_allowed=True))
def create_from_info(
    data: Union[pd.DataFrame, torch.utils.data.Dataset], info: dict
) -> "DataLoader":
    """Helper for creating a DataLoader from existing information."""
    if info["data_type"] == "generic":
        return GenericDataLoader.from_info(data, info)
    elif info["data_type"] == "survival_analysis":
        return SurvivalAnalysisDataLoader.from_info(data, info)
    elif info["data_type"] == "time_series":
        return TimeSeriesDataLoader.from_info(data, info)
    elif info["data_type"] == "time_series_survival":
        return TimeSeriesSurvivalDataLoader.from_info(data, info)
    elif info["data_type"] == "images":
        return ImageDataLoader.from_info(data, info)
    else:
        raise RuntimeError(f"invalid datatype {info}")


from synthcity.plugins.core.constraints import Constraints
from synthcity.plugins.core.models.syn_seq_encoder import Syn_SeqEncoder


class Syn_SeqDataLoader(DataLoader):
    """
    A DataLoader that applies Syn_Seq-style preprocessing to input data,
    inheriting directly from DataLoader and implementing all required
    abstract methods.

    - syn_order: The order of columns to keep or process. If not provided (None or empty),
                 use the raw DataFrame column order.
    - columns_special_values: A dict of { column_name : list_of_special_values },
      specifying which values to treat as 'special' or missing in numeric columns.
    - col_type: A dict { column_name : "category"/"numeric"/"date"/... } used to force
      how each column is handled.
    - max_categories: numeric threshold for deciding numeric vs. categorical if col_type
      isn't specified.
    """

    def __init__(
        self,
        data: pd.DataFrame,
        syn_order: Optional[List[str]] = None,
        special_value: Optional[Dict[str, List[Any]]] = None,
        col_type: Optional[Dict[str, str]] = None,
        max_categories: int = 20,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        """
        Args:
            data: the raw DataFrame
            syn_order: optional list of columns in the desired processing order
            special_value: { "col_name": [list_of_special_values], ... }
            col_type: { "col_name": "category"/"numeric"/"date"/... }
            max_categories: threshold for deciding numeric vs. categorical if not declared
            random_state: for reproducibility
            train_size: fraction of data for 'train' (rest goes to 'test')
        """
        if not syn_order:
            print("[INFO] syn_order not provided; using data.columns as default.")
            syn_order = list(data.columns)

        # ensure all requested columns exist
        missing_columns = set(syn_order) - set(data.columns)
        if missing_columns:
            raise ValueError(f"Missing columns in input data: {missing_columns}")

        # store user parameters
        self.syn_order = syn_order
        self.columns_special_values = special_value or {}
        self.col_type = col_type or {}
        self.max_categories = max_categories

        # reorder data based on syn_order
        filtered_data = data[self.syn_order].copy()

        # call parent constructor
        super().__init__(
            data_type="syn_seq",
            data=filtered_data,
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

        # keep a reference
        self._df = filtered_data

        # debug print
        print("[INFO] Syn_SeqDataLoader init complete:")
        print(f"  - syn_order: {self.syn_order}")
        print(f"  - special_value (columns_special_values): {self.columns_special_values}")
        print(f"  - col_type: {self.col_type}")
        print(f"  - data shape: {self._df.shape}")

        # create + fit our Syn_SeqEncoder (just fit, actual transform is in encode())
        self._encoder = Syn_SeqEncoder(
            columns_special_values=self.columns_special_values,
            syn_order=self.syn_order,
            max_categories=self.max_categories,
            col_type=self.col_type,
        )
        self._encoder.fit(self._df)

        print("[DEBUG] After encoder.fit(), detected info:")
        print(f"  - encoder.column_order_: {self._encoder.column_order_}")
        print(f"  - numeric_info_: {self._encoder.numeric_info_}")
        print(f"  - categorical_info_: {self._encoder.categorical_info_}")
        if self._encoder.variable_selection_ is not None:
            print("  - variable_selection_:\n", self._encoder.variable_selection_)
        print("----------------------------------------------------------------")

    # ----------------------------------------------------------------
    # Inherited/required abstract methods
    # ----------------------------------------------------------------
    @property
    def shape(self) -> tuple:
        return self._df.shape

    @property
    def columns(self) -> list:
        return list(self._df.columns)

    def dataframe(self) -> pd.DataFrame:
        return self._df

    def numpy(self) -> pd.DataFrame:
        return self._df.values

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "train_size": self.train_size,
            "random_state": self.random_state,
            "syn_order": self.syn_order,
            "max_categories": self.max_categories,
            "col_type": self.col_type,
            "columns_special_values": self.columns_special_values,
        }

    def __len__(self) -> int:
        return len(self._df)

    def satisfies(self, constraints: Constraints) -> bool:
        return constraints.is_valid(self._df)

    def match(self, constraints: Constraints) -> "Syn_SeqDataLoader":
        matched_df = constraints.match(self._df)
        return self.decorate(matched_df)

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "Syn_SeqDataLoader":
        return Syn_SeqDataLoader(
            data=data,
            syn_order=info.get("syn_order"),
            special_value=info.get("columns_special_values", {}),
            col_type=info.get("col_type", {}),
            max_categories=info.get("max_categories", 20),
            random_state=info["random_state"],
            train_size=info["train_size"],
        )

    def sample(self, count: int, random_state: int = 0) -> "Syn_SeqDataLoader":
        sampled_df = self._df.sample(count, random_state=random_state)
        return self.decorate(sampled_df)

    def drop(self, columns: list = []) -> "Syn_SeqDataLoader":
        dropped_df = self._df.drop(columns=columns, errors="ignore")
        return self.decorate(dropped_df)

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self._df[feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self._df[feature] = val

    def train(self) -> "Syn_SeqDataLoader":
        ntrain = int(len(self._df) * self.train_size)
        train_df = self._df.iloc[:ntrain].copy()
        return self.decorate(train_df)

    def test(self) -> "Syn_SeqDataLoader":
        ntrain = int(len(self._df) * self.train_size)
        test_df = self._df.iloc[ntrain:].copy()
        return self.decorate(test_df)

    def fillna(self, value: Any) -> "Syn_SeqDataLoader":
        filled_df = self._df.fillna(value)
        return self.decorate(filled_df)

    def compression_protected_features(self) -> list:
        return []

    def is_tabular(self) -> bool:
        return True

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        if as_numpy:
            return self._df.to_numpy()
        return self._df

    def get_fairness_column(self) -> Union[str, Any]:
        return None

    # ----------------------------------------------------------------
    # Syn_Seq-specific encode/decode
    # ----------------------------------------------------------------
    def encode(
        self, encoders: Optional[Dict[str, Any]] = None
    ) -> Tuple["Syn_SeqDataLoader", Dict]:
        if encoders is None:
            encoded_data = self._encoder.transform(self._df)
            new_loader = self.decorate(encoded_data)
            return new_loader, {"syn_seq_encoder": self._encoder}
        else:
            return self, encoders

    def decode(self, encoders: Dict[str, Any]) -> "Syn_SeqDataLoader":
        if "syn_seq_encoder" in encoders:
            encoder = encoders["syn_seq_encoder"]
            if not isinstance(encoder, Syn_SeqEncoder):
                raise TypeError(f"Expected Syn_SeqEncoder, got {type(encoder)}")
            decoded_data = encoder.inverse_transform(self._df)
            return self.decorate(decoded_data)
        return self

    def decorate(self, data: pd.DataFrame) -> "Syn_SeqDataLoader":
        """
        Helper for creating a new instance with the same settings but new data.
        """
        return Syn_SeqDataLoader(
            data=data,
            syn_order=self.syn_order,
            special_value=self.columns_special_values,
            col_type=self.col_type,
            max_categories=self.max_categories,
            random_state=self.random_state,
            train_size=self.train_size,
        )


src/synthcity/plugins/generic/plugin_syn_seq.py
# File: plugin_syn_seq.py

from typing import Any, Dict, List, Optional, Union

import pandas as pd
import numpy as np

# synthcity absolute
from synthcity.plugins.core.plugin import Plugin
from synthcity.plugins.core.dataloader import DataLoader, Syn_SeqDataLoader
from synthcity.plugins.core.schema import Schema
from synthcity.plugins.core.constraints import Constraints

# local aggregator
from synthcity.plugins.core.models.syn_seq import Syn_Seq


class Syn_SeqPlugin(Plugin):
    """
    A plugin for a sequential (column-by-column) synthetic data approach,
    mirroring R's 'synthpop'. Internally, it delegates to the `Syn_Seq`
    aggregator from syn_seq.py for the actual column-by-column (sequential) logic.

    - This is intended for sequential regression style: each column is modeled
      in the order, using prior columns as predictors.
    - We rely on the custom `Syn_SeqDataLoader`, which organizes columns,
      applies a Syn_SeqEncoder, etc.
    - The aggregator's generate(...) returns a Syn_SeqDataLoader as well.

    Basic usage:

        # Suppose we have df, and we wrap it in a Syn_SeqDataLoader:
        loader = Syn_SeqDataLoader(
            data = df, 
            syn_order = [...],
            col_type = {...},
            special_value = {...},
        )

        # Build plugin
        syn_model = Syn_SeqPlugin(
            random_state=42,
            default_first_method="SWR", 
            default_other_method="CART",
            # optionally strict=True, sampling_patience=500, ...
        )

        # Fit with per-column method
        methods = ["SWR"] + ["CART"]*(len(loader.columns)-1)
        var_sel = {"N2": ["C1","C2"], "N1":["C1","C2","N2"]}

        syn_model.fit(loader, method=methods, variable_selection=var_sel)

        # Now generate
        synthetic_data_loader = syn_model.generate(count=100)
        synthetic_df = synthetic_data_loader.dataframe()

        # If you need constraints:
        constraints = {
          "N1": [">", 100],
          "C2": ["in", ["A","B"]]
        }
        synthetic_data_loader = syn_model.generate(count=100, constraints=constraints)
        # Above remains a Syn_SeqDataLoader
    """

    @staticmethod
    def name() -> str:
        return "syn_seq"

    @staticmethod
    def type() -> str:
        return "syn_seq"

    @staticmethod
    def hyperparameter_space(**kwargs: Any) -> list:
        # No tunable hyperparameters for now
        return []

    def __init__(
        self,
        random_state: int = 0,
        default_first_method: str = "SWR",
        default_other_method: str = "CART",
        **kwargs: Any,
    ):
        """
        Args:
            random_state: For reproducibility.
            default_first_method: fallback method for the first column if user doesn't override.
            default_other_method: fallback method for subsequent columns.
            **kwargs: forwarded to Plugin(...) => can contain strict, sampling_patience, etc.
        """
        super().__init__(random_state=random_state, **kwargs)

        # We hold a Syn_Seq aggregator. This aggregator does the real sequential logic:
        self._aggregator = Syn_Seq(
            random_state=random_state,
            default_first_method=default_first_method,
            default_other_method=default_other_method,
            strict=self.strict,
            sampling_patience=self.sampling_patience,
        )
        self._model_trained = False

    def _fit(
        self,
        X: DataLoader,
        method: Optional[List[str]] = None,
        variable_selection: Optional[Dict[str, List[str]]] = None,
        *args: Any,
        **kwargs: Any
    ) -> "Syn_SeqPlugin":
        """
        We expect X to be a Syn_SeqDataLoader for sequential usage.
        We pass 'method' and 'variable_selection' to the aggregator.
        """
        if not isinstance(X, Syn_SeqDataLoader):
            raise TypeError("Syn_SeqPlugin expects a Syn_SeqDataLoader for sequential usage.")

        self._aggregator.fit(
            loader=X,
            method=method,
            variable_selection=variable_selection,
            *args,
            **kwargs
        )
        self._model_trained = True
        return self

    def _generate(
        self,
        count: int,
        syn_schema: Schema,
        constraints: Optional[Constraints] = None,
        **kwargs: Any
    ) -> DataLoader:
        """
        Create synthetic data as a Syn_SeqDataLoader. We pass optional constraints
        (which might be a dictionary or a Constraints object) to aggregator.
        """
        if not self._model_trained:
            raise RuntimeError("Must fit Syn_SeqPlugin before generating data.")

        return self._aggregator.generate(
            count=count,
            constraint=constraints,
            **kwargs
        )

plugin = Syn_SeqPlugin

(base) minkeychang@Minkeys-Laptop synthcity % tree
.
 CONTRIBUTING.MD
 INSTRUCTION.txt
 LICENSE
 README.md
 code_collector.py
 code_collector2.py
 docs
    Makefile
    README.md
    _templates
       module.rst_t
    advanced.rst
    arch.png
    conf.py
    dataloaders.rst
    examples.rst
    generators.rst
    index.rst
    logo.png
    logo_text.png
    make.bat
    metrics.rst
    requirements.txt
    tutorials -> ../tutorials/\012
 prereq.txt
 prompt.txt
 prompt2.txt
 pyproject.toml
 setup.cfg
 setup.py
 src
    synthcity
       __init__.py
       __pycache__
          __init__.cpython-311.pyc
          __init__.cpython-39.pyc
          logger.cpython-311.pyc
          logger.cpython-39.pyc
          version.cpython-39.pyc
       benchmark
          __init__.py
          utils.py
       logger.py
       metrics
          __init__.py
          __pycache__
             __init__.cpython-311.pyc
             __init__.cpython-39.pyc
             _utils.cpython-39.pyc
             eval.cpython-311.pyc
             eval.cpython-39.pyc
             eval_detection.cpython-39.pyc
             eval_performance.cpython-39.pyc
             eval_privacy.cpython-39.pyc
             eval_sanity.cpython-39.pyc
             eval_statistical.cpython-39.pyc
             plots.cpython-39.pyc
             scores.cpython-39.pyc
             weighted_metrics.cpython-39.pyc
          _utils.py
          core
             __init__.py
             __pycache__
                __init__.cpython-39.pyc
                metric.cpython-39.pyc
             metric.py
          eval.py
          eval_attacks.py
          eval_detection.py
          eval_performance.py
          eval_privacy.py
          eval_sanity.py
          eval_statistical.py
          plots.py
          representations
             OneClass.py
             __init__.py
             __pycache__
                OneClass.cpython-39.pyc
                __init__.cpython-39.pyc
                networks.cpython-39.pyc
             networks.py
          scores.py
          weighted_metrics.py
       plugins
          __init__.py
          __pycache__
             __init__.cpython-311.pyc
             __init__.cpython-39.pyc
          core
             __init__.py
             __pycache__
                __init__.cpython-311.pyc
                __init__.cpython-39.pyc
                constraints.cpython-39.pyc
                dataloader.cpython-311.pyc
                dataloader.cpython-39.pyc
                dataset.cpython-39.pyc
                distribution.cpython-39.pyc
                plugin.cpython-311.pyc
                plugin.cpython-39.pyc
                schema.cpython-39.pyc
                serializable.cpython-39.pyc
             constraints.py
             dataloader.py
             dataset.py
             distribution.py
             models
                RGCNConv.py
                __init__.py
                __pycache__
                   __init__.cpython-39.pyc
                   aim.cpython-39.pyc
                   bnaf.cpython-39.pyc
                   convnet.cpython-39.pyc
                   factory.cpython-39.pyc
                   feature_encoder.cpython-39.pyc
                   flows.cpython-39.pyc
                   functions.cpython-39.pyc
                   gan.cpython-39.pyc
                   goggle.cpython-39.pyc
                   image_gan.cpython-39.pyc
                   layers.cpython-39.pyc
                   mlp.cpython-39.pyc
                   syn_seq.cpython-39.pyc
                   syn_seq_encoder.cpython-39.pyc
                   tabular_aim.cpython-39.pyc
                   tabular_arf.cpython-39.pyc
                   tabular_encoder.cpython-39.pyc
                   tabular_flows.cpython-39.pyc
                   tabular_gan.cpython-39.pyc
                   tabular_goggle.cpython-39.pyc
                   tabular_great.cpython-39.pyc
                   tabular_vae.cpython-39.pyc
                   transformer.cpython-39.pyc
                   ts_gan.cpython-39.pyc
                   ts_model.cpython-39.pyc
                   ts_tabular_gan.cpython-39.pyc
                   ts_tabular_vae.cpython-39.pyc
                   ts_vae.cpython-39.pyc
                   vae.cpython-39.pyc
                aim.py
                bnaf.py
                convnet.py
                dag
                   __init__.py
                   __pycache__
                      __init__.cpython-39.pyc
                      data.cpython-39.pyc
                      dsl.cpython-39.pyc
                      dstruct.cpython-39.pyc
                      utils.cpython-39.pyc
                   data.py
                   dsl.py
                   dstruct.py
                   utils.py
                factory.py
                feature_encoder.py
                flows.py
                functions.py
                gan.py
                goggle.py
                image_gan.py
                layers.py
                mbi
                   __init__.py
                   __pycache__
                      __init__.cpython-39.pyc
                      callbacks.cpython-39.pyc
                      clique_vector.cpython-39.pyc
                      dataset.cpython-39.pyc
                      domain.cpython-39.pyc
                      factor.cpython-39.pyc
                      graphical_model.cpython-39.pyc
                      identity.cpython-39.pyc
                      inference.cpython-39.pyc
                      junction_tree.cpython-39.pyc
                   callbacks.py
                   clique_vector.py
                   dataset.py
                   domain.py
                   factor.py
                   factor_graph.py
                   graphical_model.py
                   identity.py
                   inference.py
                   junction_tree.py
                   local_inference.py
                   mechanism.py
                   mixture_inference.py
                   public_inference.py
                   region_graph.py
                   torch_factor.py
                mlp.py
                survival_analysis
                   __init__.py
                   __pycache__
                      __init__.cpython-39.pyc
                      _base.cpython-39.pyc
                      benchmarks.cpython-39.pyc
                      loader.cpython-39.pyc
                      metrics.cpython-39.pyc
                      surv_aft.cpython-39.pyc
                      surv_coxph.cpython-39.pyc
                      surv_deephit.cpython-39.pyc
                      surv_xgb.cpython-39.pyc
                   _base.py
                   benchmarks.py
                   loader.py
                   metrics.py
                   surv_aft.py
                   surv_coxph.py
                   surv_deephit.py
                   surv_xgb.py
                   third_party
                       __init__.py
                       __pycache__
                          __init__.cpython-39.pyc
                          metrics.cpython-39.pyc
                          nonparametric.cpython-39.pyc
                          util.cpython-39.pyc
                       metrics.py
                       nonparametric.py
                       util.py
                syn_seq
                   __init__.py
                   cart.py
                   ctree.py
                   logreg.py
                   misc.py
                   norm.py
                   pmm.py
                   polyreg.py
                   rf.py
                   syn_seq.py
                   syn_seq_constraints.py
                   syn_seq_encoder.py
                tabnet.py
                tabular_aim.py
                tabular_arf.py
                tabular_ddpm
                   __init__.py
                   __pycache__
                      __init__.cpython-39.pyc
                      gaussian_multinomial_diffsuion.cpython-39.pyc
                      modules.cpython-39.pyc
                      utils.cpython-39.pyc
                   gaussian_multinomial_diffsuion.py
                   modules.py
                   utils.py
                tabular_encoder.py
                tabular_flows.py
                tabular_gan.py
                tabular_goggle.py
                tabular_great.py
                tabular_vae.py
                time_series_survival
                   __init__.py
                   __pycache__
                      __init__.cpython-39.pyc
                      _base.cpython-39.pyc
                      benchmarks.cpython-39.pyc
                      loader.cpython-39.pyc
                      ts_surv_coxph.cpython-39.pyc
                      ts_surv_dynamic_deephit.cpython-39.pyc
                      ts_surv_xgb.cpython-39.pyc
                      utils.cpython-39.pyc
                   _base.py
                   benchmarks.py
                   loader.py
                   ts_surv_coxph.py
                   ts_surv_dynamic_deephit.py
                   ts_surv_xgb.py
                   utils.py
                time_to_event
                   __init__.py
                   __pycache__
                      __init__.cpython-39.pyc
                      _base.cpython-39.pyc
                      benchmarks.cpython-39.pyc
                      loader.cpython-39.pyc
                      metrics.cpython-39.pyc
                      tte_aft.cpython-39.pyc
                      tte_coxph.cpython-39.pyc
                      tte_date.cpython-39.pyc
                      tte_deephit.cpython-39.pyc
                      tte_survival_function_regression.cpython-39.pyc
                      tte_tenn.cpython-39.pyc
                      tte_xgb.cpython-39.pyc
                   _base.py
                   benchmarks.py
                   loader.py
                   metrics.py
                   tte_aft.py
                   tte_coxph.py
                   tte_date.py
                   tte_deephit.py
                   tte_survival_function_regression.py
                   tte_survival_time_series.py
                   tte_tenn.py
                   tte_xgb.py
                transformer.py
                ts_gan.py
                ts_model.py
                ts_tabular_gan.py
                ts_tabular_vae.py
                ts_vae.py
                vae.py
             plugin.py
             schema.py
             serializable.py
          domain_adaptation
             __init__.py
             __pycache__
                plugin_radialgan.cpython-39.pyc
             plugin_radialgan.py
          generic
             __init__.py
             __pycache__
                plugin_arf.cpython-39.pyc
                plugin_bayesian_network.cpython-39.pyc
                plugin_ctgan.cpython-39.pyc
                plugin_ddpm.cpython-39.pyc
                plugin_dummy_sampler.cpython-39.pyc
                plugin_goggle.cpython-39.pyc
                plugin_great.cpython-39.pyc
                plugin_marginal_distributions.cpython-39.pyc
                plugin_nflow.cpython-39.pyc
                plugin_rtvae.cpython-39.pyc
                plugin_syn_seq.cpython-39.pyc
                plugin_tvae.cpython-39.pyc
                plugin_uniform_sampler.cpython-39.pyc
             plugin_arf.py
             plugin_bayesian_network.py
             plugin_ctgan.py
             plugin_ddpm.py
             plugin_dummy_sampler.py
             plugin_goggle.py
             plugin_great.py
             plugin_marginal_distributions.py
             plugin_nflow.py
             plugin_rtvae.py
             plugin_syn_seq.py
             plugin_tvae.py
             plugin_uniform_sampler.py
          images
             __init__.py
             __pycache__
                plugin_image_adsgan.cpython-39.pyc
                plugin_image_cgan.cpython-39.pyc
             plugin_image_adsgan.py
             plugin_image_cgan.py
          privacy
             __init__.py
             __pycache__
                plugin_adsgan.cpython-39.pyc
                plugin_aim.cpython-39.pyc
                plugin_decaf.cpython-39.pyc
                plugin_dpgan.cpython-39.pyc
                plugin_pategan.cpython-39.pyc
                plugin_privbayes.cpython-39.pyc
             plugin_adsgan.py
             plugin_aim.py
             plugin_decaf.py
             plugin_dpgan.py
             plugin_pategan.py
             plugin_privbayes.py
          survival_analysis
             __init__.py
             __pycache__
                __init__.cpython-39.pyc
                _survival_pipeline.cpython-39.pyc
                plugin_survae.cpython-39.pyc
                plugin_survival_ctgan.cpython-39.pyc
                plugin_survival_gan.cpython-39.pyc
                plugin_survival_nflow.cpython-39.pyc
             _survival_pipeline.py
             plugin_survae.py
             plugin_survival_ctgan.py
             plugin_survival_gan.py
             plugin_survival_nflow.py
          time_series
              __init__.py
              __pycache__
                 plugin_fflows.cpython-39.pyc
                 plugin_timegan.cpython-39.pyc
                 plugin_timevae.cpython-39.pyc
              plugin_fflows.py
              plugin_timegan.py
              plugin_timevae.py
       utils
          __pycache__
             callbacks.cpython-39.pyc
             compression.cpython-39.pyc
             constants.cpython-39.pyc
             dataframe.cpython-39.pyc
             evaluation.cpython-39.pyc
             optimizer.cpython-39.pyc
             redis_wrapper.cpython-39.pyc
             reproducibility.cpython-39.pyc
             samplers.cpython-39.pyc
             serialization.cpython-39.pyc
          anonymization.py
          callbacks.py
          compression.py
          constants.py
          dataframe.py
          datasets
             __init__.py
             categorical
                __init__.py
                categorical_adult.py
                data
                    __init__ .py
             time_series
                 __init__.py
                 data
                    __init__.py
                 google_stocks.py
                 pbc.py
                 sine.py
          evaluation.py
          optimizer.py
          optuna_sample.py
          redis_wrapper.py
          reproducibility.py
          samplers.py
          serialization.py
       version.py
    synthcity.egg-info
        PKG-INFO
        SOURCES.txt
        dependency_links.txt
        not-zip-safe
        requires.txt
        top_level.txt
 tests
    benchmarks
       test_benchmarks.py
    conftest.py
    metrics
       test_api.py
       test_attacks.py
       test_detection.py
       test_performance.py
       test_plots.py
       test_privacy.py
       test_sanity.py
       test_statistical.py
    nb_eval.py
    plugins
       core
          models
             dag
                test_dag_sanity.py
             helpers.py
             survival_analysis
                test_surv_aft.py
                test_surv_coxph.py
                test_surv_deephit.py
                test_surv_metrics.py
                test_surv_xgb.py
             test_convnet.py
             test_gan.py
             test_image_gan.py
             test_mlp.py
             test_normalizing_flows.py
             test_tabular_encoder.py
             test_tabular_gan.py
             test_tabular_vae.py
             test_transformer.py
             test_ts_gan.py
             test_ts_model.py
             test_ts_tabular_gan.py
             test_ts_tabular_vae.py
             test_vae.py
             time_series_survival
                test_ts_surv_coxph.py
                test_ts_surv_dynamic_deephit.py
                test_ts_surv_xgb.py
             time_to_event
                 test_tte_aft.py
                 test_tte_coxph.py
                 test_tte_date.py
                 test_tte_deephit.py
                 test_tte_metrics.py
                 test_tte_survival_function_regression.py
                 test_tte_survival_time_series.py
                 test_tte_tenn.py
                 test_tte_xgb.py
          test_constraints.py
          test_dataloader.py
          test_distribution.py
          test_plugin.py
          test_schema.py
       domain_adaptation
          da_helpers.py
          test_radialgan.py
       generic
          generic_helpers.py
          test_arf.py
          test_bayesian_network.py
          test_ctgan.py
          test_ddpm.py
          test_dummy_sampler.py
          test_goggle.py
          test_great.py
          test_marginal_distributions.py
          test_nflow.py
          test_rtvae.py
          test_tvae.py
          test_uniform_sampler.py
       images
          img_helpers.py
          test_image_adsgan.py
          test_image_cgan.py
       privacy
          fhelpers.py
          test_adsgan.py
          test_aim.py
          test_decaf.py
          test_dpgan.py
          test_pategan.py
          test_privbayes.py
       survival_analysis
          surv_helpers.py
          test_generic_marginal_distributions.py
          test_survae.py
          test_survival_ctgan.py
          test_survival_gan.py
       test_plugin_add.py
       test_plugin_serialization.py
       time_series
           test_fflows.py
           test_timegan.py
           test_ts_marginal_distributions.py
           ts_helpers.py
    test_logger.py
    utils
        datasets
           time_series
               test_google_stocks.py
               test_pbc.py
               test_sine.py
        test_anonymization.py
        test_compression.py
        test_serialization.py
 tutorials
     plugins
        domain_adaptation
           plugin_radialgan.ipynb
        generic
           plugin_arf.ipynb
           plugin_bayesian_network.ipynb
           plugin_ctgan.ipynb
           plugin_ddpm.ipynb
           plugin_dummy_sampler.ipynb
           plugin_great.ipynb
           plugin_marginal_distributions.ipynb
           plugin_nflow.ipynb
           plugin_tvae.ipynb
           plugin_uniform_sampler.ipynb
        images
           plugin_image_adsgan.ipynb
           plugin_image_cgan.ipynb
        privacy
           plugin_adsgan.ipynb
           plugin_aim.ipynb
           plugin_decaf.ipynb
           plugin_dpgan.ipynb
           plugin_pategan.ipynb
           plugin_privbayes.ipynb
        time_series
            plugin_ctgan(generic).ipynb
            plugin_fourier_flows.ipynb
            plugin_timegan.ipynb
     tutorial0_basic_examples.ipynb
     tutorial10_sequential_synthesis.ipynb
     tutorial1_add_a_new_plugin.ipynb
     tutorial2_benchmarks.ipynb
     tutorial3_survival_analysis.ipynb
     tutorial4_time_series.ipynb
     tutorial5_differential_privacy.ipynb
     tutorial6_time_series_data_preparation.ipynb
     tutorial7_image_generation_using_mednist.ipynb
     tutorial8_hyperparameter_optimization.ipynb
     tutorial9_dealing_with_missing_data.ipynb
     workspace

81 directories, 508 files
(base) minkeychang@Minkeys-Laptop synthcity % 
