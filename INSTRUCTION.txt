INSTRUCTION

Re-write the "syn_seq.py" to finalize the approach.
Also, let me know if I need to modify Syn_SeqDataLoader class or syn_seq_encoder.py or plugin_syn_seq.py. Just let me know, don't generate the code for these files.

I want to add the code to re build the schema based on encoded dataset like the following.

# Suppose 'encoded_loader' is your Syn_SeqDataLoader after encode(),
# which has the new columns, correct dtypes, etc.

# 1) Build a schema for the new (encoded) dataset
encoded_schema = Schema(data=encoded_loader)

# 2) If you have user constraints, extend them *after* the new schemaâ€™s constraints
gen_constraints = encoded_schema.as_constraints()
if user_constraints is not None:
    gen_constraints = gen_constraints.extend(user_constraints)

# 3) Now build the final schema from these updated constraints
syn_schema = Schema.from_constraints(gen_constraints)


Thanks.


ISSUE

Issue persists even though we have modified syn_seq.py and plugin_syn_seq.py. Problem with running the syn_seq is that our 'categorization' approach is conflict with original 'schema' framework. Since some numeric columns that has special values, manually assigned or imbalanced distribution are 'categorized' but current generating structure throw out error because of data types.
To be more precise, while loading and fitting the model such columns are assigned as 'category' and their data types are string. But when we try to build the schema using base structure, it recognize the variable as numeric.
It is true that they should be 'numeric' at the end but during the fitting and generation they should be left as categorical. This flow is conflict with current synthcity structure.
This is why despite we are using encoder and decoder to even map the data types it still throws error.
To resolve this, we are thinking about generating them as categorical(object) and later manually assigning the data types based on original data's type. This may be saved in loader, encoder, or somewhere else.
We cannot patch constraints.py, distribution.py, schema.py, nor any core modules because they are for general purposes.

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[8], line 7
      1 constraints = {
      2   "target":[
      3     ("bmi", ">", 0.15),
      4     ("target", ">", 0)
      5   ]
      6 }
----> 7 synthetic_data = syn_model.generate(nrows = len(ods), constraints = constraints).dataframe()

File ~/anaconda3/envs/syn_seq/lib/python3.9/site-packages/pydantic/deprecated/decorator.py:55, in validate_arguments.<locals>.validate.<locals>.wrapper_function(*args, **kwargs)
     53 @wraps(_func)
     54 def wrapper_function(*args: Any, **kwargs: Any) -> Any:
---> 55     return vd.call(*args, **kwargs)

File ~/anaconda3/envs/syn_seq/lib/python3.9/site-packages/pydantic/deprecated/decorator.py:150, in ValidatedFunction.call(self, *args, **kwargs)
    148 def call(self, *args: Any, **kwargs: Any) -> Any:
    149     m = self.init_model_instance(*args, **kwargs)
--> 150     return self.execute(m)

File ~/anaconda3/envs/syn_seq/lib/python3.9/site-packages/pydantic/deprecated/decorator.py:226, in ValidatedFunction.execute(self, m)
    224     return self.raw_function(*args_, **kwargs, **var_kwargs)
    225 else:
--> 226     return self.raw_function(**d, **var_kwargs)

File ~/synthcity/src/synthcity/plugins/core/plugin.py:347, in Plugin.generate(self, count, constraints, random_state, **kwargs)
    344 if constraints is not None:
    345     gen_constraints = gen_constraints.extend(constraints)
--> 347 syn_schema = Schema.from_constraints(gen_constraints)
    349 X_syn = self._generate(count=count, syn_schema=syn_schema, **kwargs)
    351 if X_syn.is_tabular():

File ~/synthcity/src/synthcity/plugins/core/schema.py:213, in Schema.from_constraints(cls, constraints)
    211 if dtype == "float":
    212     if params["low"] is None or params["high"] is None:
--> 213         raise ValueError(
    214             f"Cannot create FloatDistribution for '{feature}' without 'low' and 'high' values."
    215         )
    216     domain[feature] = FloatDistribution(
    217         name=params["name"],
    218         random_state=params["random_state"],
    219         low=params["low"],
    220         high=params["high"],
    221     )
    222 elif dtype == "int":

ValueError: Cannot create FloatDistribution for 'age' without 'low' and 'high' values.

PROJECT OUTLINE

Explanation: From the following example, we have used sample dataset. We assume 'target' column is highly imbalanced which there are majority of 0 values.
That's why loader automatically assigned 0 for its special value. We also designated special value for 'bp' column.
For 'age' column, syn_seq automatically assign it as numeric so user assigned it as category. There are only two column types, numeric and category.
In following example, user changed syn_order and method of some variable. Use also implemented constraint on variable which if bmi is greater than certain value, target must be greater than 0.
Always remember you are building package, not the one time running code. This is just an example. It needs to be highly generalizable.
Package also changed variable selection matrix accordingly and printed it out it fit process.
Also notice that '_cat' columns were created for numeric variables with special values. They were used in fitting and generating the data.
To be more specific, when plugin takes input loader and run fit, loader.encode is run and return new loader. This is processed loader with new dataframe which contains _cat columns, new order, new methods and all the attributes updated.
Given new loader, syn_seq now starts fitting the model and save them for later generation. We believed that new loader contains all the necessary information to decode the synthesized dataframe so we decided not to return encoder.
To elaborate, after synthesizing 'age', if we use default methods, cart fit 'sex'~'age' and save the model. After saving the model, predict 'sex' based on synthesized(sampled) 'age' and generate the next column.
Once this process is done, 'bmi' is generated with 'bmi'~('sex', 'age'). Now 'bmi' is predicted with two synthesized variables. This is why it is called sequential synthesis. In case of _cat columns, it is fitted and generated as the others. However, method is fixed to 'cart'.
Such synthesis order and variable selection is controled with syn_order and variable_selection. Issue is that when _cat column is created, syn_order and variable_selection needs modification.
For example, if 'bp_cat' and 'target_cat' are created, we need to have temporary syn_order ['sex', 'bmi', 'age', 'bp_cat', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'target_cat', 'target'] which is not directly shown to the user but we use this order.
If in variable selection we don't use 'bp', that means we also don't use 'bp_cat' to predict such variable.
In case of original 'bp' column, fit is applied only with numerics. 'bp_cat' is fit back later into 'bp' where it is coded nan temporary. In generation step, for example at cart, it is re-sampled at leaf node if constraint is not met.
If it is equality constraint, it is substituted. If it is other constraints, prediction is done again. If prediction never finds value appropriate for constraint, it means there's something wrong with data so through an error.

EXAMPLE OUTPUT

from synthcity.plugins import Plugins
ods = pd.read_csv("ods.csv", header = True)

loader = Syn_SeqDataLoader(ods, col_type = {'age':'category'}, columns_special_values{'bp':[-0.040099, -0.005670]})
[INFO] Most of the time, it is recommened to have category variables before synthesizing numeric variables
[INFO] Syn_SeqDataLoader init complete:
  - syn_order: ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'target']
  - special_value: {'bp': [-0.040099, -0.00567]}
  - col_type: {'age': 'category'}
  - data shape: (442, 11)
[DEBUG] After encoder.fit(), detected info:
  - encoder.col_map =>
       age : {'original_dtype': 'float64', 'converted_type': 'category', 'method': 'cart'}
       sex : {'original_dtype': 'float64', 'converted_type': 'category', 'method': 'cart'}
       bmi : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       bp : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s1 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s2 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s3 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s4 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s5 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s6 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       target : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
  - variable_selection_:
         age  sex  bmi  bp  s1  s2  s3  s4  s5  s6  target
age       0    0    0   0   0   0   0   0   0   0       0
sex       1    0    0   0   0   0   0   0   0   0       0
bmi       1    1    0   0   0   0   0   0   0   0       0
bp        1    1    1   0   0   0   0   0   0   0       0
s1        1    1    1   1   0   0   0   0   0   0       0
s2        1    1    1   1   1   0   0   0   0   0       0
s3        1    1    1   1   1   1   0   0   0   0       0
s4        1    1    1   1   1   1   1   0   0   0       0
s5        1    1    1   1   1   1   1   1   0   0       0
s6        1    1    1   1   1   1   1   1   1   0       0
target    1    1    1   1   1   1   1   1   1   1       0
  - date_mins: {}
----------------------------------------------------------------

[INFO] Loader created. loader.shape = (442, 11)
loader columns = ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'target']


user_custom = {
  'syn_order' : ['sex', 'bmi', 'age', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'target'],
  'method' : {'bp':'polyreg'},
  'variable_selection' : {
    "s4": ['sex', 'bmi', 'age', 'bp', 's1', 's2']
    "target": ['sex', 'bmi', 'age', 'bp', 's1', 's2', 's3']
  }
}

syn_model = Plugins().get("syn_seq")
syn_model.fit(nrows = len(ods),user_custom)

[INFO] final synthesis:
  - syn_order: ['sex', 'bmi', 'age', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'target']
  - method: {'age':'swr', 'sex':'cart', 'bmi':'cart', 'bp':'polyreg', 's1':'cart', 's2':'cart', 's3':'cart', 's4':'cart', 's5':'cart', 's6':'cart', 'target':'cart'}
  - variable_selection_:
        sex  bmi  age  bp  s1  s2  s3  s4  s5  s6  target
age       0    0    0   0   0   0   0   0   0   0       0
sex       1    0    0   0   0   0   0   0   0   0       0
bmi       1    1    0   0   0   0   0   0   0   0       0
bp        1    1    1   0   0   0   0   0   0   0       0
s1        1    1    1   1   0   0   0   0   0   0       0
s2        1    1    1   1   1   0   0   0   0   0       0
s3        1    1    1   1   1   1   0   0   0   0       0
s4        1    1    1   1   1   1   0   0   0   0       0
s5        1    1    1   1   1   1   1   1   0   0       0
s6        1    1    1   1   1   1   1   1   1   0       0
target    1    1    1   1   1   1   1   0   0   0       0

[INFO] model fitting
Fitting 'sex' ... Done!
Fitting 'bmi' ... Done!
Fitting 'age' ... Done!
Fitting 'bp_cat' ... Done!
Fitting 'bp' ... Done!
Fitting 's1' ... Done!
Fitting 's2' ... Done!
Fitting 's3' ... Done!
Fitting 's4' ... Done!
Fitting 's5' ... Done!
Fitting 's6' ... Done!
Fitting 'target_cat' ... Done!
Fitting 'target' ... Done!

constraints = {
  "target":[
    ("bmi", ">", 0.15),
    ("target", ">", 0)
  ]
}
synthetic_data = syn_model.generate(nrows = len(ods), constraints = constraints).dataframe()

Generating 'sex' ... Done!
Generating 'bmi' ... Done!
Generating 'age' ... Done!
Generating 'bp_cat' ... Done!
Generating 'bp' ... Done!
Generating 's1' ... Done!
Generating 's2' ... Done!
Generating 's3' ... Done!
Generating 's4' ... Done!
Generating 's5' ... Done!
Generating 's6' ... Done!
Generating 'target_cat' ... Done!
Generating 'target' ... Done!


INSTRUCTION

Current issue running the syn_seq is that our 'categorization' approach is conflict with original 'schema' framework. Since some numeric columns that has special values, manually assigned or imbalanced distribution are 'categorized' but current generating structure throw out error because of data types.
To be more precise, while loading and fitting the model such columns are assigned as 'category' and their data types are string. But when we try to build the schema using base structure, it recognize the variable as numeric.
It is true that they should be 'numeric' at the end but during the fitting and generation they should be left as categorical. This flow is conflict with current synthcity structure.
This is why despite we are using encoder and decoder to even map the data types it still throws error.
To resolve this, we are thinking about generating them as categorical(object) and later manually assigning the data types based on mapping we have been passing on since dataloader.
We cannot patch constraints.py, distribution.py, schema.py, nor any core modules because they are for general purposes.
With this in mind, please re write the "syn_seq.py" to finalize the approach. Thanks.

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[8], line 7
      1 constraints = {
      2   "target":[
      3     ("bmi", ">", 0.15),
      4     ("target", ">", 0)
      5   ]
      6 }
----> 7 synthetic_data = syn_model.generate(nrows = len(ods), constraints = constraints).dataframe()

File ~/anaconda3/envs/syn_seq/lib/python3.9/site-packages/pydantic/deprecated/decorator.py:55, in validate_arguments.<locals>.validate.<locals>.wrapper_function(*args, **kwargs)
     53 @wraps(_func)
     54 def wrapper_function(*args: Any, **kwargs: Any) -> Any:
---> 55     return vd.call(*args, **kwargs)

File ~/anaconda3/envs/syn_seq/lib/python3.9/site-packages/pydantic/deprecated/decorator.py:150, in ValidatedFunction.call(self, *args, **kwargs)
    148 def call(self, *args: Any, **kwargs: Any) -> Any:
    149     m = self.init_model_instance(*args, **kwargs)
--> 150     return self.execute(m)

File ~/anaconda3/envs/syn_seq/lib/python3.9/site-packages/pydantic/deprecated/decorator.py:226, in ValidatedFunction.execute(self, m)
    224     return self.raw_function(*args_, **kwargs, **var_kwargs)
    225 else:
--> 226     return self.raw_function(**d, **var_kwargs)

File ~/synthcity/src/synthcity/plugins/core/plugin.py:347, in Plugin.generate(self, count, constraints, random_state, **kwargs)
    344 if constraints is not None:
    345     gen_constraints = gen_constraints.extend(constraints)
--> 347 syn_schema = Schema.from_constraints(gen_constraints)
    349 X_syn = self._generate(count=count, syn_schema=syn_schema, **kwargs)
    351 if X_syn.is_tabular():

File ~/synthcity/src/synthcity/plugins/core/schema.py:213, in Schema.from_constraints(cls, constraints)
    211 if dtype == "float":
    212     if params["low"] is None or params["high"] is None:
--> 213         raise ValueError(
    214             f"Cannot create FloatDistribution for '{feature}' without 'low' and 'high' values."
    215         )
    216     domain[feature] = FloatDistribution(
    217         name=params["name"],
    218         random_state=params["random_state"],
    219         low=params["low"],
    220         high=params["high"],
    221     )
    222 elif dtype == "int":

ValueError: Cannot create FloatDistribution for 'age' without 'low' and 'high' values.

PROJECT OUTLINE

Explanation: From the following example, we have used sample dataset. We assume 'target' column is highly imbalanced which there are majority of 0 values.
That's why loader automatically assigned 0 for its special value. We also designated special value for 'bp' column.
For 'age' column, syn_seq automatically assign it as numeric so user assigned it as category. There are only two column types, numeric and category.
In following example, user changed syn_order and method of some variable. Use also implemented constraint on variable which if bmi is greater than certain value, target must be greater than 0.
Always remember you are building package, not the one time running code. This is just an example. It needs to be highly generalizable.
Package also changed variable selection matrix accordingly and printed it out it fit process.
Also notice that '_cat' columns were created for numeric variables with special values. They were used in fitting and generating the data.
To be more specific, when plugin takes input loader and run fit, loader.encode is run and return new loader. This is processed loader with new dataframe which contains _cat columns, new order, new methods and all the attributes updated.
Given new loader, syn_seq now starts fitting the model and save them for later generation. We believed that new loader contains all the necessary information to decode the synthesized dataframe so we decided not to return encoder.
To elaborate, after synthesizing 'age', if we use default methods, cart fit 'sex'~'age' and save the model. After saving the model, predict 'sex' based on synthesized(sampled) 'age' and generate the next column.
Once this process is done, 'bmi' is generated with 'bmi'~('sex', 'age'). Now 'bmi' is predicted with two synthesized variables. This is why it is called sequential synthesis. In case of _cat columns, it is fitted and generated as the others. However, method is fixed to 'cart'.
Such synthesis order and variable selection is controled with syn_order and variable_selection. Issue is that when _cat column is created, syn_order and variable_selection needs modification.
For example, if 'bp_cat' and 'target_cat' are created, we need to have temporary syn_order ['sex', 'bmi', 'age', 'bp_cat', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'target_cat', 'target'] which is not directly shown to the user but we use this order.
If in variable selection we don't use 'bp', that means we also don't use 'bp_cat' to predict such variable.
In case of original 'bp' column, fit is applied only with numerics. 'bp_cat' is fit back later into 'bp' where it is coded nan temporary. In generation step, for example at cart, it is re-sampled at leaf node if constraint is not met.
If it is equality constraint, it is substituted. If it is other constraints, prediction is done again. If prediction never finds value appropriate for constraint, it means there's something wrong with data so through an error.

EXAMPLE OUTPUT

from synthcity.plugins import Plugins
ods = pd.read_csv("ods.csv", header = True)

loader = Syn_SeqDataLoader(ods, col_type = {'age':'category'}, columns_special_values{'bp':[-0.040099, -0.005670]})
[INFO] Most of the time, it is recommened to have category variables before synthesizing numeric variables
[INFO] Syn_SeqDataLoader init complete:
  - syn_order: ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'target']
  - special_value: {'bp': [-0.040099, -0.00567]}
  - col_type: {'age': 'category'}
  - data shape: (442, 11)
[DEBUG] After encoder.fit(), detected info:
  - encoder.col_map =>
       age : {'original_dtype': 'float64', 'converted_type': 'category', 'method': 'cart'}
       sex : {'original_dtype': 'float64', 'converted_type': 'category', 'method': 'cart'}
       bmi : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       bp : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s1 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s2 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s3 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s4 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s5 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       s6 : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
       target : {'original_dtype': 'float64', 'converted_type': 'numeric', 'method': 'cart'}
  - variable_selection_:
         age  sex  bmi  bp  s1  s2  s3  s4  s5  s6  target
age       0    0    0   0   0   0   0   0   0   0       0
sex       1    0    0   0   0   0   0   0   0   0       0
bmi       1    1    0   0   0   0   0   0   0   0       0
bp        1    1    1   0   0   0   0   0   0   0       0
s1        1    1    1   1   0   0   0   0   0   0       0
s2        1    1    1   1   1   0   0   0   0   0       0
s3        1    1    1   1   1   1   0   0   0   0       0
s4        1    1    1   1   1   1   1   0   0   0       0
s5        1    1    1   1   1   1   1   1   0   0       0
s6        1    1    1   1   1   1   1   1   1   0       0
target    1    1    1   1   1   1   1   1   1   1       0
  - date_mins: {}
----------------------------------------------------------------

[INFO] Loader created. loader.shape = (442, 11)
loader columns = ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'target']


user_custom = {
  'syn_order' : ['sex', 'bmi', 'age', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'target'],
  'method' : {'bp':'polyreg'},
  'variable_selection' : {
    "s4": ['sex', 'bmi', 'age', 'bp', 's1', 's2']
    "target": ['sex', 'bmi', 'age', 'bp', 's1', 's2', 's3']
  }
}

syn_model = Plugins().get("syn_seq")
syn_model.fit(nrows = len(ods),user_custom)

[INFO] final synthesis:
  - syn_order: ['sex', 'bmi', 'age', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'target']
  - method: {'age':'swr', 'sex':'cart', 'bmi':'cart', 'bp':'polyreg', 's1':'cart', 's2':'cart', 's3':'cart', 's4':'cart', 's5':'cart', 's6':'cart', 'target':'cart'}
  - variable_selection_:
        sex  bmi  age  bp  s1  s2  s3  s4  s5  s6  target
age       0    0    0   0   0   0   0   0   0   0       0
sex       1    0    0   0   0   0   0   0   0   0       0
bmi       1    1    0   0   0   0   0   0   0   0       0
bp        1    1    1   0   0   0   0   0   0   0       0
s1        1    1    1   1   0   0   0   0   0   0       0
s2        1    1    1   1   1   0   0   0   0   0       0
s3        1    1    1   1   1   1   0   0   0   0       0
s4        1    1    1   1   1   1   0   0   0   0       0
s5        1    1    1   1   1   1   1   1   0   0       0
s6        1    1    1   1   1   1   1   1   1   0       0
target    1    1    1   1   1   1   1   0   0   0       0

[INFO] model fitting
Fitting 'sex' ... Done!
Fitting 'bmi' ... Done!
Fitting 'age' ... Done!
Fitting 'bp_cat' ... Done!
Fitting 'bp' ... Done!
Fitting 's1' ... Done!
Fitting 's2' ... Done!
Fitting 's3' ... Done!
Fitting 's4' ... Done!
Fitting 's5' ... Done!
Fitting 's6' ... Done!
Fitting 'target_cat' ... Done!
Fitting 'target' ... Done!

constraints = {
  "target":[
    ("bmi", ">", 0.15),
    ("target", ">", 0)
  ]
}
synthetic_data = syn_model.generate(nrows = len(ods), constraints = constraints).dataframe()

Generating 'sex' ... Done!
Generating 'bmi' ... Done!
Generating 'age' ... Done!
Generating 'bp_cat' ... Done!
Generating 'bp' ... Done!
Generating 's1' ... Done!
Generating 's2' ... Done!
Generating 's3' ... Done!
Generating 's4' ... Done!
Generating 's5' ... Done!
Generating 's6' ... Done!
Generating 'target_cat' ... Done!
Generating 'target' ... Done!

src/synthcity/plugins/core/plugin.py
# stdlib
import importlib.util
import platform
import sys
from abc import ABCMeta, abstractmethod
from importlib.abc import Loader
from pathlib import Path
from typing import Any, Callable, Dict, Generator, List, Optional, Type, Union

# third party
import pandas as pd
from pydantic import ConfigDict, validate_arguments

# synthcity absolute
import synthcity.logger as log
from synthcity.metrics.plots import plot_marginal_comparison, plot_tsne
from synthcity.plugins.core.constraints import Constraints
from synthcity.plugins.core.dataloader import (
    DataLoader,
    GenericDataLoader,
    TimeSeriesDataLoader,
    TimeSeriesSurvivalDataLoader,
    create_from_info,
)
from synthcity.plugins.core.distribution import (
    CategoricalDistribution,
    Distribution,
    FloatDistribution,
    IntegerDistribution,
)
from synthcity.plugins.core.schema import Schema
from synthcity.plugins.core.serializable import Serializable
from synthcity.utils.constants import DEVICE
from synthcity.utils.reproducibility import enable_reproducible_results
from synthcity.utils.serialization import load_from_file, save_to_file

PLUGIN_NAME_NOT_SET: str = "plugin_name_not_set"
PLUGIN_TYPE_NOT_SET: str = "plugin_type_not_set"


class Plugin(Serializable, metaclass=ABCMeta):
    """
    .. inheritance-diagram:: synthcity.plugins.core.plugin.Plugin
        :parts: 1

    Base class for all plugins.

    Each derived class must implement the following methods:
        type() - a static method that returns the type of the plugin. e.g., debug, generative, bayesian, etc.
        name() - a static method that returns the name of the plugin. e.g., ctgan, random_noise, etc.
        hyperparameter_space() - a static method that returns the hyperparameters that can be tuned during AutoML.
        _fit() - internal method, called by `fit` on each training set.
        _generate() - internal method, called by `generate`.

    If any method implementation is missing, the class constructor will fail.

    Args:
        strict: bool. Default = True
            If True, is raises an exception if the generated data is not following the requested constraints. If False, it returns only the rows that match the constraints.
        workspace: Path
            Path for caching intermediary results
        compress_dataset: bool. Default = False
            Drop redundant features before training the generator.
        device:
            PyTorch device: cpu or cuda.
        random_state: int
            Random seed
        sampling_patience: int.
            Max inference iterations to wait for the generated data to match the training schema.
        sampling_strategy: str
            Internal parameter for schema. marginal or uniform.
    """

    model_config = ConfigDict(arbitrary_types_allowed=True, validate_assignment=True)

    def __init__(
        self,
        sampling_patience: int = 500,
        strict: bool = True,
        device: Any = DEVICE,
        random_state: int = 0,
        workspace: Path = Path("workspace"),
        compress_dataset: bool = False,
        sampling_strategy: str = "marginal",  # uniform, marginal
    ) -> None:
        if self.name() == PLUGIN_NAME_NOT_SET:
            raise ValueError(
                f"Plugin {self.__class__.__name__} `name` was not set, use Plugins().add({self.__class__.__name__}, {self.__class__})"
            )
        if self.type() == PLUGIN_TYPE_NOT_SET:
            raise ValueError(
                f"Plugin {self.__class__.__name__} `type` was not set, use Plugins().add({self.__class__.__name__}, {self.__class__})"
            )

        super().__init__()

        enable_reproducible_results(random_state)

        self._schema: Optional[Schema] = None
        self._training_schema: Optional[Schema] = None
        self._data_encoders: Optional[Dict] = None

        self.sampling_strategy = sampling_strategy
        self.sampling_patience = sampling_patience
        self.strict = strict
        self.device = device
        self.random_state = random_state
        self.compress_dataset = compress_dataset

        workspace.mkdir(parents=True, exist_ok=True)
        self.workspace = workspace

        self.fitted = False
        self.expecting_conditional = False

    @staticmethod
    @abstractmethod
    def hyperparameter_space(**kwargs: Any) -> List[Distribution]:
        """Returns the hyperparameter space for the derived plugin."""
        ...

    @classmethod
    def sample_hyperparameters(cls, *args: Any, **kwargs: Any) -> Dict[str, Any]:
        """Sample value from the hyperparameter space for the current plugin."""
        param_space = cls.hyperparameter_space(*args, **kwargs)

        results = {}

        for hp in param_space:
            results[hp.name] = hp.sample()[0]

        return results

    @classmethod
    def sample_hyperparameters_optuna(
        cls, trial: Any, *args: Any, **kwargs: Any
    ) -> Dict[str, Any]:
        param_space = cls.hyperparameter_space(*args, **kwargs)

        results = {}

        for hp in param_space:
            if isinstance(hp, IntegerDistribution):
                results[hp.name] = trial.suggest_int(hp.name, hp.low, hp.high, hp.step)
            elif isinstance(hp, FloatDistribution):
                results[hp.name] = trial.suggest_float(hp.name, hp.low, hp.high)
            elif isinstance(hp, CategoricalDistribution):
                results[hp.name] = trial.suggest_categorical(hp.name, hp.choices)
            else:
                raise RuntimeError(f"unknown distribution type {hp}")

        return results

    @staticmethod
    @abstractmethod
    def name() -> str:
        """The name of the plugin."""
        return PLUGIN_NAME_NOT_SET

    @staticmethod
    @abstractmethod
    def type() -> str:
        """The type of the plugin."""
        return PLUGIN_TYPE_NOT_SET

    @classmethod
    def fqdn(cls) -> str:
        """The Fully-Qualified name of the plugin."""
        return cls.type() + "." + cls.name()

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def fit(self, X: Union[DataLoader, pd.DataFrame], *args: Any, **kwargs: Any) -> Any:
        """Training method the synthetic data plugin.

        Args:
            X: DataLoader.
                The reference dataset.
            cond: Optional, Union[pd.DataFrame, pd.Series, np.ndarray]
                Optional Training Conditional.
                The training conditional can be used to control to output of some models, like GANs or VAEs. The content can be anything, as long as it maps to the training dataset X.
                Usage example:
                    >>> from sklearn.datasets import load_iris
                    >>> from synthcity.plugins.core.dataloader import GenericDataLoader
                    >>> from synthcity.plugins.core.constraints import Constraints
                    >>>
                    >>> # Load in `test_plugin` the generative model of choice
                    >>> # ....
                    >>>
                    >>> X, y = load_iris(as_frame=True, return_X_y=True)
                    >>> X["target"] = y
                    >>>
                    >>> X = GenericDataLoader(X)
                    >>> test_plugin.fit(X, cond=y)
                    >>>
                    >>> count = 10
                    >>> X_gen = test_plugin.generate(count, cond=np.ones(count))
                    >>>
                    >>> # The Conditional only optimizes the output generation
                    >>> # for GANs and VAEs, but does NOT guarantee the samples
                    >>> # are only from that condition.
                    >>> # If you want to guarantee that output contains only
                    >>> # "target" == 1 samples, use Constraints.
                    >>>
                    >>> constraints = Constraints(
                    >>>     rules=[
                    >>>         ("target", "==", 1),
                    >>>     ]
                    >>> )
                    >>> X_gen = test_plugin.generate(count,
                    >>>         cond=np.ones(count),
                    >>>         constraints=constraints
                    >>>        )
                    >>> assert (X_gen["target"] == 1).all()

        Returns:
            self
        """
        if isinstance(X, (pd.DataFrame)):
            X = GenericDataLoader(X)

        if "cond" in kwargs and kwargs["cond"] is not None:
            self.expecting_conditional = True

        enable_reproducible_results(self.random_state)

        self.data_info = X.info()

        self._schema = Schema(
            data=X,
            sampling_strategy=self.sampling_strategy,
            random_state=self.random_state,
        )

        if X.is_tabular():
            X, self._data_encoders = X.encode()
            if self.compress_dataset:
                X_hash = X.hash()
                bkp_file = (
                    self.workspace
                    / f"compressed_df_{X_hash}_{platform.python_version()}.bkp"
                )
                if not bkp_file.exists():
                    X_compressed_context = X.compress()
                    save_to_file(bkp_file, X_compressed_context)

                X, self.compress_context = load_from_file(bkp_file)

        self._training_schema = Schema(
            data=X,
            sampling_strategy=self.sampling_strategy,
            random_state=self.random_state,
        )

        output = self._fit(X, *args, **kwargs)
        self.fitted = True

        return output

    @abstractmethod
    def _fit(self, X: DataLoader, *args: Any, **kwargs: Any) -> "Plugin":
        """Internal training method the synthetic data plugin.

        Args:
            X: DataLoader.
                The reference dataset.
            cond: Optional, Union[pd.DataFrame, pd.Series, np.ndarray]
                Training Conditional
        Returns:
            self
        """
        ...

    @validate_arguments
    def generate(
        self,
        count: Optional[int] = None,
        constraints: Optional[Constraints] = None,
        random_state: Optional[int] = None,
        **kwargs: Any,
    ) -> DataLoader:
        """Synthetic data generation method.

        Args:
            count: optional int.
                The number of samples to generate. If None, it generated len(reference_dataset) samples.
            cond: Optional, Union[pd.DataFrame, pd.Series, np.ndarray].
                Optional Generation Conditional. The conditional can be used only if the model was trained using a conditional too.
                If provided, it must have `count` length.
                Not all models support conditionals. The conditionals can be used in VAEs or GANs to speed-up the generation under some constraints. For model agnostic solutions, check out the `constraints` parameter.
            constraints: optional Constraints.
                Optional constraints to apply on the generated data. If none, the reference schema constraints are applied. The constraints are model agnostic, and will filter the output of the generative model.
                The constraints are a list of rules. Each rule is a tuple of the form (<feature>, <operation>, <value>).

                Valid Operations:
                    - "<", "lt" : less than <value>
                    - "<=", "le": less or equal with <value>
                    - ">", "gt" : greater than <value>
                    - ">=", "ge": greater or equal with <value>
                    - "==", "eq": equal with <value>
                    - "in": valid for categorical features, and <value> must be array. for example, ("target", "in", [0, 1])
                    - "dtype": <value> can be a data type. For example, ("target", "dtype", "int")

                Usage example:
                    >>> from synthcity.plugins.core.constraints import Constraints
                    >>> constraints = Constraints(
                    >>>   rules=[
                    >>>             ("InterestingFeature", "==", 0),
                    >>>         ]
                    >>>     )
                    >>>
                    >>> syn_data = syn_model.generate(
                            count=count,
                            constraints=constraints
                        ).dataframe()
                    >>>
                    >>> assert (syn_data["InterestingFeature"] == 0).all()

            random_state: optional int.
                Optional random seed to use.

        Returns:
            <count> synthetic samples
        """
        if not self.fitted:
            raise RuntimeError("Fit the generator first")

        if self._schema is None:
            raise RuntimeError("Fit the model first")

        if random_state is not None:
            enable_reproducible_results(random_state)

        has_gen_cond = "cond" in kwargs and kwargs["cond"] is not None
        if has_gen_cond and not self.expecting_conditional:
            raise RuntimeError(
                "Conditional mismatch. Got inference conditional, without any training conditional"
            )

        if count is None:
            count = self.data_info["len"]

        # We use the training schema for the generation
        gen_constraints = self.training_schema().as_constraints()
        if constraints is not None:
            gen_constraints = gen_constraints.extend(constraints)

        syn_schema = Schema.from_constraints(gen_constraints)

        X_syn = self._generate(count=count, syn_schema=syn_schema, **kwargs)

        if X_syn.is_tabular():
            if self.compress_dataset:
                X_syn = X_syn.decompress(self.compress_context)
            if self._data_encoders is not None:
                X_syn = X_syn.decode(self._data_encoders)

        # The dataset is decompressed here, we can use the public schema
        gen_constraints = self.schema().as_constraints()
        if constraints is not None:
            gen_constraints = gen_constraints.extend(constraints)

        if not X_syn.satisfies(gen_constraints) and self.strict:
            raise RuntimeError(
                f"Plugin {self.name()} failed to meet the synthetic constraints."
            )

        if self.strict:
            X_syn = X_syn.match(gen_constraints)

        return X_syn

    @abstractmethod
    def _generate(
        self,
        count: int,
        syn_schema: Schema,
        **kwargs: Any,
    ) -> DataLoader:
        """Internal synthetic data generation method.

        Args:
            count: optional int.
                The number of samples to generate. If None, it generated len(reference_dataset) samples.
            syn_schema:
                The schema/constraints that need to be satisfied by the synthetic data.
            cond: Optional, Union[pd.DataFrame, pd.Series, np.ndarray]
                Generation Conditional

        Returns:
            <count> synthetic samples
        """
        ...

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def _safe_generate(
        self, gen_cbk: Callable, count: int, syn_schema: Schema, **kwargs: Any
    ) -> DataLoader:
        constraints = syn_schema.as_constraints()

        data_synth = pd.DataFrame([], columns=self.training_schema().features())
        for it in range(self.sampling_patience):
            # sample
            iter_samples = gen_cbk(count, **kwargs)
            iter_samples_df = pd.DataFrame(
                iter_samples, columns=self.training_schema().features()
            )

            # Handle protected columns
            for col in syn_schema.protected_cols:
                if col not in iter_samples_df.columns:
                    # Sample the protected column using its distribution
                    iter_samples_df[col] = syn_schema.domain[col].sample(count)

            # validate schema
            iter_samples_df = self.training_schema().adapt_dtypes(iter_samples_df)

            if self.strict:
                iter_samples_df = constraints.match(iter_samples_df)
                iter_samples_df = iter_samples_df.drop_duplicates()

            data_synth = pd.concat([data_synth, iter_samples_df], ignore_index=True)

            if len(data_synth) >= count:
                break

        data_synth = self.training_schema().adapt_dtypes(data_synth).head(count)

        return create_from_info(data_synth, self.data_info)

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def _safe_generate_time_series(
        self, gen_cbk: Callable, count: int, syn_schema: Schema, **kwargs: Any
    ) -> DataLoader:
        if self.data_info["data_type"] not in ["time_series", "time_series_survival"]:
            raise ValueError(
                f"Invalid data type for time series = {self.data_info['data_type']}"
            )
        constraints = syn_schema.as_constraints()

        data_synth = pd.DataFrame([], columns=self.training_schema().features())
        data_info = self.data_info
        offset = 0
        seq_offset = 0
        for it in range(self.sampling_patience):
            # sample
            if self.data_info["data_type"] == "time_series":
                static, temporal, observation_times, outcome = gen_cbk(
                    count - offset, **kwargs
                )
                loader = TimeSeriesDataLoader(
                    temporal_data=temporal,
                    observation_times=observation_times,
                    static_data=static,
                    outcome=outcome,
                    seq_offset=seq_offset,
                )
            elif self.data_info["data_type"] == "time_series_survival":
                static, temporal, observation_times, T, E = gen_cbk(
                    count - offset, **kwargs
                )
                loader = TimeSeriesSurvivalDataLoader(
                    temporal_data=temporal,
                    observation_times=observation_times,
                    static_data=static,
                    T=T,
                    E=E,
                    seq_offset=seq_offset,
                )

            # validate schema
            iter_samples_df = loader.dataframe()
            id_col = loader.info()["seq_id_feature"]

            iter_samples_df = self.training_schema().adapt_dtypes(iter_samples_df)

            if self.strict:
                iter_samples_df = constraints.match(iter_samples_df)

            if len(iter_samples_df) == 0:
                continue

            data_synth = pd.concat([data_synth, iter_samples_df], ignore_index=True)
            offset = len(data_synth[id_col].unique())
            seq_offset = max(data_synth[id_col].unique()) + 1

            if offset >= count:
                break

        data_synth = self.training_schema().adapt_dtypes(data_synth)
        return create_from_info(data_synth, data_info)

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def _safe_generate_images(
        self, gen_cbk: Callable, count: int, syn_schema: Schema, **kwargs: Any
    ) -> DataLoader:
        data_synth = gen_cbk(count, **kwargs)

        return create_from_info(data_synth, self.data_info)

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def schema_includes(self, other: Union[DataLoader, pd.DataFrame]) -> bool:
        """Helper method to test if the reference schema includes a Dataset

        Args:
            other: DataLoader.
                The dataset to test

        Returns:
            bool, if the schema includes the dataset or not.

        """
        other_schema = Schema(data=other)
        return self.schema().includes(other_schema)

    def schema(self) -> Schema:
        """The reference schema"""
        if self._schema is None:
            raise RuntimeError("Fit the model first")

        return self._schema

    def training_schema(self) -> Schema:
        """The internal schema"""
        if self._training_schema is None:
            raise RuntimeError("Fit the model first")

        return self._training_schema

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def plot(
        self,
        plt: Any,
        X: DataLoader,
        count: Optional[int] = None,
        plots: list = ["marginal", "associations", "tsne"],
        **kwargs: Any,
    ) -> Any:
        """Plot the real-synthetic distributions.

        Args:
            plt: output
            X: DataLoader.
                The reference dataset.

        Returns:
            self
        """
        X_syn = self.generate(count=count, **kwargs)

        if "marginal" in plots:
            plot_marginal_comparison(plt, X, X_syn)
        if "tsne" in plots:
            plot_tsne(plt, X, X_syn)


PLUGIN_CATEGORY_REGISTRY: Dict[str, List[str]] = dict()
PLUGIN_REGISTRY: Dict[str, Type[Plugin]] = dict()


class PluginLoader:
    """Plugin loading utility class.
    Used to load the plugins from the current folder.
    """

    @validate_arguments
    def __init__(self, plugins: list, expected_type: Type, categories: list) -> None:
        global PLUGIN_CATEGORY_REGISTRY
        PLUGIN_CATEGORY_REGISTRY = {cat: [] for cat in categories}
        self._refresh()
        self._available_plugins = {}
        for plugin in plugins:
            stem = Path(plugin).stem.split("plugin_")[-1]
            cls = self._load_single_plugin_impl(plugin)
            if cls is None:
                continue
            self._available_plugins[stem] = plugin
        self._expected_type = expected_type

    def _refresh(self) -> None:
        """Refresh the list of available plugins"""
        self._plugins: Dict[str, Type[Plugin]] = PLUGIN_REGISTRY
        self._categories: Dict[str, List[str]] = PLUGIN_CATEGORY_REGISTRY

    @validate_arguments
    def _load_single_plugin_impl(self, plugin_name: str) -> Optional[Type]:
        """Helper for loading a single plugin implementation"""
        plugin = Path(plugin_name)
        name = plugin.stem
        ptype = plugin.parent.name

        module_name = f"synthcity.plugins.{ptype}.{name}"

        failed = False
        for retry in range(2):
            try:
                if module_name in sys.modules:
                    mod = sys.modules[module_name]
                else:
                    spec = importlib.util.spec_from_file_location(module_name, plugin)
                    if spec is None:
                        raise RuntimeError("invalid spec")
                    if not isinstance(spec.loader, Loader):
                        raise RuntimeError("invalid plugin type")

                    mod = importlib.util.module_from_spec(spec)
                    if module_name not in sys.modules:
                        sys.modules[module_name] = mod

                    spec.loader.exec_module(mod)
                cls = mod.plugin
                if cls is None:
                    log.critical(f"module disabled: {plugin_name}")
                    return None

                failed = False
                break
            except BaseException as e:
                log.critical(f"load failed: {e}")
                failed = True

        if failed:
            log.critical(f"module {name} load failed")
            return None

        return cls

    @validate_arguments
    def _load_single_plugin(self, plugin_name: str) -> bool:
        """Helper for loading a single plugin"""
        cls = self._load_single_plugin_impl(plugin_name)
        if cls is None:
            return False

        self.add(cls.name(), cls)
        return True

    def list(self) -> List[str]:
        """Get all the available plugins."""
        self._refresh()
        all_plugins = list(self._plugins.keys()) + list(self._available_plugins.keys())
        plugins = []
        for plugin in all_plugins:
            if self.get_type(plugin).type() in self._categories:
                plugins.append(plugin)
        return list(set(plugins))

    def types(self) -> List[Type]:
        """Get the loaded plugins types"""
        self._refresh()
        return list(self._plugins.values())

    def _add_category(self, category: str, name: str) -> "PluginLoader":
        """Add a new plugin category"""
        log.debug(f"Registering plugin category {category}")
        if (
            category in PLUGIN_CATEGORY_REGISTRY
            and name in PLUGIN_CATEGORY_REGISTRY[category]
        ):
            raise TypeError(
                f"Plugin {name} is already registered as category: {category}"
            )
        if PLUGIN_CATEGORY_REGISTRY.get(category, None) is not None:
            PLUGIN_CATEGORY_REGISTRY[category].append(name)
        else:
            PLUGIN_CATEGORY_REGISTRY[category] = [name]
        return self

    def add(self, name: str, cls: Type) -> "PluginLoader":
        """Add a new plugin"""
        global PLUGIN_REGISTRY
        global PLUGIN_CATEGORY_REGISTRY
        self._refresh()
        if name in self._plugins:
            log.info(f"Plugin {name} already exists. Overwriting")

        if not issubclass(cls, self._expected_type):
            raise ValueError(
                f"Plugin {name} must derive the {self._expected_type} interface."
            )

        if (
            cls.type() not in PLUGIN_CATEGORY_REGISTRY.keys()
            or name not in PLUGIN_CATEGORY_REGISTRY.get(cls.type(), [])
        ):
            self._add_category(str(cls.type()), name)
        PLUGIN_REGISTRY[name] = cls
        return self

    @validate_arguments
    def load(self, buff: bytes) -> Any:
        """Load serialized plugin"""
        return Plugin.load(buff)

    @validate_arguments
    def get(self, name: str, *args: Any, **kwargs: Any) -> Any:
        """Create a new object from a plugin.
        Args:
            name: str. The name of the plugin
            &args, **kwargs. Plugin specific arguments

        Returns:
            The new object
        """
        self._refresh()
        if name not in self._plugins and name not in self._available_plugins:
            raise ValueError(f"Plugin {name} doesn't exist.")

        if name not in self._plugins:
            self._load_single_plugin(self._available_plugins[name])

        if name not in self._plugins:
            raise ValueError(f"Plugin {name} cannot be loaded.")

        return self._plugins[name](*args, **kwargs)

    @validate_arguments
    def get_type(self, name: str) -> Type:
        """Get the class type of a plugin.
        Args:
            name: str. The name of the plugin

        Returns:
            The class of the plugin
        """
        self._refresh()
        if name not in self._plugins and name not in self._available_plugins:
            raise ValueError(f"Plugin {name} doesn't exist.")

        if name not in self._plugins:
            self._load_single_plugin(self._available_plugins[name])

        if name not in self._plugins:
            raise ValueError(f"Plugin {name} doesn't exist.")

        return self._plugins[name]

    def __iter__(self) -> Generator:
        """Iterate the loaded plugins."""
        self._refresh()
        for x in self._plugins:
            yield x

    def __len__(self) -> int:
        """The number of available plugins."""
        return len(self.list())

    @validate_arguments
    def __getitem__(self, key: str) -> Any:
        return self.get(key)

    def reload(self) -> "PluginLoader":
        global PLUGIN_CATEGORY_REGISTRY
        global PLUGIN_REGISTRY
        PLUGIN_CATEGORY_REGISTRY = dict()
        PLUGIN_REGISTRY = dict()
        return self


src/synthcity/plugins/core/schema.py
# stdlib
from typing import Any, Dict, Generator, List, Optional, Union

# third party
import pandas as pd
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    field_validator,
    model_validator,
    validate_arguments,
)

# synthcity absolute
import synthcity.logger as log
from synthcity.plugins.core.constraints import Constraints
from synthcity.plugins.core.dataloader import DataLoader, GenericDataLoader
from synthcity.plugins.core.distribution import (
    CategoricalDistribution,
    DatetimeDistribution,
    Distribution,
    FloatDistribution,
    IntegerDistribution,
    PassThroughDistribution,
)


class Schema(BaseModel):
    """
    Utility class for defining the schema of a Dataset.

    Constructor Args:
        domain: Dict
            A dictionary of feature_name: Distribution.
        sampling_strategy: str
            Taking value of "marginal" (default) or "uniform" (for debugging).
        protected_cols: List[str]
            List of columns that are exempt from distributional constraints (e.g. ID column)
        random_state: int
            Random seed (default 0)
        data: Any
            (Optional) the data set
    """

    sampling_strategy: str = Field(default="marginal")
    protected_cols: List[str] = []
    random_state: int = Field(default=0)
    domain: Dict = Field(default_factory=dict)

    data: Optional[Union[DataLoader, pd.DataFrame]] = Field(default=None, exclude=True)

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @field_validator("data", mode="before")
    def validate_data(cls, v: Any) -> Optional[DataLoader]:
        if v is not None:
            if isinstance(v, pd.DataFrame):
                return GenericDataLoader(v)
            elif isinstance(v, DataLoader):
                return v
            else:
                raise ValueError(
                    f"Invalid data type for 'data': {type(v)}. Expected DataLoader or pandas DataFrame."
                )
        return v

    @model_validator(mode="after")
    def initialize_domain(cls, model: "Schema") -> "Schema":
        if model.data is not None:
            X = model.data.dataframe()
            model.domain = model._infer_domain(
                X,
                sampling_strategy=model.sampling_strategy,
                random_state=model.random_state,
            )
            # Remove 'data' attribute from the model
            del model.__dict__["data"]
            if "data" in model.__fields_set__:
                model.__fields_set__.remove("data")
        return model

    @validate_arguments
    def get(self, feature: str) -> Distribution:
        """Get the Distribution of a feature.

        Args:
            feature: str. the feature name

        Returns:
            The feature distribution
        """
        if feature not in self.domain:
            raise ValueError(f"invalid feature {feature}")

        return self.domain[feature]

    @validate_arguments
    def __getitem__(self, key: str) -> Distribution:
        """Get the Distribution of a feature.

        Args:
            feature: str. the feature name

        Returns:
            The feature distribution
        """
        return self.get(key)

    def __iter__(self) -> Generator:
        """Iterate the features distribution"""
        for x in self.domain:
            yield x

    def __len__(self) -> int:
        """Get the number of features"""
        return len(self.domain)

    def includes(self, other: "Schema") -> bool:
        """Test if another schema is included in the local one."""
        for feature in other:
            if feature in self.protected_cols:
                continue
            if feature not in self.domain:
                return False

            if not self[feature].includes(other[feature]):
                return False

        return True

    def features(self) -> List:
        return list(self.domain.keys())

    def sample(self, count: int) -> pd.DataFrame:
        data = {}
        for col, dist in self.domain.items():
            samples = dist.sample(count)
            data[col] = samples
        return pd.DataFrame(data)

    def adapt_dtypes(self, X: pd.DataFrame) -> pd.DataFrame:
        """Applying the data type to a new data frame

        Args:
            X: pd.DataFrame
                A new data frame to be adapted.

        Returns:
            A data frame whose data types are coerced to be the same with the Schema.
            If the data frame contains new features, these will be retained as is.
        """
        for feature in self.domain:
            if feature not in X.columns:
                continue
            X[feature] = X[feature].astype(
                self.domain[feature].dtype(), errors="ignore"
            )

        return X

    def as_constraints(self) -> Constraints:
        rules = []
        for feature, dist in self.domain.items():
            rules.extend(dist.as_constraint().rules)
        return Constraints(rules=rules)

    @classmethod
    def from_constraints(cls, constraints: Constraints) -> "Schema":
        domain: Dict = {}
        feature_params: Dict = {}

        # Collect constraint information
        for feature, op, value in constraints.rules:
            if feature not in feature_params:
                feature_params[feature] = {
                    "name": feature,
                    "random_state": None,
                    "low": None,
                    "high": None,
                    "dtype": "float",  # Default to 'float' if not specified
                    "choices": [],
                }

            params = feature_params[feature]

            if op in ["ge", ">="]:
                if params["low"] is None or value > params["low"]:
                    params["low"] = value
            elif op in ["le", "<="]:
                if params["high"] is None or value < params["high"]:
                    params["high"] = value
            elif op in ["eq", "=="]:
                # For '==', set both 'low' and 'high' to value
                params["low"] = value
                params["high"] = value
            elif op in ["in", "isin"]:
                if isinstance(value, list):
                    params["choices"].extend(value)
                else:
                    params["choices"].append(value)
            elif op == "dtype":
                params["dtype"] = value
            else:
                # Handle other operators if necessary
                pass

        # Create distribution objects
        for feature, params in feature_params.items():
            dtype = params["dtype"]
            if dtype == "float":
                if params["low"] is None or params["high"] is None:
                    raise ValueError(
                        f"Cannot create FloatDistribution for '{feature}' without 'low' and 'high' values."
                    )
                domain[feature] = FloatDistribution(
                    name=params["name"],
                    random_state=params["random_state"],
                    low=params["low"],
                    high=params["high"],
                )
            elif dtype == "int":
                if params["low"] is None or params["high"] is None:
                    raise ValueError(
                        f"Cannot create IntegerDistribution for '{feature}' without 'low' and 'high' values."
                    )
                domain[feature] = IntegerDistribution(
                    name=params["name"],
                    random_state=params["random_state"],
                    low=int(params["low"]),
                    high=int(params["high"]),
                    step=1,  # Default step to 1 or adjust as needed
                )
            elif dtype in ["category", "object"]:
                choices = params.get("choices")
                if choices is None or not choices:
                    raise ValueError(
                        f"Cannot create CategoricalDistribution for '{feature}' without 'choices'."
                    )
                domain[feature] = CategoricalDistribution(
                    name=params["name"],
                    random_state=params["random_state"],
                    choices=list(set(choices)),
                )
            else:
                raise ValueError(
                    f"Unsupported dtype '{dtype}' for feature '{feature}'."
                )

        return cls(domain=domain)

    def _infer_domain(
        self,
        X: pd.DataFrame,
        sampling_strategy: str,
        random_state: int,
    ) -> Dict[str, Distribution]:
        feature_domain: Dict[str, Distribution] = {}

        for idx, col in enumerate(X.columns):
            col_random_state = random_state + idx + 1  # Ensure unique seeds

            try:
                if sampling_strategy == "marginal":
                    if col in self.protected_cols:
                        feature_domain[col] = PassThroughDistribution(
                            name=col,
                            data=X[col],
                            random_state=col_random_state,
                        )
                        continue

                    is_categorical = pd.api.types.is_categorical_dtype(X[col])
                    is_object = X[col].dtype == object
                    is_bool = pd.api.types.is_bool_dtype(X[col])
                    is_integer = pd.api.types.is_integer_dtype(X[col])
                    is_float = pd.api.types.is_float_dtype(X[col])
                    is_datetime = pd.api.types.is_datetime64_any_dtype(X[col])

                    if is_categorical or is_object or is_bool:
                        feature_domain[col] = CategoricalDistribution(
                            name=col,
                            data=X[col],
                            random_state=col_random_state,
                        )
                    elif is_integer:
                        feature_domain[col] = IntegerDistribution(
                            name=col,
                            data=X[col],
                            random_state=col_random_state,
                        )
                    elif is_float:
                        feature_domain[col] = FloatDistribution(
                            name=col,
                            data=X[col],
                            random_state=col_random_state,
                        )
                    elif is_datetime:
                        feature_domain[col] = DatetimeDistribution(
                            name=col,
                            data=X[col],
                            random_state=col_random_state,
                        )
                    else:
                        raise ValueError(
                            f"Unsupported data type for column '{col}' with dtype {X[col].dtype}"
                        )
                elif sampling_strategy == "uniform":

                    is_categorical = pd.api.types.is_categorical_dtype(X[col])
                    is_object = X[col].dtype == object
                    is_bool = pd.api.types.is_bool_dtype(X[col])
                    is_integer = pd.api.types.is_integer_dtype(X[col])
                    is_float = pd.api.types.is_float_dtype(X[col])
                    is_datetime = pd.api.types.is_datetime64_any_dtype(X[col])

                    if (
                        pd.api.types.is_categorical_dtype(X[col])
                        or X[col].dtype == object
                        or pd.api.types.is_bool_dtype(X[col])
                    ):
                        feature_domain[col] = CategoricalDistribution(
                            name=col,
                            choices=list(X[col].unique()),
                            random_state=col_random_state,
                            sampling_strategy=sampling_strategy,
                        )
                    elif pd.api.types.is_integer_dtype(X[col]):
                        feature_domain[col] = IntegerDistribution(
                            name=col,
                            low=X[col].min(),
                            high=X[col].max(),
                            random_state=col_random_state,
                            sampling_strategy=sampling_strategy,
                        )
                    elif pd.api.types.is_float_dtype(X[col]):
                        feature_domain[col] = FloatDistribution(
                            name=col,
                            low=X[col].min(),
                            high=X[col].max(),
                            random_state=col_random_state,
                            sampling_strategy=sampling_strategy,
                        )
                    elif pd.api.types.is_datetime64_any_dtype(X[col]):
                        feature_domain[col] = DatetimeDistribution(
                            name=col,
                            low=X[col].min(),
                            high=X[col].max(),
                            random_state=col_random_state,
                            sampling_strategy=sampling_strategy,
                        )
                else:
                    raise ValueError(
                        f"Unsupported sampling strategy '{sampling_strategy}'"
                    )
            except Exception as e:
                log.error(f"Exception occurred while processing column '{col}': {e}")
                raise
        return feature_domain


src/synthcity/plugins/core/serializable.py
# stdlib
import copy
import importlib.util
import os
from importlib.abc import Loader
from pathlib import Path
from typing import Any, Optional

# third party
from pydantic import validate_arguments

# synthcity absolute
from synthcity.utils.serialization import load as deserialize
from synthcity.utils.serialization import save as serialize
from synthcity.version import MAJOR_VERSION

module_path = Path(__file__).resolve()
module_parent_path = module_path.parent


class Serializable:
    """Utility class for model persistence."""

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)
        derived_module_path: Optional[Path] = None
        self.fitted = (
            False  # make sure all serializable objects are not fitted by default
        )

        search_module = self.__class__.__module__
        if not search_module.endswith(".py"):
            search_module = search_module.split(".")[-1]
            search_module += ".py"

        for path in module_path.parent.parent.rglob(search_module):
            derived_module_path = path
            break

        self.module_relative_path: Optional[Path] = None

        if derived_module_path is not None:
            relative_path = Path(
                os.path.relpath(derived_module_path, start=module_path.parent)
            )

            if not (module_parent_path / relative_path).resolve().exists():
                raise RuntimeError(
                    f"cannot find relative module path for {relative_path.resolve()}"
                )

            self.module_relative_path = relative_path

        self.module_name = self.__class__.__module__
        self.class_name = self.__class__.__qualname__
        self.raw_class = self.__class__

    def save_dict(self) -> dict:
        members: dict = {}

        for key in self.__dict__:
            data = self.__dict__[key]
            if isinstance(data, Serializable):
                members[key] = self.__dict__[key].save_dict()
            elif key == "model":
                members[key] = serialize(self.__dict__[key])
            else:
                members[key] = copy.deepcopy(self.__dict__[key])

        if "fitted" not in members:
            members["fitted"] = self.fitted  # Ensure 'fitted' is always serialized

        return {
            "source": "synthcity",
            "data": members,
            "version": self.version(),
            "class_name": self.class_name,
            "class": self.raw_class,
            "module_name": self.module_name,
            "module_relative_path": self.module_relative_path,
        }

    def save(self) -> bytes:
        return serialize(self.save_dict())

    @validate_arguments
    def save_to_file(self, path: Path) -> bytes:
        raise NotImplementedError()

    @staticmethod
    # @validate_arguments
    def load_dict(representation: dict) -> Any:
        if "source" not in representation or representation["source"] != "synthcity":
            raise ValueError("Invalid synthcity object")

        if representation["version"] != Serializable.version():
            raise RuntimeError(
                f"Invalid synthcity API version. Current version is {Serializable.version()}, but the object was serialized using version {representation['version']}"
            )

        if representation["module_relative_path"] is not None:
            module_path = module_parent_path / representation["module_relative_path"]

            if not module_path.exists():
                raise RuntimeError(f"Unknown module path {module_path}")

            spec = importlib.util.spec_from_file_location(
                representation["module_name"], module_path
            )
            if spec is None:
                raise RuntimeError("Invalid spec")

            if not isinstance(spec.loader, Loader):
                raise RuntimeError("invalid synthcity object type")

            mod = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(mod)

        cls = representation["class"]

        obj = cls()

        obj_dict = {}
        for key in representation["data"]:
            val = representation["data"][key]

            if (
                isinstance(val, dict)
                and "source" in val
                and val["source"] == "synthcity"
            ):
                obj_dict[key] = Serializable.load_dict(val)
            else:
                obj_dict[key] = val

        obj.__dict__ = obj_dict

        return obj

    @staticmethod
    @validate_arguments
    def load(buff: bytes) -> Any:
        representation = deserialize(buff)

        return Serializable.load_dict(representation)

    @staticmethod
    def version() -> str:
        "API version"
        return MAJOR_VERSION


src/synthcity/plugins/core/constraints.py
# stdlib
from typing import Any, Generator, List, Tuple

# third party
import numpy as np
import pandas as pd
from pydantic import BaseModel, field_validator, validate_arguments

# synthcity absolute
import synthcity.logger as log

Rule = Tuple[str, str, Any]  # Define a type alias for clarity


class Constraints(BaseModel):
    """
    .. inheritance-diagram:: synthcity.plugins.core.constraints.Constraints
        :parts: 1


    Constraints on data.

    The Constraints class allows users to specify constraints on the features. Examples include the feature value range, allowed item set, and data type.
    These constraints can be used to filter out invalid values in synthetic datasets.

    Constructor Args:
        rules: List[Tuple]
            Each tuple in the list specifies a constraint on a feature. The tuple has the form of (feature, op, thresh),
            where feature is the feature name to apply constraint on, op takes values in [
                    "<",
                    ">=",
                    "<=",
                    ">",
                    "==",
                    "lt",
                    "le",
                    "gt",
                    "ge",
                    "eq",
                    "in",
                    "dtype",
                ],
            and thresh is the threshold or data type.
    """

    rules: list[Rule] = []

    @field_validator("rules", mode="before")
    def _validate_rules(cls: Any, rules: List) -> List:
        supported_ops: list = [
            "<",
            ">=",
            "<=",
            ">",
            "==",
            "lt",
            "le",
            "gt",
            "ge",
            "eq",
            "in",
            "dtype",
        ]

        for rule in rules:
            if len(rule) < 3:
                raise ValueError(f"Invalid constraint. Expecting tuple, but got {rule}")

            feature, op, thresh = rule

            if op not in supported_ops:
                raise ValueError(
                    f"Invalid operation {op}. Supported ops: {supported_ops}"
                )
            if op in ["in"]:
                if not isinstance(thresh, list):
                    raise ValueError("Invalid type for threshold = {type(thresh)}")
            elif op in ["dtype"]:
                if not isinstance(thresh, str):
                    raise ValueError("Invalid type for threshold = {type(thresh)}")

        return rules

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def _eval(self, X: pd.DataFrame, feature: str, op: str, operand: Any) -> pd.Index:
        """Evaluation primitive.

        Args:
            X: DataFrame. The dataset to apply the constraint on.
            feature: str. The column in the dataset to apply the constraint on.
            op: str. The operation to execute for the constraint.
            operand: Any. The operand for the binary operation.

        Returns:
            The pandas.Index which matches the constraint.
        """
        if op == "lt" or op == "<":
            return (X[feature] < operand) | X[feature].isna()
        elif op == "le" or op == "<=":
            return (X[feature] <= operand) | X[feature].isna()
        elif op == "gt" or op == ">":
            return (X[feature] > operand) | X[feature].isna()
        elif op == "ge" or op == ">=":
            return (X[feature] >= operand) | X[feature].isna()
        elif op == "eq" or op == "==":
            return (X[feature] == operand) | X[feature].isna()
        elif op == "in":
            return (X[feature].isin(operand)) | X[feature].isna()
        elif op == "dtype":
            return operand in str(X[feature])
        else:
            raise RuntimeError("unsupported operation", op)

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def _correct(
        self, X: pd.DataFrame, feature: str, op: str, operand: Any
    ) -> pd.DataFrame:
        """Correct limits.

        Args:
            X: DataFrame. The dataset to apply the constraint on.
            feature: str. The column in the dataset to apply the constraint on.
            op: str. The operation to execute for the constraint.
            operand: Any. The operand for the binary operation.

        """
        _filter = self._eval(X, feature, op, operand)
        if op in [
            "lt",
            "le",
            "gt",
            "ge",
            "eq",
            "<",
            "<=",
            ">",
            ">=",
            "==",
        ]:
            X.loc[~_filter, feature] = operand

        return X

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def filter(self, X: pd.DataFrame) -> pd.DataFrame:
        """Apply the constraints to a DataFrame X.

        Args:
            X: DataFrame. The dataset to apply the constraints on.

        Returns:
            pandas.Index which matches all the constraints
        """
        X = pd.DataFrame(X)
        res = pd.Series([True] * len(X), index=X.index)
        for feature, op, thresh in self.rules:
            if feature not in X:
                res &= False
                break

            prev = res.sum()
            res &= self._eval(
                X,
                feature,
                op,
                thresh,
            )
            if res.sum() < prev:
                log.info(
                    f"[{feature}] quality loss for constraints {op} = {thresh}. Remaining {res.sum()}. prev length {prev}. Original dtype {X[feature].dtype}.",
                )
        return res

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def match(self, X: pd.DataFrame) -> pd.DataFrame:
        """Apply the constraints to a DataFrame X and return the filtered dataset.

        Args:
            X: DataFrame. The dataset to apply the constraints on.

        Returns:
            The filtered Dataframe
        """

        return X[self.filter(X)]

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def is_valid(self, X: pd.DataFrame) -> bool:
        """Checks if all the rows in X meet the constraints.

        Args:
            X: DataFrame. The dataset to apply the constraints on.

        Returns:
            True if all rows match the constraints, False otherwise
        """

        return self.filter(X).sum() == len(X)

    def extend(self, other: "Constraints") -> "Constraints":
        """Extend the local constraints with more constraints.

        Args:
            other: The new constraints to add.

        Returns:
            self with the updated constraints.
        """
        self.rules.extend(other.rules)

        return self

    def __len__(self) -> int:
        """The number of constraint rules."""
        return len(self.rules)

    def __iter__(self) -> Generator:
        """Iterate the constraint rules."""
        for x in self.rules:
            yield x

    def features(self) -> List:
        """Return list of feature names in an undefined order"""
        results = []
        for feature, _, _ in self.rules:
            results.append(feature)

        return list(set(results))

    def feature_constraints(self, ref_feature: str) -> List:
        """Get constraints for a given feature

        Args:
            ref_feature: str
                The name of the feature of interest.

        Returns:
            A list of tuples of (op, threshold). For example:

            [('le', 3.), ('gt', 1.)]

            If ref_feature has no constraint, None will be returned.
        """
        results = []
        for feature, op, threshold in self.rules:
            if feature != ref_feature:
                continue
            results.append((op, threshold))

        return results

    def feature_params(self, feature: str) -> Tuple:
        """Provide the parameters of Distribution from the Constraint

        This is to be used with the constraint_to_distribution function in distribution module.

        Args:
            feature: str
                The name of the feature of interest.

        Returns:
            dist_template: str
                The type of inferred distribution from ("categorical", "float", "integer")
            dist_args: Dict
                The arguments to the constructor of the Distribution.
        """

        rules = self.feature_constraints(feature)

        dist_template = "float"
        dist_args = {"low": np.iinfo(np.int64).min, "high": np.iinfo(np.int64).max}

        for op, value in rules:
            if op == "in":
                dist_template = "categorical"
                if "choices" not in dist_args:
                    dist_args["choices"] = value
                    continue
                dist_args["choices"] = [v for v in value if v in dist_args["choices"]]

            elif op == "dtype" and value in ["int", "int32", "int64", "integer"]:
                dist_template = "integer"
            elif (op == "le" or op == "<=") and value < dist_args["high"]:
                dist_args["high"] = value
                if "choices" in dist_args:
                    dist_args["choices"] = [
                        v for v in dist_args["choices"] if v <= value
                    ]
            elif (op == "lt" or op == "<") and value < dist_args["high"]:
                dist_args["high"] = value - 1
                if "choices" in dist_args:
                    dist_args["choices"] = [
                        v for v in dist_args["choices"] if v < value
                    ]
            elif (op == "ge" or op == ">=") and dist_args["low"] < value:
                dist_args["low"] = value
                if "choices" in dist_args:
                    dist_args["choices"] = [
                        v for v in dist_args["choices"] if v >= value
                    ]
            elif (op == "gt" or op == ">") and dist_args["low"] < value:
                dist_args["low"] = value + 1
                if "choices" in dist_args:
                    dist_args["choices"] = [
                        v for v in dist_args["choices"] if v > value
                    ]
            elif op == "eq" or op == "==":
                dist_args["low"] = value
                dist_args["high"] = value
                dist_args["choices"] = [value]

        return dist_template, dist_args


src/synthcity/plugins/core/distribution.py
# stdlib
from abc import ABCMeta, abstractmethod
from datetime import datetime, timedelta, timezone
from typing import Any, List, Optional, Tuple

# third party
import numpy as np
import pandas as pd
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    FieldValidationInfo,
    PrivateAttr,
    ValidationInfo,
    field_validator,
    model_validator,
)

# synthcity absolute
from synthcity.plugins.core.constraints import Constraints

Rule = Tuple[str, str, Any]  # Define a type alias for clarity


class Distribution(BaseModel, metaclass=ABCMeta):
    """
    .. inheritance-diagram:: synthcity.plugins.core.distribution.Distribution
        :parts: 1

    Base class of all Distributions.

    The Distribution class characterizes the **empirical** marginal distribution of the feature.
    Each derived class must implement the following methods:
        get() - Return the metadata of the Distribution.
        sample() - Sample a value from the Distribution.
        includes() - Test if another Distribution is included in the local one.
        has() - Test if a value is included in the support of the Distribution.
        as_constraint() - Convert the Distribution to a set of Constraints.
        min() - Return the minimum of the support.
        max() - Return the maximum of the support.
        __eq__() - Testing equality of two Distributions.
        dtype() - Return the data type

    Examples of derived classes include CategoricalDistribution, FloatDistribution, and IntegerDistribution.
    """

    name: str
    data: Optional[pd.Series] = None
    random_state: Optional[int] = None
    sampling_strategy: str = "marginal"
    _rng: np.random.Generator = PrivateAttr()
    # DP parameters
    marginal_distribution: Optional[pd.Series] = None

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @field_validator("marginal_distribution", mode="before")
    def _validate_marginal_distribution(
        cls: Any, v: Any, values: FieldValidationInfo
    ) -> Optional[pd.Series]:
        if "data" not in values.data or values.data["data"] is None:
            return v

        data = values.data["data"]
        if not isinstance(data, pd.Series):
            raise ValueError(f"Invalid data type {type(data)}")

        marginal = data.value_counts(dropna=False)
        del values["data"]

        return marginal

    @model_validator(mode="after")
    def initialize_rng(cls, model: "Distribution") -> "Distribution":
        """
        Initializes the random number generator after model validation.
        """
        if model.random_state is not None:
            model._rng = np.random.default_rng(model.random_state)
        else:
            model._rng = np.random.default_rng()
        return model

    def marginal_states(self) -> Optional[List]:
        if self.marginal_distribution is None:
            return None

        return self.marginal_distribution.index.values

    def marginal_probabilities(self) -> Optional[List]:
        if self.marginal_distribution is None:
            return None

        return (
            self.marginal_distribution.values / self.marginal_distribution.values.sum()
        )

    def sample_marginal(self, count: int = 1) -> Any:
        if self.marginal_distribution is None:
            return None

        return self._rng.choice(
            self.marginal_states(),
            count,
            p=self.marginal_probabilities(),
        ).tolist()

    @abstractmethod
    def get(self) -> List[Any]:
        """Return the metadata of the Distribution."""
        ...

    @abstractmethod
    def sample(self, count: int = 1) -> Any:
        """Sample a value from the Distribution."""
        ...

    @abstractmethod
    def includes(self, other: "Distribution") -> bool:
        """Test if another Distribution is included in the local one."""
        ...

    @abstractmethod
    def has(self, val: Any) -> bool:
        """Test if a value is included in the Distribution."""
        ...

    @abstractmethod
    def as_constraint(self) -> Constraints:
        """Convert the Distribution to a set of Constraints."""
        ...

    @abstractmethod
    def min(self) -> Any:
        """Get the min value of the distribution."""
        ...

    @abstractmethod
    def max(self) -> Any:
        """Get the max value of the distribution."""
        ...

    def __eq__(self, other: Any) -> bool:
        return type(self) == type(other) and self.get() == other.get()

    def __contains__(self, item: Any) -> bool:
        """
        Example:
        >>> dist = CategoricalDistribution(name="foo", choices=["a", "b", "c"])
        >>> "a" in dist
        True
        """
        return self.has(item)

    @abstractmethod
    def dtype(self) -> str:
        ...


class CategoricalDistribution(Distribution):
    """
    .. inheritance-diagram:: synthcity.plugins.core.distribution.CategoricalDistribution
        :parts: 1
    """

    data: Optional[pd.Series] = None
    marginal_distribution: Optional[pd.Series] = None
    choices: List[Any] = Field(default_factory=list)

    @model_validator(mode="after")
    def validate_and_initialize(
        cls, model: "CategoricalDistribution"
    ) -> "CategoricalDistribution":
        """
        Validates and initializes choices and marginal_distribution based on data or provided choices.
        Ensures that choices are unique and sorted.
        """
        if model.data is not None:
            # Set marginal_distribution based on data
            model.marginal_distribution = model.data.value_counts(normalize=True)
            model.choices = model.marginal_distribution.index.tolist()
        elif model.choices is not None:
            # Ensure choices are unique and sorted
            model.choices = sorted(set(model.choices))
            # Set uniform probabilities
            probabilities = np.ones(len(model.choices)) / len(model.choices)
            model.marginal_distribution = pd.Series(probabilities, index=model.choices)
        else:
            raise ValueError(
                "Invalid CategoricalDistribution: Provide either 'data' or 'choices'."
            )

        # Additional validation to ensure consistency
        if not isinstance(model.choices, list) or len(model.choices) == 0:
            raise ValueError(
                "CategoricalDistribution must have a non-empty 'choices' list."
            )
        if not isinstance(model.marginal_distribution, pd.Series):
            raise ValueError(
                "CategoricalDistribution must have a valid 'marginal_distribution'."
            )
        if len(model.choices) != len(model.marginal_distribution):
            raise ValueError(
                "'choices' and 'marginal_distribution' must have the same length."
            )

        return model

    def sample(self, count: int = 1) -> Any:
        """
        Samples values from the distribution based on the specified sampling strategy.
        If the distribution has only one choice, returns an array filled with that value.
        """
        if self.choices is not None and len(self.choices) == 1:
            samples = np.full(count, self.choices[0])
        else:
            if self.sampling_strategy == "marginal":
                if self.marginal_distribution is None:
                    raise ValueError(
                        "Cannot sample based on marginal distribution: marginal_distribution is not provided."
                    )
                return self._rng.choice(
                    self.marginal_distribution.index,
                    size=count,
                    p=self.marginal_distribution.values,
                )
            elif self.sampling_strategy == "uniform":
                return self._rng.choice(self.choices, size=count)
            else:
                raise ValueError(
                    f"Unsupported sampling strategy '{self.sampling_strategy}'."
                )
        return samples

    def get(self) -> List[Any]:
        """
        Returns the metadata of the distribution.
        """
        return [self.name, self.choices]

    def has(self, val: Any) -> bool:
        """
        Checks if a value is among the distribution's choices.
        """
        return val in self.choices

    def includes(self, other: "Distribution") -> bool:
        """
        Checks if another categorical distribution's choices are a subset of this distribution's choices.
        """
        if not isinstance(other, CategoricalDistribution):
            return False
        return set(other.choices).issubset(set(self.choices))

    def as_constraint(self) -> Constraints:
        """
        Converts the distribution to a set of constraints.
        """
        return Constraints(rules=[(self.name, "in", list(self.choices))])

    def min(self) -> Any:
        """
        Returns the minimum value among the choices.
        """
        return min(self.choices)

    def max(self) -> Any:
        """
        Returns the maximum value among the choices.
        """
        return max(self.choices)

    def dtype(self) -> str:
        """
        Determines the data type based on the choices.
        """
        types = {
            "object": 0,
            "float": 0,
            "int": 0,
        }
        for v in self.choices:
            if isinstance(v, float):
                types["float"] += 1
            elif isinstance(v, int):
                types["int"] += 1
            else:
                types["object"] += 1

        for t in types:
            if types[t] != 0:
                return t

        return "object"


class FloatDistribution(Distribution):
    """
    .. inheritance-diagram:: synthcity.plugins.core.distribution.FloatDistribution
        :parts: 1
    """

    low: Optional[float] = Field(default=None)
    high: Optional[float] = Field(default=None)
    _is_constant: bool = PrivateAttr(False)

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @model_validator(mode="after")
    def validate_and_initialize(cls, model: "FloatDistribution") -> "FloatDistribution":
        """
        Validates and initializes the distribution.
        Sets '_is_constant' based on whether 'low' equals 'high'.
        Initializes 'marginal_distribution' based on 'data' if provided.
        """
        if model.data is not None:
            # Initialize marginal_distribution based on data
            # For float data, use value_counts(normalize=True) if data has repeated values
            # This will create a discrete approximation of the distribution
            model.marginal_distribution = model.data.value_counts(
                normalize=True
            ).sort_index()
            model.low = float(model.data.min())
            model.high = float(model.data.max())
        elif model.marginal_distribution is not None:
            # Set 'low' and 'high' based on marginal_distribution
            model.low = float(model.marginal_distribution.index.min())
            model.high = float(model.marginal_distribution.index.max())
        else:
            # Ensure 'low' and 'high' are provided
            if model.low is None or model.high is None:
                raise ValueError(
                    "FloatDistribution requires 'low' and 'high' values if 'data' or 'marginal_distribution' is not provided."
                )

        # Validate that low <= high
        if model.low > model.high:
            raise ValueError(
                f"Invalid range for '{model.name}': low ({model.low}) cannot be greater than high ({model.high})."
            )

        # Set _is_constant based on low == high
        model._is_constant = model.low == model.high

        # Ensure that low and high are finite numbers
        if not np.isfinite(model.low) or not np.isfinite(model.high):
            raise ValueError(
                f"Invalid range for '{model.name}': low or high is not finite (low={model.low}, high={model.high})."
            )

        return model

    def sample(self, count: int = 1) -> Any:
        """
        Samples values from the distribution.
        If the distribution is constant, returns an array filled with the constant value.
        Otherwise, samples based on the marginal distribution or uniform sampling.
        """
        if self._is_constant:
            if self.low is None:
                raise ValueError(
                    "Cannot sample: 'low' is None for a constant distribution."
                )
            samples = np.full(count, self.low)
        else:
            if self.low is None or self.high is None:
                raise ValueError("Cannot sample: 'low' or 'high' is None.")
            if (
                self.sampling_strategy == "marginal"
                and self.marginal_distribution is not None
            ):
                # Sample based on marginal distribution
                return self._rng.choice(
                    self.marginal_distribution.index.values,
                    size=count,
                    p=self.marginal_distribution.values,
                )
            else:
                # Proceed with uniform sampling
                samples = self._rng.uniform(low=self.low, high=self.high, size=count)
        return samples

    def get(self) -> List[Any]:
        """
        Returns the metadata of the distribution.
        """
        return [self.name, self.low, self.high]

    def has(self, val: Any) -> bool:
        """
        Checks if a value is within the distribution's range.
        """
        return self.low <= val <= self.high

    def includes(self, other: "Distribution") -> bool:
        """
        Checks if another distribution is entirely within this distribution.
        """
        if self.min() is None or self.max() is None:
            return False
        if other.min() is None or other.max() is None:
            return False
        return self.min() <= other.min() and other.max() <= self.max()

    def as_constraint(self) -> Constraints:
        """
        Converts the distribution to a set of constraints.
        """
        return Constraints(
            rules=[
                (self.name, "le", self.high),
                (self.name, "ge", self.low),
                (self.name, "dtype", "float"),
            ]
        )

    def min(self) -> Any:
        """
        Returns the minimum value of the distribution.
        """
        return self.low

    def max(self) -> Any:
        """
        Returns the maximum value of the distribution.
        """
        return self.high

    def dtype(self) -> str:
        """
        Returns the data type of the distribution.
        """
        return "float"


class LogDistribution(FloatDistribution):
    low: float = np.finfo(np.float64).tiny
    high: float = np.finfo(np.float64).max

    def get(self) -> List[Any]:
        return [self.name, self.low, self.high]

    def sample(self, count: int = 1) -> Any:
        msamples = self.sample_marginal(count)
        if msamples is not None:
            return msamples
        lo, hi = np.log2(self.low), np.log2(self.high)
        return 2.0 ** self._rng.uniform(lo, hi, count)


class IntegerDistribution(Distribution):
    """
    .. inheritance-diagram:: synthcity.plugins.core.distribution.IntegerDistribution
        :parts: 1
    """

    low: Optional[int] = Field(default=None)
    high: Optional[int] = Field(default=None)
    step: int = Field(default=1)
    _is_constant: bool = PrivateAttr(False)

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @model_validator(mode="after")
    def validate_and_initialize(
        cls, model: "IntegerDistribution"
    ) -> "IntegerDistribution":
        """
        Validates and initializes the distribution.
        Sets '_is_constant' based on whether 'low' equals 'high'.
        Initializes 'marginal_distribution' based on 'data' if provided.
        """
        if model.data is not None:
            # Initialize marginal_distribution based on data
            model.marginal_distribution = model.data.value_counts(
                normalize=True
            ).sort_index()
            model.low = int(model.data.min())
            model.high = int(model.data.max())
        elif model.marginal_distribution is not None:
            # Infer 'low' and 'high' from the marginal distribution's index
            model.low = int(model.marginal_distribution.index.min())
            model.high = int(model.marginal_distribution.index.max())
        else:
            # Ensure 'low' and 'high' are provided
            if model.low is None or model.high is None:
                raise ValueError(
                    "IntegerDistribution requires 'low' and 'high' values if 'data' or 'marginal_distribution' is not provided."
                )

        # Validate that low <= high
        if model.low > model.high:
            raise ValueError(
                f"Invalid range for '{model.name}': low ({model.low}) cannot be greater than high ({model.high})."
            )

        # Set _is_constant based on low == high
        model._is_constant = model.low == model.high

        # Ensure that low and high are finite integers
        if not np.isfinite(model.low) or not np.isfinite(model.high):
            raise ValueError(
                f"Invalid range for '{model.name}': low or high is not finite (low={model.low}, high={model.high})."
            )

        # Ensure that 'step' is a positive integer
        if model.step <= 0:
            raise ValueError("'step' must be a positive integer.")

        # Adjust 'low' and 'high' to be compatible with 'step'
        model.low = model.low - ((model.low - (model.low % model.step)) % model.step)
        model.high = model.high - (
            (model.high - (model.high % model.step)) % model.step
        )

        # Re-validate after adjustment
        if model.low > model.high:
            raise ValueError(
                f"After adjusting with step, invalid range for '{model.name}': low ({model.low}) cannot be greater than high ({model.high})."
            )

        return model

    def sample(self, count: int = 1) -> Any:
        """
        Samples values from the distribution.
        If the distribution is constant, returns an array filled with the constant value.
        Otherwise, samples based on the marginal distribution or uniform sampling.
        """
        if self._is_constant:
            if self.low is None:
                raise ValueError(
                    "Cannot sample: 'low' is None for a constant distribution."
                )
            samples = np.full(count, self.low)
        else:
            if self.low is None or self.high is None:
                raise ValueError("Cannot sample: 'low' or 'high' is None.")
            if (
                self.sampling_strategy == "marginal"
                and self.marginal_distribution is not None
            ):
                # Sample based on marginal distribution
                return self._rng.choice(
                    self.marginal_distribution.index,
                    size=count,
                    p=self.marginal_distribution.values,
                )
            else:
                if self.low is None or self.high is None:
                    raise ValueError(
                        "Cannot sample based on uniform distribution: low or high is not provided."
                    )
                # Proceed with uniform sampling
                possible_values = np.arange(self.low, self.high + 1, self.step)
                samples = self._rng.choice(possible_values, size=count)
        return samples

    def get(self) -> List[Any]:
        """
        Returns the metadata of the distribution.
        """
        return [self.name, self.low, self.high, self.step]

    def has(self, val: Any) -> bool:
        """
        Checks if a value is within the distribution's range.
        """
        return self.low <= val <= self.high

    def includes(self, other: "Distribution") -> bool:
        """
        Checks if another distribution is entirely within this distribution.
        """
        if self.min() is None or self.max() is None:
            return False
        if other.min() is None or other.max() is None:
            return False
        return self.min() <= other.min() and other.max() <= self.max()

    def as_constraint(self) -> Constraints:
        """
        Converts the distribution to a set of constraints.
        """
        rules: List[Rule] = []
        if self.low is not None:
            rules.append((self.name, "ge", self.low))
        if self.high is not None:
            rules.append((self.name, "le", self.high))
        rules.append((self.name, "dtype", "int"))
        return Constraints(rules=rules)

    def min(self) -> Any:
        """
        Returns the minimum value of the distribution.
        """
        return self.low

    def max(self) -> Any:
        """
        Returns the maximum value of the distribution.
        """
        return self.high

    def dtype(self) -> str:
        """
        Returns the data type of the distribution.
        """
        return "int"


class IntLogDistribution(IntegerDistribution):
    low: int = Field(default=1)
    high: int = Field(default=np.iinfo(np.int64).max)

    @field_validator("step", mode="before")
    def _validate_step(cls: Any, v: int, values: ValidationInfo) -> int:
        if v != 1:
            raise ValueError("Step must be 1 for IntLogDistribution")
        return v

    def get(self) -> List[Any]:
        return [self.name, self.low, self.high]

    def sample(self, count: int = 1) -> Any:
        msamples = self.sample_marginal(count)
        if msamples is not None:
            return msamples
        lo, hi = np.log2(self.low), np.log2(self.high)
        samples = 2.0 ** self._rng.uniform(lo, hi, count)
        return samples.astype(int)


class DatetimeDistribution(Distribution):
    """
    .. inheritance-diagram:: synthcity.plugins.core.distribution.DatetimeDistribution
        :parts: 1
    """

    low: Optional[datetime] = Field(default=None)
    high: Optional[datetime] = Field(default=None)
    step: timedelta = Field(default=timedelta(microseconds=1))
    offset: timedelta = Field(default=timedelta(seconds=120))
    _is_constant: bool = PrivateAttr(False)  # Correctly named with leading underscore

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @model_validator(mode="after")
    def validate_low_high(cls, model: "DatetimeDistribution") -> "DatetimeDistribution":
        """
        Validates that 'low' is less than or equal to 'high'.
        Sets '_is_constant' based on whether 'low' equals 'high'.
        """
        if model.marginal_distribution is not None:
            # Infer 'low' and 'high' from the marginal distribution's index
            model.low = model.marginal_distribution.index.min()
            model.high = model.marginal_distribution.index.max()
        else:
            # If 'marginal_distribution' is not provided, ensure 'low' and 'high' are set
            if model.low is None or model.high is None:
                if model.data is not None:
                    model.low = model.data.min()
                    model.high = model.data.max()
                else:
                    # Set default finite datetime values if not provided
                    model.low = datetime.fromtimestamp(0, timezone.utc)
                    model.high = datetime.now()
        if model.low is None or model.high is None:
            raise ValueError(
                "DatetimeDistribution requires 'low' and 'high' values if 'data' or 'marginal_distribution' is not provided."
            )
        # Validate that low <= high
        if model.low > model.high:
            raise ValueError(
                f"Invalid range for {model.name}: low ({model.low}) cannot be greater than high ({model.high})."
            )

        # Set _is_constant based on low == high
        model._is_constant = model.low == model.high

        # Ensure that low and high are valid datetime objects
        if not isinstance(model.low, datetime) or not isinstance(model.high, datetime):
            raise ValueError(
                f"Invalid range for {model.name}: low or high is not a valid datetime object (low={model.low}, high={model.high})."
            )

        # Ensure that 'step' is positive and non-zero
        if model.step.total_seconds() <= 0:
            raise ValueError("'step' must be a positive timedelta.")

        return model

    def sample(self, count: int = 1) -> Any:
        """
        Samples datetime values from the distribution.
        If the distribution is constant, returns a list filled with the constant datetime value.
        Otherwise, samples based on the specified sampling strategy.
        """
        if self._is_constant:
            if self.low is None:
                raise ValueError(
                    "Cannot sample constant datetime distribution: low is not provided."
                )
            samples = [self.low for _ in range(count)]
        else:
            if self.low is None or self.high is None:
                raise ValueError(
                    "Cannot sample datetime distribution: low or high is not provided."
                )
            if self.sampling_strategy in ["marginal", "uniform"]:
                msamples = self.sample_marginal(count)
                if msamples is not None:
                    return msamples
                if self.low is None or self.high is None:
                    raise ValueError(
                        "Cannot sample based on marginal distribution: low or high is not provided."
                    )
                total_seconds = (self.high - self.low).total_seconds()
                step_seconds = self.step.total_seconds()
                steps = int(total_seconds / step_seconds)
                step_indices = self._rng.integers(0, steps + 1, count)
                samples = [self.low + self.step * int(s) for s in step_indices]
            else:
                raise ValueError(
                    f"Unsupported sampling strategy '{self.sampling_strategy}'."
                )
        return samples

    def get(self) -> List[Any]:
        """
        Returns the metadata of the distribution.
        """
        return [self.name, self.low, self.high, self.step, self.offset]

    def has(self, val: datetime) -> bool:
        """
        Checks if a datetime value is within the distribution's range.
        """
        if self.low is None or self.high is None:
            raise ValueError("Cannot determine 'has' because 'low' or 'high' is None.")
        return self.low <= val <= self.high

    def includes(self, other: "Distribution") -> bool:
        """
        Checks if another datetime distribution is entirely within this distribution, considering the offset.
        """
        if self.low is None or self.high is None:
            return False
        if other.min() is None or other.max() is None:
            return False
        return (
            self.low - self.offset <= other.min()
            and other.max() <= self.high + self.offset
        )

    def as_constraint(self) -> Constraints:
        """
        Converts the distribution to a set of constraints.
        """
        return Constraints(
            rules=[
                (self.name, "le", self.high),
                (self.name, "ge", self.low),
                (self.name, "dtype", "datetime"),
            ]
        )

    def min(self) -> Optional[datetime]:
        """
        Returns the minimum datetime value of the distribution.
        """
        return self.low

    def max(self) -> Optional[datetime]:
        """
        Returns the maximum datetime value of the distribution.
        """
        return self.high

    def dtype(self) -> str:
        """
        Returns the data type of the distribution.
        """
        return "datetime"


class PassThroughDistribution(Distribution):
    """
    .. inheritance-diagram:: synthcity.plugins.core.distribution.PassThroughDistribution
        :parts: 1
    """

    data: pd.Series
    _dtype: str = PrivateAttr("")

    def setup_distribution(self) -> None:
        if self.data is None:
            raise ValueError("'data' must be provided for PassThroughDistribution.")

        # No additional attributes to set up since 'data' is used directly
        # Optionally, store the data type for dtype method
        self._dtype = str(self.data.dtype)

    def sample(self, count: int = 1) -> Any:
        msamples = self.sample_marginal(count)
        if msamples is not None:
            return msamples
        return self.data.sample(
            n=count, replace=True, random_state=self.random_state
        ).values

    def as_constraint(self) -> Constraints:
        # No constraints needed for pass-through columns
        return Constraints(rules=[])

    def get(self) -> List[Any]:
        # Return the unique values or any relevant info
        return [self.name]

    def has(self, val: Any) -> bool:
        # Check if the value exists in the data
        return val in self.data.values

    def includes(self, other: "Distribution") -> bool:
        # Since we are passing through values, we can define includes as checking if all values in other are in self.data
        if isinstance(other, PassThroughDistribution):
            return set(other.data.unique()).issubset(set(self.data.unique()))
        else:
            return False

    def min(self) -> Any:
        return self.data.min()

    def max(self) -> Any:
        return self.data.max()

    def dtype(self) -> str:
        return str(self.data.dtype)


def constraint_to_distribution(constraints: Constraints, feature: str) -> Distribution:
    """Infer Distribution from Constraints.

    Args:
        constraints: Constraints
            The Constraints on features.
        feature: str
            The name of the feature in question.

    Returns:
        The inferred Distribution.
    """
    dist_name, dist_args = constraints.feature_params(feature)

    if dist_name == "categorical":
        dist_template = CategoricalDistribution
    elif dist_name == "integer":
        dist_template = IntegerDistribution
    elif dist_name == "datetime":
        dist_template = DatetimeDistribution
    else:
        dist_template = FloatDistribution

    return dist_template(name=feature, **dist_args)


src/synthcity/plugins/core/dataset.py
# stdlib
from typing import List, Optional, Tuple

# third party
import numpy as np
import torch

# synthcity absolute
from synthcity.utils.constants import DEVICE


class FlexibleDataset(torch.utils.data.Dataset):
    """Helper dataset wrapper for post-processing or transforming another dataset. Used for controlling the image sizes for the synthcity models.

    The class supports adding custom transforms to existing datasets, and to subsample a set of indices.

    Args:
        data: torch.Dataset
        transform: An optional list of transforms
        indices: An optional list of indices to subsample
    """

    def __init__(
        self,
        data: torch.utils.data.Dataset,
        transform: Optional[torch.nn.Module] = None,
        indices: Optional[list] = None,
    ) -> None:
        super().__init__()

        if indices is None:
            indices = np.arange(len(data))

        self.indices = np.asarray(indices)
        self.data = data
        self.transform = transform
        self.ndarrays: Optional[Tuple[torch.Tensor, torch.Tensor]] = None

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:
        x, y = self.data[self.indices[index]]
        if self.transform:
            x = self.transform(x)
        return x, y

    def __len__(self) -> int:
        return len(self.indices)

    def shape(self) -> Tuple:
        x, _ = self[self.indices[0]]

        return (len(self), *x.shape)

    def numpy(self) -> Tuple[np.ndarray, np.ndarray]:
        if self.ndarrays is not None:
            return self.ndarrays

        x_buff = []
        y_buff = []
        for idx in range(len(self)):
            x_local, y_local = self[idx]
            x_buff.append(x_local.unsqueeze(0).cpu().numpy())
            y_buff.append(y_local)

        x = np.concatenate(x_buff, axis=0)
        y = np.asarray(y_buff)

        self.ndarrays = (x, y)
        return x, y

    def tensors(self) -> Tuple[torch.Tensor, torch.Tensor]:
        x, y = self.numpy()

        return torch.from_numpy(x), torch.from_numpy(y)

    def labels(self) -> np.ndarray:
        labels = []
        for idx in self.indices:
            _, y = self.data[idx]
            labels.append(y)

        return np.asarray(labels)

    def filter_indices(self, indices: List[int]) -> "FlexibleDataset":
        for idx in indices:
            if idx >= len(self.indices):
                raise ValueError(
                    "Invalid filtering list. {idx} not found in the current list of indices"
                )
        return FlexibleDataset(
            data=self.data, transform=self.transform, indices=self.indices[indices]
        )


class TensorDataset(torch.utils.data.Dataset):
    """Helper dataset for wrapping existing tensors

    Args:
        images: Tensor
        targets: Tensor
    """

    def __init__(
        self,
        images: torch.Tensor,
        targets: Optional[torch.Tensor],
    ) -> None:
        super().__init__()

        if targets is not None and len(targets) != len(images):
            raise ValueError("Invalid input")

        self.images = images
        self.targets = targets

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        y: Optional[torch.Tensor] = None
        x = self.images[index]

        if self.targets is not None:
            y = self.targets[index]

        return x, y

    def __len__(self) -> int:
        return len(self.images)

    def labels(self) -> Optional[np.ndarray]:
        if self.targets is None:
            return None

        return self.targets.cpu().numpy()


class ConditionalDataset(torch.utils.data.Dataset):
    """Helper dataset for wrapping existing datasets with custom tensors

    Args:
        data: torch.Dataset
        cond: Optional Tensor
    """

    def __init__(
        self,
        data: torch.utils.data.Dataset,
        cond: Optional[torch.Tensor] = None,
    ) -> None:
        super().__init__()

        if cond is not None and len(cond) != len(data):
            raise ValueError("Invalid input")

        self.data = data
        self.cond = cond

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        cond: Optional[torch.Tensor] = None
        x = self.data[index][0]

        if self.cond is not None:
            cond = self.cond[index]

        return x, cond

    def __len__(self) -> int:
        return len(self.data)


class NumpyDataset(torch.utils.data.Dataset):
    """Helper class for wrapping Numpy arrays in torch Datasets
    Args:
        X: np.ndarray
        y: np.ndarray
    """

    def __init__(self, X: np.ndarray, y: np.ndarray) -> None:
        super().__init__()

        self.X = X
        self.y = y

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:
        x = self.X[index]
        y = self.y[index]

        return torch.from_numpy(x).to(DEVICE), y

    def __len__(self) -> int:
        return len(self.X)


src/synthcity/plugins/core/models/syn_seq/syn_seq_encoder.py
from typing import Optional, Dict, List, Any
import pandas as pd
import numpy as np
from sklearn.base import TransformerMixin, BaseEstimator

class Syn_SeqEncoder(TransformerMixin, BaseEstimator):
    """
    A minimal version of the syn_seq encoder that:
      - Maintains col_map with { original_dtype, converted_type, method } for each column
      - For date columns, does day-offset numeric transformation => converted_type="numeric"
      - Splits numeric columns into col + col_cat, with col_cat placed before the original col in syn_order
      - Expands variable_selection_ accordingly
      - inverse_transform can restore date columns from day-offset,
        and attempt to restore original_dtype if possible
    """

    def __init__(
        self,
        columns_special_values: Optional[Dict[str, List]] = None,
        syn_order: Optional[List[str]] = None,
        max_categories: int = 20,
        col_type: Optional[Dict[str, str]] = None,
        default_method: str = "cart",
    ) -> None:
        """
        Args:
            columns_special_values: { colName : [specialVals...] } ex) {"age":[999], "bp":[-0.04]}
            syn_order: column order to follow
            max_categories: threshold for deciding numeric vs category if not declared
            col_type: user overrides => { "birthdate":"date","sex":"category","bp":"numeric"... }
            default_method: default method name for newly recognized columns
        """
        self.columns_special_values = columns_special_values or {}
        self.syn_order = syn_order or []
        self.max_categories = max_categories
        self.col_type = (col_type or {}).copy()  # user override
        self.default_method = default_method

        # col_map: each col -> { "original_dtype", "converted_type", "method" }
        # converted_type âˆˆ {"numeric","category"}
        self.col_map: Dict[str, Dict[str, Any]] = {}

        # date minimums for offset
        self.date_mins: Dict[str, pd.Timestamp] = {}

        # variable_selection matrix
        self.variable_selection_: Optional[pd.DataFrame] = None

    # ----------------------------------------------------------------
    # Fit
    # ----------------------------------------------------------------
    def fit(self, X: pd.DataFrame, y=None) -> "Syn_SeqEncoder":
        X = X.copy()
        # 1) Decide syn_order (if not given)
        self._detect_syn_order(X)
        # 2) init col_map
        self._init_col_map(X)
        # 3) detect major special values
        self._detect_special_values(X)
        # 4) build default variable_selection
        self._build_variable_selection(X)
        # 5) store date mins
        self._store_date_min(X)
        return self

    # ----------------------------------------------------------------
    # Transform
    # ----------------------------------------------------------------
    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()

        # 1) reorder columns
        X = self._reorder_columns(X)
        # 2) date -> day offset => converted_type="numeric"
        X = self._convert_date_to_offset(X)
        # 3) numeric -> split col_cat, place col_cat before original col
        X = self._split_numeric_cols_in_front(X)
        # 4) apply final dtype => numeric or category
        X = self._apply_converted_dtype(X)
        # 5) expand variable_selection if splitted col
        self._update_variable_selection_after_split(X)
        return X

    # ----------------------------------------------------------------
    # inverse_transform
    # ----------------------------------------------------------------
    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()

        # 1) single special => restore
        for col, specs in self.columns_special_values.items():
            if col in X.columns and len(specs)==1:
                X[col] = X[col].replace(pd.NA, specs[0])

        # 2) offset -> date
        X = self._convert_offset_to_date(X)

        # 3) restore original_dtype if possible
        for col in X.columns:
            if col not in self.col_map:
                # splitted col?
                continue
            orig_dt = self.col_map[col]["original_dtype"]
            if orig_dt is not None:
                try:
                    X[col] = X[col].astype(orig_dt)
                except:
                    pass
        return X

    # ----------------------------------------------------------------
    # set_method / get_method : if aggregator or user wants to override
    # ----------------------------------------------------------------
    def set_method(self, col: str, method: str) -> None:
        """Override method for a column."""
        if col not in self.col_map:
            print(f"[WARNING] col_map has no '{col}'")
            return
        self.col_map[col]["method"] = method

    def get_method_map(self) -> Dict[str, str]:
        """Return { col : method } for all columns in col_map."""
        return { c: info["method"] for c, info in self.col_map.items() }

    # =================================================================
    # Internals
    # =================================================================
    def _detect_syn_order(self, X: pd.DataFrame):
        if not self.syn_order:
            self.syn_order = list(X.columns)
        else:
            self.syn_order = [c for c in self.syn_order if c in X.columns]

    def _init_col_map(self, X: pd.DataFrame):
        """
        For each col in syn_order, decide if numeric or category or date => eventually store converted_type
         - date => will become numeric in transform, but we'll keep track for inverse
        """
        self.col_map.clear()

        for col in self.syn_order:
            if col not in X.columns:
                continue
            orig_dt = str(X[col].dtype)

            # Decide if date => (converted_type="numeric"), else numeric/category
            declared = self.col_type.get(col, "").lower()
            if declared == "date":
                # We'll treat as numeric, but keep date info for inverse
                conv_type = "numeric"
            elif declared == "numeric":
                conv_type = "numeric"
            elif declared == "category":
                conv_type = "category"
            else:
                # fallback auto
                if pd.api.types.is_datetime64_any_dtype(X[col]):
                    conv_type = "numeric"  # internally day-offset
                    self.col_type[col] = "date"  # mark for inverse
                else:
                    nuniq = X[col].nunique()
                    if nuniq > self.max_categories:
                        conv_type = "numeric"
                    else:
                        conv_type = "category"

            # save to col_map
            self.col_map[col] = {
                "original_dtype": orig_dt,
                "converted_type": conv_type,
                "method": self.default_method
            }

    def _detect_special_values(self, X: pd.DataFrame):
        for col in self.syn_order:
            if col not in X.columns:
                continue
            freq = X[col].value_counts(dropna=False, normalize=True)
            big_ones = freq[freq>0.9].index.tolist()
            if big_ones:
                exist = self.columns_special_values.get(col, [])
                merged = set(exist).union(big_ones)
                self.columns_special_values[col] = list(merged)

    def _build_variable_selection(self, X: pd.DataFrame):
        idx = self.syn_order
        vs = pd.DataFrame(0, index=idx, columns=idx)
        for i in range(len(idx)):
            for j in range(i):
                vs.iat[i,j] = 1
        self.variable_selection_ = vs

    def _store_date_min(self, X: pd.DataFrame):
        """For date columns => store min date for offset calc"""
        for col in self.syn_order:
            # if user said date or we auto-detected => self.col_type[col]=="date"
            # but in col_map => "converted_type":"numeric"
            # we can check self.col_type if "date"
            if self.col_type.get(col) == "date":
                dt_series = pd.to_datetime(X[col], errors="coerce")
                self.date_mins[col] = dt_series.min()

    def _reorder_columns(self, X: pd.DataFrame) -> pd.DataFrame:
        keep = [c for c in self.syn_order if c in X.columns]
        return X[keep]

    def _convert_date_to_offset(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        If col_type[col]=="date", then X[col] => day offset vs min
        col_map[col]["converted_type"] stays "numeric"
        """
        for col in self.syn_order:
            if self.col_type.get(col) == "date" and col in X.columns:
                mindt = self.date_mins.get(col, None)
                X[col] = pd.to_datetime(X[col], errors="coerce")
                if mindt is None:
                    mindt = X[col].min()
                    self.date_mins[col] = mindt
                X[col] = (X[col] - mindt).dt.days
        return X

    def _split_numeric_cols_in_front(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        For each col whose converted_type is "numeric", create col_cat.
        Insert col_cat before col in syn_order, add to col_map
        """
        for col in list(self.syn_order):
            if col not in self.col_map:
                continue
            if self.col_map[col]["converted_type"] != "numeric":
                continue
            if col not in X.columns:
                continue

            cat_col = col + "_cat"
            specials = self.columns_special_values.get(col, [])

            X[cat_col] = X[col].apply(
                lambda v: v if (pd.isna(v) or v in specials) else -777
            )
            X[cat_col] = X[cat_col].fillna(-9999)
            X[col] = X[col].apply(
                lambda v: v if (not pd.isna(v) and v not in specials) else pd.NA
            )
            X[cat_col] = X[cat_col].astype("category")

            # insert cat_col in syn_order
            if cat_col not in self.syn_order:
                idx = self.syn_order.index(col)
                self.syn_order.insert(idx, cat_col)

            # add to col_map
            self.col_map[cat_col] = {
                "original_dtype": None,
                "converted_type": "category",
                "method": self.default_method
            }

        return X

    def _apply_converted_dtype(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        If converted_type=="numeric" => to_numeric
        else "category" => astype("category")
        """
        for col in self.syn_order:
            if col not in X.columns:
                continue
            cinfo = self.col_map.get(col, {})
            ctype = cinfo.get("converted_type","category")
            if ctype=="numeric":
                X[col] = pd.to_numeric(X[col], errors="coerce")
            else:
                X[col] = X[col].astype("category")
        return X

    def _convert_offset_to_date(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        If self.col_type[col]=="date", => offset => date
        """
        for col in self.syn_order:
            if self.col_type.get(col)=="date" and col in X.columns:
                offset = pd.to_numeric(X[col], errors="coerce")
                mindt = self.date_mins.get(col, None)
                if mindt is not None:
                    X[col] = pd.to_timedelta(offset, unit="D") + mindt
        return X

    def _update_variable_selection_after_split(self, X: pd.DataFrame):
        if self.variable_selection_ is None:
            return
        old_vs = self.variable_selection_
        old_idx = list(old_vs.index)
        old_cols = list(old_vs.columns)
        new_cols = [c for c in X.columns if c not in old_idx]
        if not new_cols:
            return

        vs_new = pd.DataFrame(0, index=old_idx+new_cols, columns=old_cols+new_cols)
        for r in old_idx:
            for c in old_cols:
                vs_new.at[r,c] = old_vs.at[r,c]

        for c_new in new_cols:
            if c_new.endswith("_cat"):
                c_base = c_new[:-4]
                # copy row/col from base
                if c_base in vs_new.index and c_base in vs_new.columns:
                    for c2 in old_cols:
                        vs_new.at[c_new, c2] = vs_new.at[c_base, c2]
                    for r2 in old_idx:
                        vs_new.at[r2, c_new] = vs_new.at[r2, c_base]
                vs_new.at[c_new, c_new] = 0

        self.variable_selection_ = vs_new

    # ------------------ update_variable_selection
    @staticmethod
    def update_variable_selection(
        var_sel_df: pd.DataFrame,
        user_dict: Dict[str, List[str]]
    ) -> pd.DataFrame:
        """
        For row=target, col= predictor => set 1
        """
        for tgt, preds in user_dict.items():
            if tgt not in var_sel_df.index:
                print(f"[WARNING] {tgt} not in var_sel_df => skip")
                continue
            var_sel_df.loc[tgt, :] = 0
            for p in preds:
                if p in var_sel_df.columns:
                    var_sel_df.loc[tgt, p] = 1
                else:
                    print(f"[WARNING] predictor {p} not in columns => skip")
        return var_sel_df


src/synthcity/plugins/core/models/syn_seq/syn_seq.py
# File: syn_seq.py

from typing import Any, Dict, List, Optional, Tuple, Union
import pandas as pd
import numpy as np

# If your constraints class is in a separate file, e.g., syn_seq_constraints.py:
# from synthcity.plugins.core.models.syn_seq.syn_seq_constraints import Syn_SeqConstraints
# Otherwise import from the main constraints location
from synthcity.plugins.core.constraints import Constraints

# The column-by-column methods (you may have them in separate files)
from synthcity.plugins.core.models.syn_seq.methods.cart import syn_cart
from synthcity.plugins.core.models.syn_seq.methods.ctree import syn_ctree
from synthcity.plugins.core.models.syn_seq.methods.logreg import syn_logreg
from synthcity.plugins.core.models.syn_seq.methods.norm import syn_norm, syn_lognorm
from synthcity.plugins.core.models.syn_seq.methods.pmm import syn_pmm
from synthcity.plugins.core.models.syn_seq.methods.polyreg import syn_polyreg
from synthcity.plugins.core.models.syn_seq.methods.rf import syn_rf
from synthcity.plugins.core.models.syn_seq.methods.misc import syn_random, syn_swr

# Our custom Syn_SeqEncoder also might be in another file. Import it:
# from synthcity.plugins.core.models.syn_seq.syn_seq_encoder import Syn_SeqEncoder
# For brevity, we assume your environment has it.

###############################################
# Define which methods are allowed for numeric vs category
###############################################
ALLOWED_NUMERIC_METHODS = {
    "norm",
    "lognorm",
    "pmm",
    "cart",
    "ctree",
    "rf",
    "random",
    "swr",
    "polyreg",  # optional for numeric if e.g. strictly binary
    "logreg"    # also optional if numeric is strictly 0/1
}

ALLOWED_CATEGORY_METHODS = {
    "cart",
    "ctree",
    "rf",
    "logreg",
    "polyreg",
    "random",
    "swr",
}


class Syn_Seq:
    """
    A sequential-synthesis aggregator that:

      1) Optionally merges user_custom => syn_order, method, variable_selection
      2) Calls loader.encode() => returns an encoded loader (which may introduce _cat columns).
      3) Partial-fit each splitted column in an internal hidden order: _cat columns first => forced "cart"
         Then the main column => user-chosen or fallback. If declared numeric => fallback to an allowed numeric method.
         If declared category => fallback to an allowed category method.
      4) generate(...) => synthesizes col-by-col in that hidden order, optionally applying constraints.
      5) decode => final result is a DataFrame with only the original columns (no _cat). 
         - We forcibly cast columns that were originally numeric back to numeric at this step.
    """

    def __init__(
        self,
        random_state: int = 0,
        strict: bool = True,
        sampling_patience: int = 100,
        default_first_method: str = "swr",
        default_other_method: str = "cart",
        seq_id_col: str = "seq_id",
        seq_time_col: str = "seq_time_id",
    ):
        """
        Args:
            random_state: Reproducibility
            strict: if True => repeated attempts to meet constraints
            sampling_patience: max tries if strict
            default_first_method: fallback for the first column if user didn't specify
            default_other_method: fallback for subsequent columns if user didn't specify
            seq_id_col, seq_time_col: for sequence-based constraints if needed
        """
        self.random_state = random_state
        self.strict = strict
        self.sampling_patience = sampling_patience
        self.default_first_method = default_first_method
        self.default_other_method = default_other_method
        self.seq_id_col = seq_id_col
        self.seq_time_col = seq_time_col

        # Store partial fits: splitted_col => {method, predictors, fit_info}
        self._col_models: Dict[str, Dict[str, Any]] = {}
        self._model_trained = False

        # We'll store enc_dict from loader.encode(...) so we can decode after generation
        self._enc_dict: Dict[str, Any] = {}

        # Original col_type (so we know which columns must be forcibly numeric after decode)
        self._original_col_type: Dict[str, str] = {}

    def fit(
        self,
        loader: Any,  # typically a Syn_SeqDataLoader
        user_custom: Optional[dict] = None,
        *args,
        **kwargs
    ) -> "Syn_Seq":
        """
        1) Merge user_custom => loader.update_user_custom(...)
        2) loader.encode(...) => splitted columns => new df with _cat columns => encoded_loader
        3) partial-fit each splitted column => store partial fits
        4) track col_type so that at the end of generation, we can cast numeric columns back to numeric
        """
        # 1) user overrides
        if user_custom:
            loader.update_user_custom(user_custom)

        # 2) encode => splitted columns
        encoded_loader, enc_dict = loader.encode(encoders=None)
        self._enc_dict = enc_dict

        # Record the original col_type mapping from the loader
        if hasattr(loader, "col_type"):
            self._original_col_type = loader.col_type.copy()

        df_encoded = encoded_loader.dataframe()
        if df_encoded.empty:
            raise ValueError("No data after encoding => cannot train on empty DataFrame.")

        syn_order = encoded_loader.syn_order           
        method_dict = getattr(encoded_loader, "_method", {})
        varsel_df = getattr(encoded_loader._encoder, "variable_selection_", None)
        if varsel_df is None:
            varsel_df = pd.DataFrame(0, index=syn_order, columns=syn_order)

        # fallback if user didn't specify a method => default
        final_method: Dict[str, str] = {}
        for i, col_name in enumerate(syn_order):
            if col_name not in method_dict:
                final_method[col_name] = (
                    self.default_first_method if i == 0 else self.default_other_method
                )
            else:
                final_method[col_name] = method_dict[col_name]

        # read col_type from encoded_loader
        col_type_map = getattr(encoded_loader, "col_type", {})

        # splitted_map => e.g. "bp" => ["bp_cat","bp"]
        splitted_map: Dict[str, List[str]] = {c: [] for c in syn_order}
        for c_enc in df_encoded.columns:
            base_c = c_enc[:-4] if c_enc.endswith("_cat") else c_enc
            splitted_map.setdefault(base_c, []).append(c_enc)

        # build final "fit_order" => cat first => forced 'cart', then main col
        fit_order: List[str] = []
        method_map: Dict[str, str] = {}

        for i, base_col in enumerate(syn_order):
            sub_cols = splitted_map.get(base_col, [])
            cat_cols = [x for x in sub_cols if x.endswith("_cat")]
            main_cols = [x for x in sub_cols if not x.endswith("_cat")]

            # (A) cat columns => forced "cart"
            for sc in cat_cols:
                fit_order.append(sc)
                method_map[sc] = "cart"  # forced

            # (B) main col => check col_type => fix method if mismatch
            chosen_m = final_method.get(base_col, self.default_other_method)
            declared_t = col_type_map.get(base_col, "category")  # fallback

            cfix = chosen_m.strip().lower()
            if declared_t.lower() == "numeric":
                if cfix not in ALLOWED_NUMERIC_METHODS:
                    fallback_m = self.default_first_method if i == 0 else "norm"
                    print(f"[TYPE-CHECK] '{base_col}' is numeric but method '{cfix}' invalid => fallback '{fallback_m}'")
                    cfix = fallback_m
            else:
                # category => ensure cfix in ALLOWED_CATEGORY_METHODS
                if cfix not in ALLOWED_CATEGORY_METHODS:
                    print(f"[TYPE-CHECK] '{base_col}' is category but method '{cfix}' invalid => fallback 'cart'")
                    cfix = "cart"

            for sc in main_cols:
                fit_order.append(sc)
                method_map[sc] = cfix

        # leftover columns not in syn_order => fallback
        leftover = [c for c in df_encoded.columns if c not in fit_order]
        for c in leftover:
            fit_order.append(c)
            method_map[c] = self.default_other_method

        # Logging
        print("[INFO] final synthesis:")
        print(f"  - syn_order: {syn_order}")
        print(f"  - method: {final_method}")
        print("  - variable_selection_:")
        print(varsel_df)
        print("\n[INFO] model fitting")

        self._col_models.clear()
        for col_enc in fit_order:
            base_c = col_enc[:-4] if col_enc.endswith("_cat") else col_enc
            chosen_m = method_map[col_enc]

            # gather predictors from varsel_df
            preds_list = []
            if base_c in varsel_df.index:
                row_mask = varsel_df.loc[base_c] == 1
                preds_list = varsel_df.columns[row_mask].tolist()
                # remove base conflicts if needed
                preds_list = [
                    p for p in preds_list
                    if not (p.endswith("_cat") and p[:-4] not in preds_list)
                ]

            y = df_encoded[col_enc].values
            X = df_encoded[preds_list].values if preds_list else np.zeros((len(y), 0))

            print(f"Fitting '{col_enc}' ... ", end="")
            fit_info = self._fit_single_column(y, X, chosen_m)
            self._col_models[col_enc] = {
                "method": chosen_m,
                "predictors": preds_list,
                "fit_info": fit_info,
            }
            print("Done!")

        self._model_trained = True
        return self

    def _fit_single_column(self, y: np.ndarray, X: np.ndarray, method_name: str) -> Dict[str, Any]:
        """
        Store partial info for each col => { "type":..., "obs_y":..., "obs_X":... }
        used at generation time.
        """
        m = method_name.strip().lower()
        if m in {
            "cart", "ctree", "rf", "norm", "lognorm", "pmm", "logreg", "polyreg"
        }:
            return {"type": m, "obs_y": y, "obs_X": X}
        elif m == "swr":
            return {"type": "swr", "obs_y": y}
        elif m == "random":
            return {"type": "random", "obs_y": y}
        else:
            # fallback => random
            return {"type": "random", "obs_y": y}

    def generate(
        self,
        nrows: int,
        constraints: Optional[Constraints] = None,
        encoded_loader: Optional[Any] = None,
        **kwargs
    ) -> "Syn_SeqGeneratorResult":
        """
        Main entry point for synthesis:
          - nrows: how many to generate
          - constraints: optional constraints
          - encoded_loader: pass the same loader we used for fit (or a new one)
        """
        if not self._model_trained:
            raise RuntimeError("Must fit Syn_Seq before generating data.")

        if encoded_loader is None:
            raise ValueError("generate requires an encoded_loader for final decode")

        # We can combine constraints if needed (not shown here)
        syn_constraints = constraints

        # Generate data
        if not self.strict:
            # Single pass => no repeated attempts
            result_df = self._generate_once(nrows)
        else:
            # Repeated tries => gather enough rows passing constraints
            result_df = self._attempt_strict_generation(nrows, syn_constraints)

        # Wrap in a holder so user can chain .dataframe() or .decode()
        return Syn_SeqGeneratorResult(
            aggregator=self,
            encoded_df=result_df,
            encoded_loader=encoded_loader,
            original_col_type=self._original_col_type
        )

    def _attempt_strict_generation(self, count: int, constraints: Optional[Constraints]) -> pd.DataFrame:
        """
        repeated tries => gather enough rows that pass constraints
        """
        result_df = pd.DataFrame()
        tries = 0

        while len(result_df) < count and tries < self.sampling_patience:
            tries += 1
            chunk = self._generate_once(count)
            if constraints is not None:
                chunk = constraints.match(chunk)
            chunk = chunk.drop_duplicates()
            result_df = pd.concat([result_df, chunk], ignore_index=True)

        return result_df.head(count)

    def _generate_once(self, count: int) -> pd.DataFrame:
        """
        Single pass => produce splitted columns in fit_order
        """
        fit_order = list(self._col_models.keys())  # splitted columns
        syn_df = pd.DataFrame(index=range(count))

        for col_enc in fit_order:
            info = self._col_models[col_enc]
            method_name = info["method"]
            preds = info["predictors"]
            fit_data = info["fit_info"]

            Xp = syn_df[preds].values if preds else np.zeros((count, 0))
            new_vals = self._generate_single_column(method_name, fit_data, Xp, count)
            syn_df[col_enc] = new_vals

        return syn_df

    def _generate_single_column(
        self,
        method: str,
        fit_info: Dict[str, Any],
        Xp: np.ndarray,
        count: int
    ) -> pd.Series:
        """Dispatch to the specialized column-synthesis function."""
        m = method.strip().lower()
        y_obs = fit_info.get("obs_y")
        X_obs = fit_info.get("obs_X")

        if m == "cart":
            result = syn_cart(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(result["res"])
        elif m == "ctree":
            result = syn_ctree(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(result["res"])
        elif m == "rf":
            result = syn_rf(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(result["res"])
        elif m == "norm":
            result = syn_norm(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(result["res"])
        elif m == "lognorm":
            result = syn_lognorm(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(result["res"])
        elif m == "pmm":
            result = syn_pmm(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(result["res"])
        elif m == "logreg":
            result = syn_logreg(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(result["res"])
        elif m == "polyreg":
            result = syn_polyreg(y=y_obs, X=X_obs, Xp=Xp, random_state=self.random_state)
            return pd.Series(result["res"])
        elif m == "swr":
            result = syn_swr(y=y_obs, X=None, Xp=np.zeros((count, 1)), random_state=self.random_state)
            return pd.Series(result["res"])
        elif m == "random":
            result = syn_random(y=y_obs, X=None, Xp=np.zeros((count,1)), random_state=self.random_state)
            return pd.Series(result["res"])
        else:
            # fallback => random
            fallback = syn_random(y=y_obs, X=None, Xp=np.zeros((count,1)), random_state=self.random_state)
            return pd.Series(fallback["res"])


class Syn_SeqGeneratorResult:
    """
    Simple container for the aggregator output DataFrame + references to decode
    and forcibly cast numeric columns that were originally numeric.
    """

    def __init__(
        self,
        aggregator: Syn_Seq,
        encoded_df: pd.DataFrame,
        encoded_loader: Any,
        original_col_type: Dict[str, str]
    ):
        self._aggregator = aggregator
        self._encoded_df = encoded_df
        self._encoded_loader = encoded_loader
        self._original_col_type = original_col_type

    def dataframe(self) -> pd.DataFrame:
        """
        Return the final synthetic DataFrame 
        => decode => remove _cat => forcibly cast numeric columns => return
        """
        # 1) Make a new loader with the encoded_df
        loader_tmp = self._encoded_loader.decorate(self._encoded_df)

        # 2) decode => remove _cat columns, restore original order
        loader_decoded = loader_tmp.decode(self._aggregator._enc_dict)

        # 3) forcibly cast columns that were originally numeric
        df_out = loader_decoded.dataframe().copy()
        for col, declared_type in self._original_col_type.items():
            if declared_type.lower() == "numeric" and col in df_out.columns:
                df_out[col] = pd.to_numeric(df_out[col], errors="coerce")

        return df_out



src/synthcity/plugins/core/models/syn_seq/syn_seq_constraints.py
# File: syn_seq_constraints.py

from typing import Any, List, Dict, Tuple, Union
import pandas as pd
import numpy as np

# We import the base Constraints to inherit from
from synthcity.plugins.core.constraints import Constraints

# 
# A single sub-rule is (feature, op, value).
# For "chained" logic, we might interpret:
#    "For rows that pass all sub-rules (1..k-1), apply sub-rule k as a filter or correction."
#
# Example constraint input:
#   {
#       "N1": [
#           ("C1", "in", ["AAA","BBB"]),
#           ("N1", ">", 125)
#       ]
#   }
# means:
#  1) If a row passes (C1 in [AAA,BBB]), 
#  2) Then also enforce (N1 > 125) on that row.
#
# If the first sub-rule is not satisfied, the second doesn't apply to that row.
# If the first sub-rule is satisfied but not the second => row fails entirely.
#

class Syn_SeqConstraints(Constraints):
    """
    An extension that supports "chained" constraints for sequential logic:
      - If sub-rule 1 is satisfied => we must also pass sub-rule 2, and so on.
      - 'chained_rules' can be a dict of { targetCol : [ (feature, op, val), (feature, op, val), ... ] }.
        Example:
          {
            "N1": [
                ("C1", "in", ["AAA","BBB"]),
                ("N1", ">", 125)
            ]
          }
        read as: "If (C1 in [AAA,BBB]) => enforce (N1 > 125)".
      
      Each sub-rule uses the same _eval logic for <, <=, >, >=, ==, in, etc.
      If a sub-rule fails => that entire row fails (filtered out) if using .match(), 
      or is corrected if possible (.correct).
    """

    def __init__(
        self,
        # You can still pass standard constraints as "rules",
        # or pass new "chained_rules" in dict format
        rules: List[Tuple[str, str, Any]] = None,
        chained_rules: Dict[str, List[Tuple[str, str, Any]]] = None,
        seq_id_feature: str = "seq_id",
        seq_time_id_feature: str = "seq_time_id",
        **kwargs: Any,
    ):
        # Let the base constructor handle standard 'rules'
        super().__init__(rules=rules if rules else [])
        self.seq_id_feature = seq_id_feature
        self.seq_time_id_feature = seq_time_id_feature

        # We'll store the "chained" sub-rules in a separate structure
        # keyed by "target column" or "some label"
        self.chained_rules = chained_rules if chained_rules else {}

    def match(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Overridden. We can do row-level or entire-sequence filtering. 
        If you still want entire-sequence filtering, you can define match_sequential() and call it below.
        For demonstration, let's do row-level filtering with chained sub-rules.
        """
        df_copy = X.copy()
        # first, apply base constraints (self.rules) at row-level:
        base_mask = super().filter(df_copy)
        df_filtered = df_copy[base_mask].copy()
        if df_filtered.empty:
            return df_filtered

        # next, apply "chained" constraints
        df_filtered = self._match_chained(df_filtered)
        return df_filtered

    def _match_chained(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        For each entry in self.chained_rules => interpret a *sequence* of sub-rules.
        If the row passes sub-rule[0], sub-rule[1], ... sub-rule[n-2], 
        then sub-rule[n-1] is also enforced. 
        If it fails at any step => the row is filtered out or "corrected" (depending on your logic).
        
        For simplicity below, we do "if sub-rule[i] is satisfied => proceed, else row is out."
        """
        df = X.copy()
        for target_key, rule_chain in self.chained_rules.items():
            # We interpret the chain in order
            # e.g. [("C1","in", [...]), ("N1",">",125)]
            # step i=0 => a filter => if row fails => out
            # step i=1 => a further filter => if row fails => out
            # etc.

            # We'll build a mask for these sub-rules
            keep_mask = pd.Series([True]*len(df), index=df.index)

            for (feature, op, operand) in rule_chain:
                cur_mask = self._eval(df, feature, op, operand)
                # only keep the rows that pass this sub-rule
                keep_mask = keep_mask & cur_mask

            # after we apply all sub-rules in the chain, 
            # rows that didn't pass => out
            df = df[keep_mask].copy()
            if df.empty:
                break

        return df

    def _eval(self, X: pd.DataFrame, feature: str, op: str, operand: Any) -> pd.Index:
        """
        If we want to also handle direct substitution '=' or '==' or to skip it, we can override _eval.
        If the user wants a different meaning for '=' in chain logic, we can do so.
        Otherwise, we rely on base Constraints._eval for <, <=, >, >=, ==, in, dtype, etc.
        """
        # If you want direct substitution for '=' or '==', do so in _correct or in some separate logic.
        if op in ["=", "=="]:
            # interpret as equality check
            return (X[feature] == operand) | X[feature].isna()
        else:
            # fallback to base method
            return super()._eval(X, feature, op, operand)

    def _correct(self, X: pd.DataFrame, feature: str, op: str, operand: Any) -> pd.DataFrame:
        """
        If user wants direct substitution for '=' => set that col's entire column to operand. 
        Or you can do row-level changes only for failing rows, etc.
        """
        if op in ["=", "=="]:
            X.loc[:, feature] = operand
            return X
        return super()._correct(X, feature, op, operand)

    # If you want entire-sequence logic, you'd define match_sequential below:

    # Example
    #     constraint = {
    #   "N1": [
    #     ["C1", "in", ["AAA","BBB"]],
    #     ["N1", ">", 125]
    #   ]
    # }

    # def match_sequential(self, X: pd.DataFrame) -> pd.DataFrame:
    #     """
    #     Example method that applies constraints at the group (sequence) level:
    #       - if a sub-rule fails for ANY row => remove entire sequence
    #       - or do direct substitution in entire sequence
    #     """
    #     df_copy = X.copy()
    #     base_mask = super().filter(df_copy)
    #     # group by seq_id
    #     grouped = df_copy.groupby(self.seq_id_feature)
    #
    #     keep_seq_ids = []
    #     for seq_id, group in grouped:
    #         # if all rows pass => keep entire sequence
    #         if base_mask[group.index].all():
    #             # next, check chained
    #             # for each sub-rule chain, we can test if group passes
    #             # if not => exclude entire seq
    #             # or we can do partial correction
    #             # ...
    #             keep_seq_ids.append(seq_id)
    #
    #     return df_copy[df_copy[self.seq_id_feature].isin(keep_seq_ids)]



src/synthcity/plugins/core/dataloader.py
# stdlib
import random
from abc import ABCMeta, abstractmethod
from typing import Any, Dict, List, Optional, Tuple, Union

# third party
import numpy as np
import numpy.ma as ma
import pandas as pd
import PIL
import torch
from pydantic import validate_arguments
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torchvision import transforms

# synthcity absolute
from synthcity.plugins.core.constraints import Constraints
from synthcity.plugins.core.dataset import FlexibleDataset, TensorDataset
from synthcity.plugins.core.models.feature_encoder import DatetimeEncoder
from synthcity.utils.compression import compress_dataset, decompress_dataset
from synthcity.utils.serialization import dataframe_hash

# Syn_Seq
from synthcity.plugins.core.models.syn_seq.syn_seq_encoder import Syn_SeqEncoder


class DataLoader(metaclass=ABCMeta):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.DataLoader
        :parts: 1

    Base class for all data loaders.

    Each derived class must implement the following methods:
        unpack() - a method that unpacks the columns and returns features and labels (X, y).
        decorate() - a method that creates a new instance of DataLoader by decorating the input data with the same DataLoader properties (e.g. sensitive features, target column, etc.)
        dataframe() - a method that returns the pandas dataframe that contains all features and samples
        numpy() - a method that returns the numpy array that contains all features and samples
        info() - a method that returns a dictionary of DataLoader information
        __len__() - a method that returns the number of samples in the DataLoader
        satisfies() - a method that tests if the current DataLoader satisfies the constraint provided
        match() - a method that returns a new DataLoader where the provided constraints are met
        from_info() - a static method that creates a DataLoader from the data and the information dictionary
        sample() - returns a new DataLoader that contains a random subset of N samples
        drop() - returns a new DataLoader with a list of columns dropped
        __getitem__() - getting features by names
        __setitem__() - setting features by names
        train() - returns a DataLoader containing the training set
        test() - returns a DataLoader containing the testing set
        fillna() - returns a DataLoader with NaN filled by the provided number(s)


    If any method implementation is missing, the class constructor will fail.

    Constructor Args:
        data_type: str
            The type of DataLoader, currently supports "generic", "time_series" and "survival".
        data: Any
            The object that contains the data
        static_features: List[str]
            List of feature names that are static features (as opposed to temporal features).
        temporal_features:
            List of feature names that are temporal features, i.e. observed over time.
        sensitive_features: List[str]
            Name of sensitive features.
        important_features: List[str]
            Default: None. Only relevant for SurvivalGAN method.
        outcome_features:
            The feature name that provides labels for downstream tasks.
    """

    def __init__(
        self,
        data_type: str,
        data: Any,
        static_features: List[str] = [],
        temporal_features: List[str] = [],
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        outcome_features: List[str] = [],
        train_size: float = 0.8,
        random_state: int = 0,
        **kwargs: Any,
    ) -> None:
        self.static_features = static_features
        self.temporal_features = temporal_features
        self.sensitive_features = sensitive_features
        self.important_features = important_features
        self.outcome_features = outcome_features
        self.random_state = random_state

        self.data = data
        self.data_type = data_type
        self.train_size = train_size

    def raw(self) -> Any:
        return self.data

    @abstractmethod
    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        ...

    @abstractmethod
    def decorate(self, data: Any) -> "DataLoader":
        ...

    def type(self) -> str:
        return self.data_type

    @property
    @abstractmethod
    def shape(self) -> tuple:
        ...

    @property
    @abstractmethod
    def columns(self) -> list:
        ...

    @abstractmethod
    def dataframe(self) -> pd.DataFrame:
        ...

    @abstractmethod
    def numpy(self) -> np.ndarray:
        ...

    @property
    def values(self) -> np.ndarray:
        return self.numpy()

    @abstractmethod
    def info(self) -> dict:
        ...

    @abstractmethod
    def __len__(self) -> int:
        ...

    @abstractmethod
    def satisfies(self, constraints: Constraints) -> bool:
        ...

    @abstractmethod
    def match(self, constraints: Constraints) -> "DataLoader":
        ...

    @staticmethod
    @abstractmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        ...

    @abstractmethod
    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        ...

    @abstractmethod
    def drop(self, columns: list = []) -> "DataLoader":
        ...

    @abstractmethod
    def __getitem__(self, feature: Union[str, list]) -> Any:
        ...

    @abstractmethod
    def __setitem__(self, feature: str, val: Any) -> None:
        ...

    @abstractmethod
    def train(self) -> "DataLoader":
        ...

    @abstractmethod
    def test(self) -> "DataLoader":
        ...

    def hash(self) -> str:
        return dataframe_hash(self.dataframe())

    def __repr__(self, *args: Any, **kwargs: Any) -> str:
        return self.dataframe().__repr__(*args, **kwargs)

    def _repr_html_(self, *args: Any, **kwargs: Any) -> Any:
        return self.dataframe()._repr_html_(*args, **kwargs)

    @abstractmethod
    def fillna(self, value: Any) -> "DataLoader":
        ...

    @abstractmethod
    def compression_protected_features(self) -> list:
        ...

    def domain(self) -> Optional[str]:
        return None

    def compress(
        self,
    ) -> Tuple["DataLoader", Dict]:
        to_compress = self.data.copy().drop(
            columns=self.compression_protected_features()
        )
        compressed, context = compress_dataset(to_compress)
        for protected_col in self.compression_protected_features():
            compressed[protected_col] = self.data[protected_col]

        return self.decorate(compressed), context

    def decompress(self, context: Dict) -> "DataLoader":
        decompressed = decompress_dataset(self.data, context)

        return self.decorate(decompressed)

    def encode(
        self,
        encoders: Optional[Dict[str, Any]] = None,
    ) -> Tuple["DataLoader", Dict]:
        encoded = self.dataframe().copy()
        if encoders is not None:
            for col in encoders:
                if col not in encoded.columns:
                    continue
                encoded[col] = encoders[col].transform(encoded[col])
        else:
            encoders = {}

            for col in encoded.columns:
                if (
                    encoded[col].infer_objects().dtype.kind == "i"
                    and encoded[col].min() == 0
                    and encoded[col].max() == len(encoded[col].unique()) - 1
                ):
                    continue

                if (
                    encoded[col].infer_objects().dtype.kind in ["O", "b"]
                    or len(encoded[col].unique()) < 15
                ):
                    encoder = LabelEncoder().fit(encoded[col])
                    encoded[col] = encoder.transform(encoded[col])
                    encoders[col] = encoder
                elif encoded[col].infer_objects().dtype.kind in ["M"]:
                    encoder = DatetimeEncoder().fit(encoded[col])
                    encoded[col] = encoder.transform(encoded[col]).values
                    encoders[col] = encoder
        return self.from_info(encoded, self.info()), encoders

    def decode(
        self,
        encoders: Dict[str, Any],
    ) -> "DataLoader":
        decoded = self.dataframe().copy()

        for col in encoders:
            if isinstance(encoders[col], LabelEncoder):
                decoded[col] = decoded[col].astype(int)
            else:
                decoded[col] = decoded[col].astype(float)

            decoded[col] = encoders[col].inverse_transform(decoded[col])

        return self.from_info(decoded, self.info())

    @abstractmethod
    def is_tabular(self) -> bool:
        ...

    @abstractmethod
    def get_fairness_column(self) -> Union[str, Any]:
        ...


class GenericDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.GenericDataLoader
        :parts: 1

    Data loader for generic tabular data.

    Constructor Args:
        data: Union[pd.DataFrame, list, np.ndarray]
            The dataset. Either a Pandas DataFrame or a Numpy Array.
        sensitive_features: List[str]
            Name of sensitive features.
        important_features: List[str]
            Default: None. Only relevant for SurvivalGAN method.
        target_column: Optional[str]
            The feature name that provides labels for downstream tasks.
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        domain_column: Optional[str]
            Optional domain label, used for domain adaptation algorithms.
        random_state: int
            Defaults to zero.

    Example:
        >>> from sklearn.datasets import load_diabetes
        >>> from synthcity.plugins.core.dataloader import GenericDataLoader
        >>> X, y = load_diabetes(return_X_y=True, as_frame=True)
        >>> X["target"] = y
        >>> # Important note: preprocessing data with OneHotEncoder or StandardScaler is not needed or recommended.
        >>> # Synthcity handles feature encoding and standardization internally.
        >>> loader = GenericDataLoader(X, target_column="target", sensitive_columns=["sex"],)
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        data: Union[pd.DataFrame, list, np.ndarray],
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        target_column: Optional[str] = None,
        fairness_column: Optional[str] = None,
        domain_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        if not isinstance(data, pd.DataFrame):
            data = pd.DataFrame(data)

        data.columns = data.columns.astype(str)
        if target_column is not None:
            self.target_column = target_column
        elif len(data.columns) > 0:
            self.target_column = data.columns[-1]
        else:
            self.target_column = "---"

        self.fairness_column = fairness_column
        self.domain_column = domain_column

        super().__init__(
            data_type="generic",
            data=data,
            static_features=list(data.columns),
            sensitive_features=sensitive_features,
            important_features=important_features,
            outcome_features=[self.target_column],
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data.shape

    def domain(self) -> Optional[str]:
        return self.domain_column

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    @property
    def columns(self) -> list:
        return list(self.data.columns)

    def compression_protected_features(self) -> list:
        out = [self.target_column]
        domain = self.domain()

        if domain is not None:
            out.append(domain)

        return out

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        X = self.data.drop(columns=[self.target_column])
        y = self.data[self.target_column]

        if as_numpy:
            return np.asarray(X), np.asarray(y)
        return X, y

    def dataframe(self) -> pd.DataFrame:
        return self.data

    def numpy(self) -> np.ndarray:
        return self.dataframe().values

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "static_features": self.static_features,
            "sensitive_features": self.sensitive_features,
            "important_features": self.important_features,
            "outcome_features": self.outcome_features,
            "target_column": self.target_column,
            "fairness_column": self.fairness_column,
            "domain_column": self.domain_column,
            "train_size": self.train_size,
        }

    def __len__(self) -> int:
        return len(self.data)

    def decorate(self, data: Any) -> "DataLoader":
        return GenericDataLoader(
            data,
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            target_column=self.target_column,
            random_state=self.random_state,
            train_size=self.train_size,
            fairness_column=self.fairness_column,
            domain_column=self.domain_column,
        )

    def satisfies(self, constraints: Constraints) -> bool:
        return constraints.is_valid(self.data)

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.decorate(constraints.match(self.data))

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        return self.decorate(self.data.sample(count, random_state=random_state))

    def drop(self, columns: list = []) -> "DataLoader":
        return self.decorate(self.data.drop(columns=columns))

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "GenericDataLoader":
        if not isinstance(data, pd.DataFrame):
            raise ValueError(f"Invalid data type {type(data)}")

        return GenericDataLoader(
            data,
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            target_column=info["target_column"],
            fairness_column=info["fairness_column"],
            domain_column=info["domain_column"],
            train_size=info["train_size"],
        )

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self.data[feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self.data[feature] = val

    def _train_test_split(self) -> Tuple:
        stratify = None
        if self.target_column in self.data:
            target = self.data[self.target_column]
            if target.value_counts().min() > 1:
                stratify = target

        return train_test_split(
            self.data,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )

    def train(self) -> "DataLoader":
        train_data, _ = self._train_test_split()
        return self.decorate(train_data.reset_index(drop=True))

    def test(self) -> "DataLoader":
        _, test_data = self._train_test_split()
        return self.decorate(test_data.reset_index(drop=True))

    def fillna(self, value: Any) -> "DataLoader":
        self.data = self.data.fillna(value)
        return self

    def is_tabular(self) -> bool:
        return True


class SurvivalAnalysisDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.SurvivalAnalysisDataLoader
        :parts: 1

    Data Loader for Survival Analysis Data

    Constructor Args:
        data: Union[pd.DataFrame, list, np.ndarray]
            The dataset. Either a Pandas DataFrame or a Numpy Array.
        time_to_event_column: str
            Survival Analysis specific time-to-event feature
        target_column: str
            The outcome: event or censoring.
        sensitive_features: List[str]
            Name of sensitive features.
        important_features: List[str]
            Default: None. Only relevant for SurvivalGAN method.
        target_column: str
            The feature name that provides labels for downstream tasks.
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        domain_column: Optional[str]
            Optional domain label, used for domain adaptation algorithms.
        random_state: int
            Defaults to zero.
        train_size: float
            The ratio to use for train splits.

    Example:
        >>> TODO
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        data: pd.DataFrame,
        time_to_event_column: str,
        target_column: str,
        time_horizons: list = [],
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        fairness_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        if target_column not in data.columns:
            raise ValueError(f"Event column {target_column} not found in the dataframe")

        if time_to_event_column not in data.columns:
            raise ValueError(
                f"Time to event column {time_to_event_column} not found in the dataframe"
            )

        T = data[time_to_event_column]
        data_filtered = data[T > 0]
        row_diff = data.shape[0] - data_filtered.shape[0]
        if row_diff > 0:
            raise ValueError(
                f"The time_to_event_column contains {row_diff} values less than or equal to zero. Please remove them."
            )

        if len(time_horizons) == 0:
            time_horizons = np.linspace(T.min(), T.max(), num=5)[1:-1].tolist()

        data.columns = data.columns.astype(str)

        self.target_column = target_column
        self.time_to_event_column = time_to_event_column
        self.time_horizons = time_horizons
        self.fairness_column = fairness_column

        super().__init__(
            data_type="survival_analysis",
            data=data,
            static_features=list(data.columns.astype(str)),
            sensitive_features=sensitive_features,
            important_features=important_features,
            outcome_features=[self.target_column],
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data.shape

    @property
    def columns(self) -> list:
        return list(self.data.columns)

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    def compression_protected_features(self) -> list:
        out = [self.target_column, self.time_to_event_column]
        domain = self.domain()

        if domain is not None:
            out.append(domain)

        return out

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        X = self.data.drop(columns=[self.target_column, self.time_to_event_column])
        T = self.data[self.time_to_event_column]
        E = self.data[self.target_column]

        if as_numpy:
            return np.asarray(X), np.asarray(T), np.asarray(E)

        return X, T, E

    def dataframe(self) -> pd.DataFrame:
        return self.data

    def numpy(self) -> np.ndarray:
        return self.dataframe().values

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "static_features": list(self.static_features),
            "sensitive_features": self.sensitive_features,
            "important_features": self.important_features,
            "outcome_features": self.outcome_features,
            "target_column": self.target_column,
            "fairness_column": self.fairness_column,
            "time_to_event_column": self.time_to_event_column,
            "time_horizons": self.time_horizons,
            "train_size": self.train_size,
        }

    def __len__(self) -> int:
        return len(self.data)

    def decorate(self, data: Any) -> "DataLoader":
        return SurvivalAnalysisDataLoader(
            data,
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            target_column=self.target_column,
            fairness_column=self.fairness_column,
            time_to_event_column=self.time_to_event_column,
            time_horizons=self.time_horizons,
            random_state=self.random_state,
            train_size=self.train_size,
        )

    def satisfies(self, constraints: Constraints) -> bool:
        return constraints.is_valid(self.data)

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.decorate(
            constraints.match(self.data),
        )

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        return self.decorate(
            self.data.sample(count, random_state=random_state),
        )

    def drop(self, columns: list = []) -> "DataLoader":
        return self.decorate(
            self.data.drop(columns=columns),
        )

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        if not isinstance(data, pd.DataFrame):
            raise ValueError(f"Invalid data type {type(data)}")

        return SurvivalAnalysisDataLoader(
            data,
            target_column=info["target_column"],
            time_to_event_column=info["time_to_event_column"],
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            time_horizons=info["time_horizons"],
            fairness_column=info["fairness_column"],
        )

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self.data[feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self.data[feature] = val

    def train(self) -> "DataLoader":
        stratify = self.data[self.target_column]
        train_data, _ = train_test_split(
            self.data, train_size=self.train_size, random_state=0, stratify=stratify
        )
        return self.decorate(
            train_data.reset_index(drop=True),
        )

    def test(self) -> "DataLoader":
        stratify = self.data[self.target_column]
        _, test_data = train_test_split(
            self.data,
            train_size=self.train_size,
            random_state=0,
            stratify=stratify,
        )
        return self.decorate(
            test_data.reset_index(drop=True),
        )

    def fillna(self, value: Any) -> "DataLoader":
        self.data = self.data.fillna(value)
        return self

    def is_tabular(self) -> bool:
        return True


class TimeSeriesDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.TimeSeriesDataLoader
        :parts: 1

    Data Loader for Time Series Data

    Constructor Args:
        temporal data: List[pd.DataFrame]
            The temporal data. A list of pandas DataFrames
        observation times: List
            List of arrays mapping directly to index of each dataframe in temporal_data
        outcome: Optional[pd.DataFrame] = None
            pandas DataFrame thatn can be anything (eg, labels, regression outcome)
        static_data: Optional[pd.DataFrame] = None
            pandas DataFrame mapping directly to index of each dataframe in temporal_data
        sensitive_features: List[str]
            Name of sensitive features
        important_features List[str]
            Default: None. Only relevant for SurvivalGAN method
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        random_state: int
            Defaults to zero.

    Example:
        >>> TODO
    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame] = None,
        static_data: Optional[pd.DataFrame] = None,
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        fairness_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        seq_offset: int = 0,
        **kwargs: Any,
    ) -> None:
        static_features = []
        self.outcome_features = []

        if len(temporal_data) == 0:
            raise ValueError("Empty temporal data")

        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)

        max_window_len = max([len(t) for t in temporal_data])
        if static_data is not None:
            if len(static_data) != len(temporal_data):
                raise ValueError("Static and temporal data mismatch")
            static_features = list(static_data.columns)
        else:
            static_data = pd.DataFrame(np.zeros((len(temporal_data), 0)))

        if outcome is not None:
            if len(outcome) != len(temporal_data):
                raise ValueError("Temporal and outcome data mismatch")
            self.outcome_features = list(outcome.columns)
        else:
            outcome = pd.DataFrame(np.zeros((len(temporal_data), 0)))

        self.window_len = max_window_len
        self.fill = np.nan
        self.seq_offset = seq_offset

        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
            seq_df,
            seq_info,
        ) = TimeSeriesDataLoader.pack_raw_data(
            static_data,
            temporal_data,
            observation_times,
            outcome,
            fill=self.fill,
            seq_offset=seq_offset,
        )
        self.seq_info = seq_info
        self.fairness_column = fairness_column

        super().__init__(
            data={
                "static_data": static_data,
                "temporal_data": temporal_data,
                "observation_times": observation_times,
                "outcome": outcome,
                "seq_data": seq_df,
            },
            data_type="time_series",
            static_features=static_features,
            temporal_features=temporal_features,
            outcome_features=self.outcome_features,
            sensitive_features=sensitive_features,
            important_features=important_features,
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data["seq_data"].shape

    @property
    def columns(self) -> list:
        return self.data["seq_data"].columns

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    def compression_protected_features(self) -> list:
        return self.outcome_features

    @property
    def raw_columns(self) -> list:
        return self.static_features + self.temporal_features + self.outcome_features

    def dataframe(self) -> pd.DataFrame:
        return self.data["seq_data"].copy()

    def numpy(self) -> np.ndarray:
        return self.dataframe().values

    def info(self) -> dict:
        generic_info = {
            "data_type": self.data_type,
            "len": len(self),
            "static_features": self.static_features,
            "temporal_features": self.temporal_features,
            "outcome_features": self.outcome_features,
            "outcome_len": len(self.data["outcome"].values.reshape(-1))
            / len(self.data["outcome"]),
            "window_len": self.window_len,
            "sensitive_features": self.sensitive_features,
            "important_features": self.important_features,
            "fairness_column": self.fairness_column,
            "random_state": self.random_state,
            "train_size": self.train_size,
            "fill": self.fill,
        }

        for key in self.seq_info:
            generic_info[key] = self.seq_info[key]

        return generic_info

    def __len__(self) -> int:
        return len(self.data["seq_data"])

    def decorate(self, data: Any) -> "DataLoader":
        static_data, temporal_data, observation_times, outcome = data

        return TimeSeriesDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            outcome=outcome,
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            fairness_column=self.fairness_column,
            random_state=self.random_state,
            train_size=self.train_size,
            seq_offset=self.seq_offset,
        )

    def unpack_and_decorate(self, data: pd.DataFrame) -> "DataLoader":
        unpacked_data = TimeSeriesDataLoader.unpack_raw_data(
            data,
            self.info(),
        )

        return self.decorate(unpacked_data)

    def satisfies(self, constraints: Constraints) -> bool:
        seq_df = self.dataframe()

        return constraints.is_valid(seq_df)

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.unpack_and_decorate(
            constraints.match(self.dataframe()),
        )

    def drop(self, columns: list = []) -> "DataLoader":
        new_data = self.data["seq_data"].drop(columns=columns)
        return self.unpack_and_decorate(new_data)

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesDataLoader.unpack_raw_data(
            data,
            info,
        )
        return TimeSeriesDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            outcome=outcome,
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            fairness_column=info["fairness_column"],
            fill=info["fill"],
            seq_offset=info["seq_offset"],
        )

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        if pad:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesDataLoader.pad_and_mask(
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )
        else:
            static_data, temporal_data, observation_times, outcome = (
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )

        if as_numpy:
            longest_observation_seq = max([len(seq) for seq in temporal_data])
            padded_temporal_data = np.zeros(
                (len(temporal_data), longest_observation_seq, 5)
            )
            mask = np.ones((len(temporal_data), longest_observation_seq, 5), dtype=bool)
            for i, arr in enumerate(temporal_data):
                padded_temporal_data[i, : arr.shape[0], :] = arr  # Copy the actual data
                mask[
                    i, : arr.shape[0], :
                ] = False  # Set mask to False where actual data is present

            masked_temporal_data = ma.masked_array(padded_temporal_data, mask)
            return (
                np.asarray(static_data),
                masked_temporal_data,  # TODO: check this works with time series benchmarks
                # masked array to handle variable length sequences
                ma.vstack(
                    [
                        ma.array(
                            np.resize(ot, longest_observation_seq),
                            mask=[True for i in range(len(ot))]
                            + [False for j in range(longest_observation_seq - len(ot))],
                        )
                        for ot in observation_times
                    ]
                ),
                np.asarray(outcome),
            )
        return (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        )

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self.data["seq_data"][feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self.data["seq_data"][feature] = val

    def ids(self) -> list:
        id_col = self.seq_info["seq_id_feature"]
        ids = self.data["seq_data"][id_col]

        return list(ids.unique())

    def filter_ids(self, ids_list: list) -> pd.DataFrame:
        seq_data = self.data["seq_data"]
        id_col = self.info()["seq_id_feature"]

        return seq_data[seq_data[id_col].isin(ids_list)]

    def train(self) -> "DataLoader":
        # TODO: stratify
        ids = self.ids()
        train_ids, _ = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
        )
        return self.unpack_and_decorate(self.filter_ids(train_ids))

    def test(self) -> "DataLoader":
        # TODO: stratify
        ids = self.ids()
        _, test_ids = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
        )
        return self.unpack_and_decorate(self.filter_ids(test_ids))

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        ids = self.ids()
        count = min(count, len(ids))
        sampled_ids = random.sample(ids, count)

        return self.unpack_and_decorate(self.filter_ids(sampled_ids))

    def fillna(self, value: Any) -> "DataLoader":
        for key in ["static_data", "outcome", "seq_data"]:
            if self.data[key] is not None:
                self.data[key] = self.data[key].fillna(value)

        for idx, item in enumerate(self.data["temporal_data"]):
            self.data["temporal_data"][idx] = self.data["temporal_data"][idx].fillna(
                value
            )

        return self

    @staticmethod
    def unique_temporal_features(temporal_data: List[pd.DataFrame]) -> List:
        temporal_features = []
        for item in temporal_data:
            temporal_features.extend(item.columns)
        return sorted(np.unique(temporal_features).tolist())

    # Padding helpers
    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pad_raw_features(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
    ) -> Any:
        fill = np.nan

        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)

        for idx, item in enumerate(temporal_data):
            # handling missing features
            for col in temporal_features:
                if col not in item.columns:
                    item[col] = fill
            item = item[temporal_features]

            if list(item.columns) != list(temporal_features):
                raise RuntimeError("Invalid features for packing")

            temporal_data[idx] = item.fillna(fill)

        return static_data, temporal_data, observation_times, outcome

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pad_raw_data(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
    ) -> Any:
        fill = np.nan

        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesDataLoader.pad_raw_features(
            static_data, temporal_data, observation_times, outcome
        )
        max_window_len = max([len(t) for t in temporal_data])
        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)

        for idx, item in enumerate(temporal_data):
            if len(item) != max_window_len:
                pads = fill * np.ones(
                    (max_window_len - len(item), len(temporal_features))
                )
                start = 0
                if len(item.index) > 0:
                    start = max(item.index) + 1
                pads_df = pd.DataFrame(
                    pads,
                    index=[start + i for i in range(len(pads))],
                    columns=item.columns,
                )
                item = pd.concat([item, pads_df])

            # handle missing time points
            if list(item.columns) != list(temporal_features):
                raise RuntimeError(
                    f"Invalid features {item.columns}. Expected {temporal_features}"
                )
            if len(item) != max_window_len:
                raise RuntimeError("Invalid window len")

            temporal_data[idx] = item

        observation_times_padded = []
        for idx, item in enumerate(observation_times):
            item = list(item)
            if len(item) != max_window_len:
                pads = fill * np.ones(max_window_len - len(item))
                item.extend(pads.tolist())
            observation_times_padded.append(item)

        return static_data, temporal_data, observation_times_padded, outcome

    # Masking helpers
    @staticmethod
    def extract_masked_features(full_temporal_features: list) -> tuple:
        temporal_features = []
        mask_features = []
        mask_prefix = "masked_"
        for feat in full_temporal_features:
            feat = str(feat)
            if not feat.startswith(mask_prefix):
                temporal_features.append(feat)
                continue

            other_feat = feat[len(mask_prefix) :]
            if other_feat in full_temporal_features:
                mask_features.append(feat)
            else:
                temporal_features.append(feat)

        return temporal_features, mask_features

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def mask_temporal_data(
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        fill: Any = 0,
    ) -> Any:
        nan_cnt = 0
        for item in temporal_data:
            nan_cnt += np.asarray(np.isnan(item)).sum()

        if nan_cnt == 0:
            return temporal_data, observation_times

        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)
        masked_features = [f"masked_{feat}" for feat in temporal_features]

        for idx, item in enumerate(temporal_data):
            item[masked_features] = (~np.isnan(item)).astype(int)
            item = item.fillna(fill)
            temporal_data[idx] = item

        for idx, item in enumerate(observation_times):
            item = np.nan_to_num(item, nan=fill).tolist()

            observation_times[idx] = item

        return temporal_data, observation_times

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def unmask_temporal_data(
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        fill: Any = np.nan,
    ) -> Any:
        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)
        temporal_features, mask_features = TimeSeriesDataLoader.extract_masked_features(
            temporal_features
        )

        missing_horizons = []
        for idx, item in enumerate(temporal_data):
            # handle existing mask
            if len(mask_features) > 0:
                mask = temporal_data[idx][mask_features].astype(bool)
                item[~mask] = np.nan

            item_missing_rows = item.isna().sum(axis=1).values
            missing_horizons.append(item_missing_rows == len(temporal_features))

            # TODO: review impact on horizons
            temporal_data[idx] = item.dropna()

        observation_times_unmasked = []
        for idx, item in enumerate(observation_times):
            item = list(item)

            for midx, mval in enumerate(missing_horizons[idx]):
                if mval:
                    item[midx] = np.nan

            local_horizons = list(filter(lambda v: v == v, item))
            observation_times_unmasked.append(local_horizons)

        return temporal_data, observation_times_unmasked

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pad_and_mask(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
        only_features: Any = False,
        fill: Any = 0,
    ) -> Any:
        if only_features:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesDataLoader.pad_raw_features(
                static_data,
                temporal_data,
                observation_times,
                outcome,
            )
        else:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesDataLoader.pad_raw_data(
                static_data,
                temporal_data,
                observation_times,
                outcome,
            )

        temporal_data, observation_times = TimeSeriesDataLoader.mask_temporal_data(
            temporal_data, observation_times, fill=fill
        )

        return static_data, temporal_data, observation_times, outcome

    @staticmethod
    def sequential_view(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
        id_col: str = "seq_id",
        time_id_col: str = "seq_time_id",
        seq_offset: int = 0,
    ) -> Tuple[pd.DataFrame, dict]:  # sequential dataframe, loader info
        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesDataLoader.pad_and_mask(
            static_data, temporal_data, observation_times, outcome, only_features=True
        )
        raw_static_features = list(static_data.columns)
        static_features = [f"seq_static_{col}" for col in raw_static_features]

        raw_outcome_features = list(outcome.columns)
        outcome_features = [f"seq_out_{col}" for col in raw_outcome_features]

        raw_temporal_features = TimeSeriesDataLoader.unique_temporal_features(
            temporal_data
        )
        temporal_features = [f"seq_temporal_{col}" for col in raw_temporal_features]
        cols = (
            [id_col, time_id_col]
            + static_features
            + temporal_features
            + outcome_features
        )

        seq = []
        for sidx, static_item in static_data.iterrows():
            real_tidx = 0
            for tidx, temporal_item in temporal_data[sidx].iterrows():
                local_seq_data = (
                    [
                        sidx + seq_offset,
                        observation_times[sidx][real_tidx],
                    ]
                    + static_item[raw_static_features].values.tolist()
                    + temporal_item[raw_temporal_features].values.tolist()
                    + outcome.loc[sidx, raw_outcome_features].values.tolist()
                )
                seq.append(local_seq_data)
                real_tidx += 1

        seq_df = pd.DataFrame(seq, columns=cols)
        info = {
            "seq_static_features": static_features,
            "seq_temporal_features": temporal_features,
            "seq_outcome_features": outcome_features,
            "seq_offset": seq_offset,
            "seq_id_feature": id_col,
            "seq_time_id_feature": time_id_col,
            "seq_features": list(seq_df.columns),
        }
        return seq_df, info

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def pack_raw_data(
        static_data: Optional[pd.DataFrame],
        temporal_data: List[pd.DataFrame],
        observation_times: List,
        outcome: Optional[pd.DataFrame],
        fill: Any = np.nan,
        seq_offset: int = 0,
    ) -> pd.DataFrame:
        # Temporal data: (subjects, temporal_sequence, temporal_feature)
        temporal_features = TimeSeriesDataLoader.unique_temporal_features(temporal_data)
        temporal_features, mask_features = TimeSeriesDataLoader.extract_masked_features(
            temporal_features
        )
        temporal_data, observation_times = TimeSeriesDataLoader.unmask_temporal_data(
            temporal_data, observation_times
        )
        seq_df, info = TimeSeriesDataLoader.sequential_view(
            static_data=static_data,
            temporal_data=temporal_data,
            observation_times=observation_times,
            outcome=outcome,
            seq_offset=seq_offset,
        )

        return static_data, temporal_data, observation_times, outcome, seq_df, info

    @staticmethod
    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def unpack_raw_data(
        data: pd.DataFrame,
        info: dict,
    ) -> Tuple[
        Optional[pd.DataFrame], List[pd.DataFrame], List, Optional[pd.DataFrame]
    ]:
        id_col = info["seq_id_feature"]
        time_col = info["seq_time_id_feature"]

        static_cols = info["seq_static_features"]
        new_static_cols = [feat.split("seq_static_")[1] for feat in static_cols]

        temporal_cols = info["seq_temporal_features"]
        new_temporal_cols = [feat.split("seq_temporal_")[1] for feat in temporal_cols]

        outcome_cols = info["seq_outcome_features"]
        new_outcome_cols = [feat.split("seq_out_")[1] for feat in outcome_cols]

        ids = sorted(list(set(data[id_col])))

        static_data = []
        temporal_data = []
        observation_times = []
        outcome_data = []

        for item_id in ids:
            item_data = data[data[id_col] == item_id]

            static_data.append(item_data[static_cols].head(1).values.squeeze().tolist())
            outcome_data.append(
                item_data[outcome_cols].head(1).values.squeeze().tolist()
            )
            local_temporal_data = item_data[temporal_cols].copy()
            local_observation_times = item_data[time_col].values.tolist()
            local_temporal_data.columns = new_temporal_cols
            # TODO: review impact on horizons
            local_temporal_data = local_temporal_data.dropna()

            temporal_data.append(local_temporal_data)
            observation_times.append(local_observation_times)

        static_df = pd.DataFrame(static_data, columns=new_static_cols)
        outcome_df = pd.DataFrame(outcome_data, columns=new_outcome_cols)

        return static_df, temporal_data, observation_times, outcome_df

    def is_tabular(self) -> bool:
        return True


class TimeSeriesSurvivalDataLoader(TimeSeriesDataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.TimeSeriesSurvivalDataLoader
        :parts: 1

    Data loader for Time series survival data

    Constructor Args:
        temporal_data: List[pd.DataFrame}
            The temporal data. A list of pandas DataFrames.
        observation_times: List
            List of arrays mapping directly to index of each dataframe in temporal_data
        T: Union[pd.Series, np.ndarray, pd.Series]
            Time-to-event data
        E: Union[pd.Series, np.ndarray, pd.Series]
            E is censored/event data
        static_data Optional[pd.DataFrame] = None
            pandas DataFrame of static features for each subject
        sensitive_features: List[str]
            Name of sensitive features
        important_features: List[str}
            Default: None. Only relevant for SurvivalGAN method.
        fairness_column: Optional[str]
            Optional fairness column label, used for fairness benchmarking.
        random_state. int
            Defaults to zero.

    Example:
        >>> TODO

    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        temporal_data: List[pd.DataFrame],
        observation_times: Union[List, np.ndarray, pd.Series],
        T: Union[pd.Series, np.ndarray, pd.Series],
        E: Union[pd.Series, np.ndarray, pd.Series],
        static_data: Optional[pd.DataFrame] = None,
        sensitive_features: List[str] = [],
        important_features: List[str] = [],
        time_horizons: list = [],
        fairness_column: Optional[str] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        seq_offset: int = 0,
        **kwargs: Any,
    ) -> None:
        self.time_to_event_col = "time_to_event"
        self.event_col = "event"
        self.fairness_column = fairness_column

        if len(time_horizons) == 0:
            time_horizons = np.linspace(T.min(), T.max(), num=5)[1:-1].tolist()
        self.time_horizons = time_horizons
        outcome = pd.concat([pd.Series(T), pd.Series(E)], axis=1)
        outcome.columns = [self.time_to_event_col, self.event_col]

        self.fill = np.nan

        super().__init__(
            temporal_data=temporal_data,
            observation_times=observation_times,
            outcome=outcome,
            static_data=static_data,
            sensitive_features=sensitive_features,
            important_features=important_features,
            random_state=random_state,
            train_size=train_size,
            seq_offset=seq_offset,
            **kwargs,
        )
        self.data_type = "time_series_survival"

    def get_fairness_column(self) -> Union[str, Any]:
        return self.fairness_column

    def info(self) -> dict:
        parent_info = super().info()
        parent_info["time_to_event_column"] = self.time_to_event_col
        parent_info["event_column"] = self.event_col
        parent_info["time_horizons"] = self.time_horizons
        parent_info["fill"] = self.fill

        return parent_info

    def decorate(self, data: Any) -> "DataLoader":
        static_data, temporal_data, observation_times, outcome = data
        if self.time_to_event_col not in outcome:
            raise ValueError(
                f"Survival outcome is missing tte column {self.time_to_event_col}"
            )
        if self.event_col not in outcome:
            raise ValueError(
                f"Survival outcome is missing event column {self.event_col}"
            )

        return TimeSeriesSurvivalDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            T=outcome[self.time_to_event_col],
            E=outcome[self.event_col],
            sensitive_features=self.sensitive_features,
            important_features=self.important_features,
            fairness_column=self.fairness_column,
            random_state=self.random_state,
            time_horizons=self.time_horizons,
            train_size=self.train_size,
            seq_offset=self.seq_offset,
        )

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "DataLoader":
        (
            static_data,
            temporal_data,
            observation_times,
            outcome,
        ) = TimeSeriesSurvivalDataLoader.unpack_raw_data(
            data,
            info,
        )
        return TimeSeriesSurvivalDataLoader(
            temporal_data,
            observation_times=observation_times,
            static_data=static_data,
            T=outcome[info["time_to_event_column"]],
            E=outcome[info["event_column"]],
            sensitive_features=info["sensitive_features"],
            important_features=info["important_features"],
            fairness_column=info["fairness_column"],
            time_horizons=info["time_horizons"],
            seq_offset=info["seq_offset"],
        )

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        if pad:
            (
                static_data,
                temporal_data,
                observation_times,
                outcome,
            ) = TimeSeriesSurvivalDataLoader.pad_and_mask(
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )
        else:
            static_data, temporal_data, observation_times, outcome = (
                self.data["static_data"],
                self.data["temporal_data"],
                self.data["observation_times"],
                self.data["outcome"],
            )

        if as_numpy:
            return (
                np.asarray(static_data),
                np.asarray(temporal_data, dtype=object),
                np.asarray(observation_times, dtype=object),
                np.asarray(outcome[self.time_to_event_col]),
                np.asarray(outcome[self.event_col]),
            )
        return (
            static_data,
            temporal_data,
            observation_times,
            outcome[self.time_to_event_col],
            outcome[self.event_col],
        )

    def match(self, constraints: Constraints) -> "DataLoader":
        return self.unpack_and_decorate(
            constraints.match(self.dataframe()),
        )

    def train(self) -> "DataLoader":
        stratify = self.data["outcome"][self.event_col]

        ids = self.ids()
        train_ids, _ = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )
        return self.unpack_and_decorate(self.filter_ids(train_ids))

    def test(self) -> "DataLoader":
        stratify = self.data["outcome"][self.event_col]
        ids = self.ids()
        _, test_ids = train_test_split(
            ids,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )
        return self.unpack_and_decorate(self.filter_ids(test_ids))


class ImageDataLoader(DataLoader):
    """
    .. inheritance-diagram:: synthcity.plugins.core.dataloader.ImageDataLoader
        :parts: 1

    Data loader for generic image data.

    Constructor Args:
        data: torch.utils.data.Dataset or torch.Tensor
            The image dataset or a tuple of (tensor images, tensor labels)
        random_state: int
            Defaults to zero.
        height: int. Default = 32
            Height to use internally
        width: Optional[int]
            Optional width to use internally. If None, it is used the same value as height.
        train_size: float = 0.8
            Train dataset ratio.
    Example:
        >>> dataset = datasets.MNIST(".", download=True)
        >>>
        >>> loader = ImageDataLoader(
        >>>     data=dataset,
        >>>     train_size=0.8,
        >>>     height=32,
        >>>     width=w32,
        >>> )

    """

    @validate_arguments(config=dict(arbitrary_types_allowed=True))
    def __init__(
        self,
        data: Union[torch.utils.data.Dataset, Tuple[torch.Tensor, torch.Tensor]],
        height: int = 32,
        width: Optional[int] = None,
        random_state: int = 0,
        train_size: float = 0.8,
        **kwargs: Any,
    ) -> None:
        if width is None:
            width = height

        if isinstance(data, tuple):
            X, y = data
            data = TensorDataset(images=X, targets=y)

        self.data_transform = None

        dummy, _ = data[0]
        img_transform = []
        if not isinstance(dummy, PIL.Image.Image):
            img_transform = [transforms.ToPILImage()]

        img_transform.extend(
            [
                transforms.Resize((height, width)),
                transforms.ToTensor(),
                transforms.Normalize(mean=(0.5,), std=(0.5,)),
            ]
        )

        self.data_transform = transforms.Compose(img_transform)
        data = FlexibleDataset(data, transform=self.data_transform)

        self.height = height
        self.width = width
        self.channels = data.shape()[1]

        super().__init__(
            data_type="images",
            data=data,
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

    @property
    def shape(self) -> tuple:
        return self.data.shape()

    def get_fairness_column(self) -> None:
        """Not implemented for ImageDataLoader"""
        ...

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        return self.data

    def numpy(self) -> np.ndarray:
        x, _ = self.data.numpy()

        return x

    def dataframe(self) -> pd.DataFrame:
        x = self.numpy().reshape(len(self), -1)

        x = pd.DataFrame(x)
        x.columns = x.columns.astype(str)

        return x

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "train_size": self.train_size,
            "height": self.height,
            "width": self.width,
            "channels": self.channels,
            "random_state": self.random_state,
        }

    def __len__(self) -> int:
        return len(self.data)

    def decorate(self, data: Any) -> "DataLoader":
        return ImageDataLoader(
            data,
            random_state=self.random_state,
            train_size=self.train_size,
            height=self.height,
            width=self.width,
        )

    def sample(self, count: int, random_state: int = 0) -> "DataLoader":
        idxs = np.random.choice(len(self), count, replace=False)
        subset = FlexibleDataset(self.data.data, indices=idxs)
        return self.decorate(subset)

    @staticmethod
    def from_info(data: torch.utils.data.Dataset, info: dict) -> "ImageDataLoader":
        if not isinstance(data, torch.utils.data.Dataset):
            raise ValueError(f"Invalid data type {type(data)}")

        return ImageDataLoader(
            data,
            train_size=info["train_size"],
            height=info["height"],
            width=info["width"],
            random_state=info["random_state"],
        )

    def __getitem__(self, index: Union[list, int, str]) -> Any:
        if isinstance(index, str):
            return self.dataframe()[index]

        return self.numpy()[index]

    def _train_test_split(self) -> Tuple:
        indices = np.arange(len(self.data))
        _, stratify = self.data.numpy()

        return train_test_split(
            indices,
            train_size=self.train_size,
            random_state=self.random_state,
            stratify=stratify,
        )

    def train(self) -> "DataLoader":
        train_idx, _ = self._train_test_split()
        subset = FlexibleDataset(self.data.data, indices=train_idx)
        return self.decorate(subset)

    def test(self) -> "DataLoader":
        _, test_idx = self._train_test_split()
        subset = FlexibleDataset(self.data.data, indices=test_idx)
        return self.decorate(subset)

    def compress(
        self,
    ) -> Tuple["DataLoader", Dict]:
        return self, {}

    def decompress(self, context: Dict) -> "DataLoader":
        return self

    def encode(
        self,
        encoders: Optional[Dict[str, Any]] = None,
    ) -> Tuple["DataLoader", Dict]:
        return self, {}

    def decode(
        self,
        encoders: Dict[str, Any],
    ) -> "DataLoader":
        return self

    def is_tabular(self) -> bool:
        return False

    @property
    def columns(self) -> list:
        return list(self.dataframe().columns)

    def satisfies(self, constraints: Constraints) -> bool:
        return True

    def match(self, constraints: Constraints) -> "DataLoader":
        return self

    def compression_protected_features(self) -> list:
        raise NotImplementedError("Images do not support the compression call")

    def drop(self, columns: list = []) -> "DataLoader":
        raise NotImplementedError()

    def __setitem__(self, feature: str, val: Any) -> None:
        raise NotImplementedError()

    def fillna(self, value: Any) -> "DataLoader":
        raise NotImplementedError()


@validate_arguments(config=dict(arbitrary_types_allowed=True))
def create_from_info(
    data: Union[pd.DataFrame, torch.utils.data.Dataset], info: dict
) -> "DataLoader":
    """Helper for creating a DataLoader from existing information."""
    if info["data_type"] == "generic":
        return GenericDataLoader.from_info(data, info)
    elif info["data_type"] == "survival_analysis":
        return SurvivalAnalysisDataLoader.from_info(data, info)
    elif info["data_type"] == "time_series":
        return TimeSeriesDataLoader.from_info(data, info)
    elif info["data_type"] == "time_series_survival":
        return TimeSeriesSurvivalDataLoader.from_info(data, info)
    elif info["data_type"] == "images":
        return ImageDataLoader.from_info(data, info)
    else:
        raise RuntimeError(f"invalid datatype {info}")


class Syn_SeqDataLoader(DataLoader):
    """
    A DataLoader that applies Syn_Seq-style preprocessing to input data,
    and optionally allows user_custom (syn_order, variable_selection, method) updates
    via update_user_custom(...).
    """

    def __init__(
        self,
        data: pd.DataFrame,
        syn_order: Optional[List[str]] = None,
        special_value: Optional[Dict[str, List[Any]]] = None,
        col_type: Optional[Dict[str, str]] = None,
        max_categories: int = 20,
        random_state: int = 0,
        train_size: float = 0.8,
        verbose: bool = True,
        **kwargs: Any,
    ) -> None:
        if not syn_order and verbose:
            print("[INFO] Most of the time, it is recommened to have category variables before synthesizing numeric variables.")
        syn_order = syn_order or list(data.columns)

        missing_columns = set(syn_order) - set(data.columns)
        if missing_columns:
            raise ValueError(f"Missing columns in input data: {missing_columns}")

        self.syn_order = syn_order
        self.columns_special_values = special_value or {}
        self.col_type = col_type or {}
        self.max_categories = max_categories
        self._verbose = verbose

        # ìž¬ë°°ì¹˜ëœ DataFrame
        self._df = data[self.syn_order].copy()

        # ë¶€ëª¨ DataLoader ìƒì„±ìž
        super().__init__(
            data_type="syn_seq",
            data=self._df,
            random_state=random_state,
            train_size=train_size,
            **kwargs,
        )

        # ---- encoder ìƒì„± + fit ----
        self._encoder = Syn_SeqEncoder(
            columns_special_values=self.columns_special_values,
            syn_order=self.syn_order,
            max_categories=self.max_categories,
            col_type=self.col_type,
        )
        self._encoder.fit(self._df)

        if self._verbose:
            print("[INFO] Syn_SeqDataLoader init complete:")
            print(f"  - syn_order: {self.syn_order}")
            print(f"  - special_value: {self.columns_special_values}")
            print(f"  - col_type: {self.col_type}")
            print(f"  - data shape: {self._df.shape}")

            print("[DEBUG] After encoder.fit(), detected info:")

            # 1) encoder.col_map (ê° ì»¬ëŸ¼: {original_dtype, converted_type, method})
            if hasattr(self._encoder, "col_map"):
                print("  - encoder.col_map =>")
                for col_name, cinfo in self._encoder.col_map.items():
                    print(f"       {col_name} : {cinfo}")

            # 2) variable_selection_ (ìžˆìœ¼ë©´ ì¶œë ¥)
            if self._encoder.variable_selection_ is not None:
                print("  - variable_selection_:\n", self._encoder.variable_selection_)

            # date_mins ë“± ë‹¤ë¥¸ í•„ë“œë¥¼ ë³´ê³  ì‹¶ë‹¤ë©´ ì•„ëž˜ì²˜ëŸ¼
            if hasattr(self._encoder, "date_mins"):
                print(f"  - date_mins: {self._encoder.date_mins}")

            print("----------------------------------------------------------------")

    # ----------------------------------------------------------------
    # Inherited/required abstract methods
    # ----------------------------------------------------------------
    @property
    def shape(self) -> tuple:
        return self._df.shape

    @property
    def columns(self) -> list:
        return list(self._df.columns)

    def dataframe(self) -> pd.DataFrame:
        return self._df

    def numpy(self) -> pd.DataFrame:
        return self._df.values

    def info(self) -> dict:
        return {
            "data_type": self.data_type,
            "len": len(self),
            "train_size": self.train_size,
            "random_state": self.random_state,
            "syn_order": self.syn_order,
            "max_categories": self.max_categories,
            "col_type": self.col_type,
            "columns_special_values": self.columns_special_values,
        }

    def __len__(self) -> int:
        return len(self._df)

    def satisfies(self, constraints: Constraints) -> bool:
        return constraints.is_valid(self._df)

    def match(self, constraints: Constraints) -> "Syn_SeqDataLoader":
        matched_df = constraints.match(self._df)
        return self.decorate(matched_df)

    @staticmethod
    def from_info(data: pd.DataFrame, info: dict) -> "Syn_SeqDataLoader":
        return Syn_SeqDataLoader(
            data=data,
            syn_order=info.get("syn_order"),
            special_value=info.get("columns_special_values", {}),
            col_type=info.get("col_type", {}),
            max_categories=info.get("max_categories", 20),
            random_state=info["random_state"],
            train_size=info["train_size"],
        )

    def sample(self, count: int, random_state: int = 0) -> "Syn_SeqDataLoader":
        sampled_df = self._df.sample(count, random_state=random_state)
        return self.decorate(sampled_df)

    def drop(self, columns: list = []) -> "Syn_SeqDataLoader":
        dropped_df = self._df.drop(columns=columns, errors="ignore")
        return self.decorate(dropped_df)

    def __getitem__(self, feature: Union[str, list, int]) -> Any:
        return self._df[feature]

    def __setitem__(self, feature: str, val: Any) -> None:
        self._df[feature] = val

    def train(self) -> "Syn_SeqDataLoader":
        ntrain = int(len(self._df) * self.train_size)
        train_df = self._df.iloc[:ntrain].copy()
        return self.decorate(train_df)

    def test(self) -> "Syn_SeqDataLoader":
        ntrain = int(len(self._df) * self.train_size)
        test_df = self._df.iloc[ntrain:].copy()
        return self.decorate(test_df)

    def fillna(self, value: Any) -> "Syn_SeqDataLoader":
        filled_df = self._df.fillna(value)
        return self.decorate(filled_df)

    def compression_protected_features(self) -> list:
        return []

    def is_tabular(self) -> bool:
        return True

    def unpack(self, as_numpy: bool = False, pad: bool = False) -> Any:
        if as_numpy:
            return self._df.to_numpy()
        return self._df

    def get_fairness_column(self) -> Union[str, Any]:
        return None

    # ----------------------------------------------------------------
    # encode/decode
    # ----------------------------------------------------------------
    def encode(
        self, encoders: Optional[Dict[str, Any]] = None
    ) -> Tuple["Syn_SeqDataLoader", Dict]:
        """
        Typically used by aggregator's fit step.
        """
        if encoders is None:
            encoded_data = self._encoder.transform(self._df)
            new_loader = self.decorate(encoded_data)
            return new_loader, {"syn_seq_encoder": self._encoder}
        else:
            return self, encoders

    def decode(self, encoders: Dict[str, Any]) -> "Syn_SeqDataLoader":
        if "syn_seq_encoder" in encoders:
            encoder = encoders["syn_seq_encoder"]
            if not isinstance(encoder, Syn_SeqEncoder):
                raise TypeError(f"Expected Syn_SeqEncoder, got {type(encoder)}")
            decoded_data = encoder.inverse_transform(self._df)
            return self.decorate(decoded_data)
        return self

    def decorate(self, data: pd.DataFrame) -> "Syn_SeqDataLoader":
        """
        Helper for creating a new instance with the same settings but new data.
        """
        new_loader = Syn_SeqDataLoader(
            data=data,
            syn_order=self.syn_order,
            special_value=self.columns_special_values,
            col_type=self.col_type,
            max_categories=self.max_categories,
            random_state=self.random_state,
            train_size=self.train_size,
            verbose=self._verbose,
        )
        return new_loader
    
    # ----------------------------------------------------------------
    # RE-INTRODUCE update_user_custom(...)
    # ----------------------------------------------------------------
    def update_user_custom(self, user_custom: Dict[str, Any]) -> "Syn_SeqDataLoader":
        """
        Allows user to update certain aspects (syn_order, variable_selection, method),
        without calling transform() here.
        The aggregator (or user) will later call self.encode() => encoder.transform() once.
        """

        # 1) syn_order
        if "syn_order" in user_custom:
            new_order = user_custom["syn_order"]
            self.syn_order = new_order
            self._encoder.syn_order = new_order

        # 2) variable_selection
        if "variable_selection" in user_custom:
            vs_val = user_custom["variable_selection"]
            if isinstance(vs_val, dict):
                current_vs = self._encoder.variable_selection_
                if current_vs is None:
                    # ë§Œì•½ encoder.variable_selection_ì´ ì•„ì§ Noneì´ë©´,
                    # syn_order í¬ê¸°ì— ë§žì¶° ìƒì„±
                    all_cols = self.syn_order
                    current_vs = pd.DataFrame(0, index=all_cols, columns=all_cols)
                updated_vs = self._encoder.update_variable_selection(current_vs, vs_val)
                self._encoder.variable_selection_ = updated_vs
            elif isinstance(vs_val, pd.DataFrame):
                # ì§ì ‘ ëŒ€ìž…
                self._encoder.variable_selection_ = vs_val.copy()
            else:
                print(f"[WARNING] variable_selection must be dict or DataFrame, got {type(vs_val)}")

        # 3) method
        if "method" in user_custom:
            # ì—¬ê¸°ì„œëŠ” ë°ì´í„°ë¡œë”ê°€ ì§ì ‘ method ì •ë³´ë¥¼ ì“°ì§€ ì•Šì„ ìˆ˜ë„ ìžˆìŒ
            # aggregator fit ì‹œì ì— í™œìš©í•˜ë„ë¡, ì¼ë‹¨ ì €ìž¥ë§Œ
            self._method = user_custom["method"]

        # (ì¤‘ìš”) transform/decorate í˜¸ì¶œ ì œê±° â†’ â€œí•œ ë²ˆë§Œ ë³€í™˜â€ ì›ì¹™
        # => ì•„ëž˜ ë‘ ì¤„(ì´ì „ ë²„ì „ì˜ step 4,5)ì€ ì œê±°/ì£¼ì„ ì²˜ë¦¬
        #
        # encoded_df = self._encoder.transform(self._df)
        # updated_loader = self.decorate(encoded_df)
        # self.__dict__.update(updated_loader.__dict__)

        # ëŒ€ì‹  ìžê¸° ìžì‹  ê·¸ëŒ€ë¡œ ë°˜í™˜
        return self



src/synthcity/plugins/generic/plugin_syn_seq.py
# synthcity/plugins/generic/plugin_syn_seq.py

# stdlib
from pathlib import Path
from typing import Any, List, Optional, Union

# third party
import numpy as np
import pandas as pd

# synthcity absolute
from synthcity.plugins.core.models.syn_seq.syn_seq import Syn_Seq
from synthcity.plugins.core.plugin import Plugin
from synthcity.plugins.core.dataloader import DataLoader, Syn_SeqDataLoader
from synthcity.plugins.core.schema import Schema


from typing import Any, Dict, List, Optional, Union

import pandas as pd
import numpy as np

# synthcity absolute
from synthcity.plugins.core.plugin import Plugin
from synthcity.plugins.core.dataloader import DataLoader, Syn_SeqDataLoader
from synthcity.plugins.core.schema import Schema
from synthcity.plugins.core.constraints import Constraints

# Our sequential aggregator
from synthcity.plugins.core.models.syn_seq.syn_seq import Syn_Seq

class Syn_SeqPlugin(Plugin):
    """
    A plugin that delegates to Syn_Seq aggregator for sequential column-by-column synthesis.
    """

    @staticmethod
    def name() -> str:
        return "syn_seq"

    @staticmethod
    def type() -> str:
        return "syn_seq"

    @staticmethod
    def hyperparameter_space(**kwargs: Any) -> list:
        # No hyperparameters to tune by default
        return []

    def __init__(
        self,
        random_state: int = 0,
        default_first_method: str = "swr",
        default_other_method: str = "cart",
        strict: bool = True,
        sampling_patience: int = 100,
        **kwargs: Any,
    ):
        super().__init__(random_state=random_state, **kwargs)
        self._aggregator = Syn_Seq(
            random_state=random_state,
            strict=strict,
            sampling_patience=sampling_patience,
            default_first_method=default_first_method,
            default_other_method=default_other_method,
        )
        self._model_trained = False

    def _fit(
        self,
        X: DataLoader,
        user_custom: Optional[dict] = None,
        *args: Any,
        **kwargs: Any
    ) -> "Syn_SeqPlugin":
        if not isinstance(X, Syn_SeqDataLoader):
            raise TypeError("Syn_SeqPlugin expects a Syn_SeqDataLoader for sequential usage.")

        self._aggregator.fit(X, user_custom=user_custom, *args, **kwargs)
        self._model_trained = True
        return self

    def _generate(
        self,
        count: int,
        syn_schema: Schema,
        constraints: Optional[Constraints] = None,
        X: Optional[DataLoader] = None,
        **kwargs: Any
    ) -> DataLoader:
        if not self._model_trained:
            raise RuntimeError("Must fit Syn_SeqPlugin before generating data.")
        if X is None:
            raise ValueError("Syn_SeqPlugin.generate requires a DataLoader reference for decoding.")

        # aggregator.generate => returns Syn_SeqGeneratorResult
        syn_result = self._aggregator.generate(
            nrows=count,
            constraints=constraints,
            encoded_loader=X,
            **kwargs
        )
        # final .dataframe() => we get a pd.DataFrame => then decorate
        df_synth = syn_result.dataframe()
        return X.decorate(df_synth)

plugin = Syn_SeqPlugin



